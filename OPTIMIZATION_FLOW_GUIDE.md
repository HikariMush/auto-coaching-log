#!/usr/bin/env python3
"""
SmashZettel-Bot: Complete Optimization Flow
FROM: User running bot + providing /teach corrections
TO: Automatic prompt optimization with dspy.Teleprompter

This script orchestrates the entire optimization pipeline.
"""

import os
import json
import sys
from pathlib import Path
from datetime import datetime

# Configuration
DATA_DIR = Path('/workspaces/auto-coaching-log/data')
TRAINING_DATA_FILE = DATA_DIR / 'training_data.jsonl'
OPTIMIZED_MODEL_FILE = DATA_DIR / 'optimized_coach_state.json'
MIN_TRAINING_EXAMPLES = 30  # Minimum examples before optimization

def print_step(num, title, description=""):
    """Pretty print workflow steps"""
    print(f"\n{'='*70}")
    print(f"STEP {num}: {title}")
    print(f"{'='*70}")
    if description:
        print(description)

def print_substep(num, title):
    """Pretty print sub-steps"""
    print(f"\n  [{num}] {title}")

def step1_setup_environment():
    """STEP 1: Environment Setup"""
    print_step(1, "Environment Setup", 
               "Initialize .env and verify API credentials")
    
    print_substep("1.1", "Check .env file exists")
    env_file = Path('/workspaces/auto-coaching-log/.env')
    if env_file.exists():
        print(f"    ‚úÖ .env file found")
        # Verify required keys
        import dotenv
        config = dotenv.dotenv_values(env_file)
        required_keys = [
            'GEMINI_API_KEY',
            'PINECONE_API_KEY',
            'DISCORD_BOT_TOKEN'
        ]
        missing = [k for k in required_keys if k not in config or not config[k]]
        if missing:
            print(f"    ‚ö†Ô∏è  Missing keys: {', '.join(missing)}")
            print(f"    üìù Edit .env and add missing values")
            return False
        print(f"    ‚úÖ All required keys present")
    else:
        print(f"    ‚ùå .env file not found")
        print(f"    üìù Copy .env.example ‚Üí .env and fill in your API keys")
        print(f"       cp .env.example .env")
        return False
    
    print_substep("1.2", "Verify data directory")
    DATA_DIR.mkdir(exist_ok=True)
    print(f"    ‚úÖ Data directory ready: {DATA_DIR}")
    
    return True

def step2_ingest_knowledge_base():
    """STEP 2: Ingest Knowledge Base"""
    print_step(2, "Ingest Knowledge Base",
               "Load raw_data and Notion into Pinecone (one-time setup)")
    
    print_substep("2.1", "Ingest raw_data/*.txt files")
    print(f"""
    Run this ONCE to populate Pinecone with SmashBros mechanics:
    
    $ python -m src.utils.ingest
    
    What it does:
    ‚îú‚îÄ Reads 42 .txt files from src/brain/raw_data/
    ‚îú‚îÄ Embeds each with Google embedding-001 (768-dim)
    ‚îú‚îÄ Uploads vectors to Pinecone index "smash-zettel"
    ‚îî‚îÄ Total: ~50-100 vectors (~200KB)
    
    ‚è±Ô∏è  Estimated time: 2-5 minutes
    """)
    
    print_substep("2.2", "Sync Notion Theory DB")
    print(f"""
    Run this FIRST TIME, then schedule for hourly runs:
    
    $ python -m src.utils.notion_sync
    
    What it does:
    ‚îú‚îÄ Fetches all pages from Notion Theory DB
    ‚îú‚îÄ Extracts block contents
    ‚îú‚îÄ Embeds with Google embedding-001
    ‚îú‚îÄ Uploads to Pinecone index
    ‚îî‚îÄ Metadata: synced_at timestamp
    
    For scheduled runs (recommended):
    ‚îú‚îÄ Google Cloud Tasks: Run hourly
    ‚îú‚îÄ Local cron: 0 * * * * cd /path && python -m src.utils.notion_sync
    ‚îî‚îÄ or manually before bot startup
    """)
    
    print("\n    ‚úÖ Knowledge base ready")
    return True

def step3_launch_discord_bot():
    """STEP 3: Launch Discord Bot"""
    print_step(3, "Launch Discord Bot",
               "Start the bot and begin collecting user corrections")
    
    print_substep("3.1", "Start the bot")
    print(f"""
    $ python src/main.py
    
    What it does:
    ‚îú‚îÄ Connects to Discord
    ‚îú‚îÄ Registers slash commands:
    ‚îÇ  ‚îú‚îÄ /ask <query>  - Get coaching advice
    ‚îÇ  ‚îî‚îÄ /teach <query> <correction> - Provide corrections
    ‚îú‚îÄ Listens for commands
    ‚îî‚îÄ Starts asyncio event loop
    
    ‚úÖ Bot is live - Ready for user interaction!
    """)
    
    print_substep("3.2", "User workflow")
    print(f"""
    Users interact with the bot:
    
    1Ô∏è‚É£  User: /ask "„ÇØ„É©„Ç¶„Éâ„Å´Âãù„Å§„Å´„ÅØÔºü"
        Bot: "„ÄêÂàÜÊûê„Äë... „Äê„Ç¢„Éâ„Éê„Ç§„Çπ„Äë..."
    
    2Ô∏è‚É£  User evaluates response
        ‚îú‚îÄ If satisfied: ‚úÖ No action needed
        ‚îî‚îÄ If needs improvement: Go to step 3
    
    3Ô∏è‚É£  User: /teach query:"„ÇØ„É©„Ç¶„Éâ„Å´Âãù„Å§„Å´„ÅØÔºü" correction:"„Çà„ÇäÂÖ∑‰ΩìÁöÑ„Å´„Ç≥„É≥„Éú„ÇíÊïô„Åà„Å¶"
        Bot: ‚úÖ Correction saved to training_data.jsonl
    
    4Ô∏è‚É£  Repeat: More queries ‚Üí More corrections accumulated
        ‚îî‚îÄ Goal: Collect 30+ corrections for optimization
    """)
    
    return True

def step4_monitor_training_data():
    """STEP 4: Monitor Training Data Accumulation"""
    print_step(4, "Monitor Training Data Accumulation",
               "Track correction submissions as they arrive")
    
    print_substep("4.1", "Check training data file")
    print(f"""
    Location: {TRAINING_DATA_FILE}
    
    Format: JSONL (one JSON object per line)
    
    $ tail -20 data/training_data.jsonl
    """)
    
    print_substep("4.2", "Analyze data")
    print(f"""
    $ python -c "
import json
from pathlib import Path

jsonl_path = Path('data/training_data.jsonl')
if jsonl_path.exists():
    lines = jsonl_path.read_text().strip().split('\\n')
    entries = [json.loads(line) for line in lines if line]
    print(f'Total corrections collected: {{len(entries)}}')
    for i, entry in enumerate(entries[-5:], 1):
        print(f'  {{i}}: Q=\"{{entry[\"question\"][:40]}}...\"')
        print(f'      Correction=\"{{entry.get(\"gold_answer\", \"?\")[:40]}}...\"')
else:
    print('No training data yet')
    "
    """)
    
    print_substep("4.3", "Wait for sufficient data")
    print(f"""
    Optimization Thresholds:
    ‚îú‚îÄ Phase 1 (Initial feedback): 5-10 corrections
    ‚îú‚îÄ Phase 2 (Pattern recognition): 20-30 corrections
    ‚îú‚îÄ Phase 3 (Robust optimization): 50+ corrections
    ‚îú‚îÄ Phase 4 (Continuous improvement): 100+ corrections
    ‚îî‚îÄ Recommended first run: 30-50 corrections
    
    üìä Data Collection Timeline:
    ‚îú‚îÄ Day 1-2: 5-10 corrections (first patterns emerge)
    ‚îú‚îÄ Day 3-5: 20-30 corrections (initial optimization possible)
    ‚îú‚îÄ Day 7-10: 50+ corrections (robust optimization)
    ‚îî‚îÄ Ongoing: 100+ corrections (continuous refinement)
    """)
    
    return True

def step5_prepare_optimization():
    """STEP 5: Prepare for Optimization"""
    print_step(5, "Prepare for Optimization",
               "Define metrics and optimization strategy")
    
    print_substep("5.1", "Define coaching quality metric")
    print(f"""
    Create a scoring function that evaluates coaching quality.
    
    Location: src/utils/optimize_coach.py (you'll create this)
    
    Example implementation:
    
    ```python
    def coaching_quality_metric(gold, pred, trace=None):
        '''
        Score prediction against gold answer.
        
        Dimensions:
        1. Relevance: Is advice related to user's question?
        2. Specificity: Is it character/move specific?
        3. Actionability: Can user execute the advice?
        4. Correctness: Does it match SmashBros mechanics?
        '''
        
        # Extract scores from gold data
        gold_data = json.loads(gold) if isinstance(gold, str) else gold
        
        # Measure prediction quality
        pred_text = pred.advice if hasattr(pred, 'advice') else str(pred)
        gold_text = gold_data.get('gold_answer', '')
        
        # Calculate metrics
        relevance_score = measure_relevance(pred_text, gold_text)
        specificity_score = measure_specificity(pred_text)
        actionability_score = measure_actionability(pred_text)
        
        # Combine scores
        final_score = (
            0.4 * relevance_score +
            0.3 * specificity_score +
            0.3 * actionability_score
        )
        
        return final_score
    ```
    """)
    
    print_substep("5.2", "Load training data")
    print(f"""
    Training data format (JSONL):
    
    {{
        "question": "„Éç„Çπ„Å´Âãù„Å§„Å´„ÅØ„Å©„ÅÜ„Åô„Çå„Å∞„ÅÑ„ÅÑÔºü",
        "gold_answer": "„Çà„ÇäÂÖ∑‰ΩìÁöÑ„Å´„Ç≥„É≥„Éú„ÇíÊïô„Åà„Å¶",
        "timestamp": "2026-01-21T12:34:56.789Z",
        "improvements": ["specificity", "actionability"],
        "original_response": "„ÄêÂàÜÊûê„Äë... „Äê„Ç¢„Éâ„Éê„Ç§„Çπ„Äë...",
    }}
    
    Read with:
    
    ```python
    import json
    from pathlib import Path
    
    def load_training_data():
        jsonl_file = Path('data/training_data.jsonl')
        entries = []
        with open(jsonl_file, 'r') as f:
            for line in f:
                if line.strip():
                    entries.append(json.loads(line))
        return entries
    ```
    """)
    
    return True

def step6_run_teleprompter_optimization():
    """STEP 6: Run dspy.Teleprompter Optimization"""
    print_step(6, "Run dspy.Teleprompter Optimization",
               "Automatically tune prompts using collected corrections")
    
    print_substep("6.1", "Create optimization script")
    print(f"""
    Create: src/utils/optimize_coach.py
    
    This script will:
    ‚îú‚îÄ Load training data from JSONL
    ‚îú‚îÄ Initialize dspy.Teleprompter
    ‚îú‚îÄ Define optimization metrics
    ‚îú‚îÄ Run optimization trials (100+ iterations)
    ‚îú‚îÄ Save optimized coach state
    ‚îî‚îÄ Output new optimized prompts
    """)
    
    print_substep("6.2", "Run optimization")
    print(f"""
    $ python -m src.utils.optimize_coach
    
    What it does:
    ‚îú‚îÄ Loads {TRAINING_DATA_FILE}
    ‚îú‚îÄ Initializes SmashCoach model
    ‚îú‚îÄ Runs dspy.Teleprompter optimizer
    ‚îÇ  ‚îú‚îÄ Tries 100+ different prompts
    ‚îÇ  ‚îú‚îÄ Evaluates each with coaching_quality_metric
    ‚îÇ  ‚îú‚îÄ Selects best performing prompts
    ‚îÇ  ‚îî‚îÄ Returns optimized model
    ‚îú‚îÄ Saves state to: {OPTIMIZED_MODEL_FILE}
    ‚îî‚îÄ Displays before/after comparison
    
    ‚è±Ô∏è  Estimated time: 10-30 minutes (depends on data size)
    üí∞ API cost: ~$5-15 USD (100 trials √ó LLM calls)
    
    Progress indicator:
    Trial 1/100: quality=0.62
    Trial 2/100: quality=0.64
    ...
    Trial 100/100: quality=0.78 ‚úÖ (Best found)
    """)
    
    return True

def step7_validate_optimization():
    """STEP 7: Validate Optimization Results"""
    print_step(7, "Validate Optimization Results",
               "Test optimized model against held-out test set")
    
    print_substep("7.1", "Compare before/after")
    print(f"""
    Run validation:
    
    $ python -c "
from src.utils.optimize_coach import validate_optimization
validate_optimization()
    "
    
    Output example:
    
    BEFORE Optimization:
    ‚îú‚îÄ Average quality score: 0.65
    ‚îú‚îÄ Relevance: 0.70
    ‚îú‚îÄ Specificity: 0.62
    ‚îú‚îÄ Actionability: 0.63
    ‚îî‚îÄ Sample response: '„ÄêÂàÜÊûê„Äë... „Äê„Ç¢„Éâ„Éê„Ç§„Çπ„Äë...'
    
    AFTER Optimization:
    ‚îú‚îÄ Average quality score: 0.78 ‚úÖ (+20%)
    ‚îú‚îÄ Relevance: 0.85
    ‚îú‚îÄ Specificity: 0.75
    ‚îú‚îÄ Actionability: 0.74
    ‚îî‚îÄ Sample response: '„ÄêÂàÜÊûêÔºàË©≥Á¥∞Ôºâ„Äë... „Äê„Ç¢„Éâ„Éê„Ç§„ÇπÔºàÂÖ∑‰ΩìÁöÑÔºâ„Äë...'
    """)
    
    print_substep("7.2", "Test with new queries")
    print(f"""
    Manually test a few queries:
    
    $ python -c "
from src.brain.model import create_coach
coach = create_coach()
pred = coach.forward('Êñ∞„Åó„ÅÑ„Ç≠„É£„É©„Å´Âãù„Å§„Å´„ÅØÔºü')
print('Analysis:', pred.analysis)
print('Advice:', pred.advice)
    "
    
    Expected improvements:
    ‚îú‚îÄ More specific to character/move combos
    ‚îú‚îÄ More actionable steps („Ç≥„É≥„Éú‰æã„Å™„Å©)
    ‚îú‚îÄ Better structured output
    ‚îî‚îÄ Fewer generic statements
    """)
    
    return True

def step8_deploy_optimized_model():
    """STEP 8: Deploy Optimized Model"""
    print_step(8, "Deploy Optimized Model",
               "Replace production model with optimized version")
    
    print_substep("8.1", "Backup current model")
    print(f"""
    $ cp src/brain/model.py src/brain/model.py.backup
    """)
    
    print_substep("8.2", "Load optimized state")
    print(f"""
    Update src/main.py to load optimized model:
    
    ```python
    # In _run_coaching():
    from pathlib import Path
    import json
    
    optimized_state_file = Path('data/optimized_coach_state.json')
    
    if optimized_state_file.exists():
        # Load optimized coach
        coach = load_optimized_coach()  # Custom function
        print("‚úÖ Loaded optimized coach")
    else:
        # Fall back to default coach
        coach = create_coach()
        print("‚ö†Ô∏è  Using default coach (optimization not yet run)")
    ```
    """)
    
    print_substep("8.3", "Restart bot")
    print(f"""
    $ python src/main.py
    
    Bot now uses OPTIMIZED prompts
    ‚îî‚îÄ Users will see improved responses
    """)
    
    return True

def step9_collect_new_data():
    """STEP 9: Continuous Improvement Cycle"""
    print_step(9, "Continuous Improvement Cycle",
               "Collect new corrections and re-optimize periodically")
    
    print_substep("9.1", "Monitor quality degradation")
    print(f"""
    Set up monitoring to track coaching quality over time:
    
    Daily check:
    $ python -c "
from pathlib import Path
import json
from datetime import datetime, timedelta

jsonl = Path('data/training_data.jsonl')
lines = jsonl.read_text().strip().split('\\n')
entries = [json.loads(l) for l in lines if l]

# Find entries from last 24h
now = datetime.fromisoformat(datetime.now().isoformat())
recent = [e for e in entries 
          if datetime.fromisoformat(e['timestamp']) > now - timedelta(days=1)]

print(f'Corrections in last 24h: {{len(recent)}}')
print(f'Total: {{len(entries)}}')
    "
    """)
    
    print_substep("9.2", "Re-optimize schedule")
    print(f"""
    Recommended re-optimization schedule:
    
    Initial phase (Week 1-2):
    ‚îú‚îÄ Run optimization daily or every 2 days
    ‚îú‚îÄ Collect 5-10 new corrections before each run
    ‚îú‚îÄ Monitor quality improvements
    ‚îî‚îÄ Adjust metric weights based on results
    
    Stabilization phase (Week 3+):
    ‚îú‚îÄ Run optimization weekly
    ‚îú‚îÄ Collect 20-30 new corrections before each run
    ‚îú‚îÄ Monitor for quality regressions
    ‚îî‚îÄ Only deploy if score improves
    
    Mature phase (Month 2+):
    ‚îú‚îÄ Run optimization monthly
    ‚îú‚îÄ Collect 50+ new corrections before each run
    ‚îú‚îÄ Focus on edge cases and rare queries
    ‚îî‚îÄ Gradual quality improvement expected
    """)
    
    print_substep("9.3", "Set up automation (optional)")
    print(f"""
    Fully automated pipeline (using GitHub Actions or Cloud Tasks):
    
    Daily workflow:
    1. Check correction count
    2. If count > threshold:
       a. Run optimization
       b. Validate results
       c. Deploy if quality improved
       d. Notify admins
    3. Commit optimized state to GitHub
    4. Tag release with version number
    
    GitHub Actions example (.github/workflows/optimize.yml):
    
    name: Daily Coach Optimization
    on:
      schedule:
        - cron: '0 2 * * *'  # 2 AM daily
    
    jobs:
      optimize:
        runs-on: ubuntu-latest
        steps:
          - uses: actions/checkout@v3
          - name: Run optimization
            run: python -m src.utils.optimize_coach
          - name: Deploy if improved
            run: python -m src.utils.deploy_optimized
          - name: Commit changes
            run: |
              git add data/optimized_coach_state.json
              git commit -m "chore: Daily coach optimization"
              git push
    """)
    
    return True

def step10_monitoring_and_analysis():
    """STEP 10: Monitoring and Analysis"""
    print_step(10, "Monitoring and Analysis",
               "Track long-term improvement trends")
    
    print_substep("10.1", "Generate reports")
    print(f"""
    Weekly quality report:
    
    $ python -c "
from src.utils.analyze_coach_quality import generate_weekly_report
report = generate_weekly_report()
print(report)
    "
    
    Report includes:
    ‚îú‚îÄ Quality score trend (graph)
    ‚îú‚îÄ Most improved dimensions (relevance/specificity/actionability)
    ‚îú‚îÄ Most corrected query types
    ‚îú‚îÄ Least corrected query types (good performance!)
    ‚îú‚îÄ Recommended focus areas
    ‚îî‚îÄ Next optimization strategy
    """)
    
    print_substep("10.2", "Build feedback loop metrics")
    print(f"""
    Track over time:
    ‚îú‚îÄ /teach usage rate (corrections per day)
    ‚îú‚îÄ Quality score per query type
    ‚îú‚îÄ Query response latency
    ‚îú‚îÄ User satisfaction indicator (no /teach = satisfied)
    ‚îî‚îÄ New vs revisited queries
    
    Goal: Minimize /teach corrections over time
    Success: 80%+ of queries satisfy users on first response
    """)
    
    return True

def main():
    """Print complete optimization flow"""
    print("\n" + "="*70)
    print("üöÄ SmashZettel-Bot: Complete User Correction ‚Üí Auto-Optimization Flow")
    print("="*70)
    print("\nThis guide walks through the COMPLETE lifecycle:")
    print("FROM: Bot startup with initial prompts")
    print("TO:   Automatic prompt optimization after collecting user corrections")
    print("\n")
    
    steps = [
        step1_setup_environment,
        step2_ingest_knowledge_base,
        step3_launch_discord_bot,
        step4_monitor_training_data,
        step5_prepare_optimization,
        step6_run_teleprompter_optimization,
        step7_validate_optimization,
        step8_deploy_optimized_model,
        step9_collect_new_data,
        step10_monitoring_and_analysis,
    ]
    
    for step_func in steps:
        step_func()
    
    print("\n" + "="*70)
    print("‚úÖ COMPLETE FLOW DOCUMENTED")
    print("="*70)
    print("""
Next actions:
1. Follow STEP 1-2 to set up (one time)
2. Run STEP 3 to start bot (ongoing)
3. Wait for STEP 4 (collect corrections)
4. When ready, run STEP 5-6 (optimize)
5. Deploy STEP 7-8 (use optimized model)
6. Repeat STEP 9 for continuous improvement

üìä Key metrics to track:
   - Correction count: target 30+ for first optimization
   - Quality score: target +20% improvement after optimization
   - /teach usage: target decreasing over time
   - Response latency: monitor for increases

Time estimates:
   - Setup (Step 1-2): 30 minutes
   - Bot running (Step 3): Ongoing
   - Data collection (Step 4): 3-7 days
   - Optimization (Step 6): 10-30 minutes
   - Deploy (Step 8): 5 minutes
   - Total first cycle: ~1-2 weeks

üí∞ API costs:
   - Knowledge ingestion: ~$1-3
   - Optimization (100 trials): ~$5-15
   - Ongoing: ~$0.10-0.50 per optimization run

Good luck! üéâ
""")

if __name__ == '__main__':
    main()
