# Curriculum v3 設計戦略

**日時**: 2026-02-09  
**目的**: SaaS化・自動化を見据えた「動的アルゴリズム」としてのカリキュラム設計

---

## 🎯 選択したアプローチ

### **アプローチ3：ユーザー状態遷移マシン（State Machine）から始める**

**判断理由**: システム実装視点での逆算設計が、SaaS化最適化につながる

---

## 📊 3つのアプローチ比較分析

| | アプローチ1（Prerequisites中心） | アプローチ2（勝利条件中心） | アプローチ3（State Machine中心）|
|---|---|---|---|
| **設計の開始点** | 「何を理解すべきか」から | 「何で勝つか」から | 「ユーザーがどう動くか」から |
| **構造** | ツリー構造（階層的依存） | 逆ツリー構造（目的から逆算） | グラフ構造（状態遷移）|
| **実装難易度** | 中（明確だが、パターンが多い） | 高（定義が曖昧になりやすい） | 中（複雑だが、明確） |
| **SaaS化への適性** | △（既存LMS的） | △（カスタマイズ性低） | ◎（動的、拡張性高） |
| **フィードバック閾値** | △（ユーザー入力を後付け） | ✗（定義変更が全体に波及） | ◎（状態を追加するだけ） |
| **自動化効率** | △（ロジックが散在） | ✗（中央集約が難） | ◎（エンジン化が簡単） |

---

## 🔄 アプローチ3の詳細：State Machine設計

### コンセプト

**ユーザーの学習状態を「状態」として定義し、入力（敗北、成功、質問）に応じて次の状態に遷移する。**

```
【状態の例】
- S0: 初期状態（未学習）
- S1-A: ステップ1の基本操作習得中
- S1-B: ステップ1のゲームシステム理解中
- S2-A: ステップ2（前提：S1-AかつS1-Bクリア）
- ...

【遷移の例】
S1-A → [CPU相手コンボ成功] → S1-A_OK（ロック解除）
S1-A_OK + S1-B_OK → S2-A_READY（ステップ2進行可能）

S2-A → [CPU着地狩り失敗] → S2-A_RETRY（復習推奨）
S2-A → [同じ失敗パターン3回] → S1-Aへ逆戻り（基礎補強）
```

### 3つの要素

#### 1. **状態（State）**
```
State = {
  step: int（1-6）,
  sub_phase: str（A, B, C...）,
  status: str（IN_PROGRESS, COMPLETED, FAILED, LOCKED）,
  mastery_level: float（0.0-1.0）,
  failure_count: int（同じパターンの失敗回数）
}
```

#### 2. **遷移ルール（Transition Rules）**
```
Rule = {
  from_state: State,
  trigger: str（"combat_result", "user_query", "timeout"）,
  condition: bool（判定ロジック）,
  to_state: State,
  action: str（推奨ドリル、フィードバック内容）
}
```

#### 3. **勝利条件との接続（Victory Condition Mapping）**
```
Victory = {
  definition: "相手の選択肢を期待値最小化まで圧縮する",
  measurable: {
    "期待値差": float（自分のEV - 相手のEV）,
    "選択肢削減率": float（相手の利得状態を最小化した比率）
  },
  per_state: {
    S1: "基本操作の安定性（エラー率<5%）",
    S2: "着地狩り成功率 > 60%",
    S3: "ライン管理の支配度（相手の移動選択肢<3）",
    S4: "確定反撃の確度（フレーム計算の正確度>95%）",
    S5: "マッチアップ有利評価の的中率>70%",
    S6: "相手パターン仮説の予測精度>75%"
  }
}
```

---

## 💡 このアプローチがもたらす効果

### 効果1：**ロジックの一元化**
- すべての学習ロジックが「状態遷移」という単一の概念で統一
- Zettelデータベースとの接続点が明確化
- バグの原因特定が容易（「どの遷移ルールが不適切か」に絞れる）

### 効果2：**動的なリアルタイム意思決定**
```
入力: ユーザーの戦績データ
→ 現在状態を計算
→ 遷移ルールを適用
→ 推奨ドリルを動的に出力

例：
入力: "Fox相手で着地狩り失敗 × 3回"
→ 状態: S3_FOX_FAILOVER（Foxの着地パターンの学習失敗）
→ 推奨: "ステップ4で Foxの着地フレームを詳細学習"
→ その後: "ステップ3に戻って空間管理の応用練習"
```

### 効果3：**フィードバック無限ループの実装**
```
敗北データ → 状態遷移に反映
→ ユーザーの学習パスが自動調整
→ 次回の推奨ドリルに反映
→ システムが自律的に改善

例：
「特定の状況（崖上がりで敵キャラがX）で失敗」
→ このパターンが記録される
→ 次のユーザーにも「この失敗パターン」が推奨ドリルとして出現
→ SZ理論自体が洗練される
```

### 効果4：**SaaS化への直結**
```
Backend：State Machine エンジン
  ↓
Database：各ユーザーの状態+敗北パターン
  ↓
API：次のドリル推奨を返す
  ↓
Frontend：ユーザーが見るドリル + フィードバック画面

→ すべてが「状態遷移」という単位で設計されているので、
  スケーリングが容易（ユーザー数 × 状態管理）
```

### 効果5：**Prerequisites と勝利条件の自動最適化**
```
v2では「手作業で決めた」Prerequisites と勝利条件
→ v3では「State Machine の遷移ルール」として実装
→ ユーザーデータから「実際の最適な遷移」を学習
→ アルゴリズムが自動的に Prerequisites を調整

例：
当初：「S1をクリアしたら S2へ」
実際データ：「S1と並行してS2を学んだ方が勝率が高い」
→ 自動的に遷移ルールを修正
```

---

## 🛠️ v3の具体的な出力物

### 1. **State Definition Spec**
- 全状態の列挙（S0～S6の全48状態程度）
- 各状態の入退条件
- 各状態での「脳報酬」の説明

### 2. **Transition Rules Spec**
- 遷移ルールの有向グラフ（視覚化）
- 各ルールのトリガー条件と判定ロジック
- フォールバック（失敗時の逆戻り）パス

### 3. **Victory Condition Mapping**
- 各ステップの「勝利」を定量的に定義
- 期待値計算式の実装例
- 選択肢削減率の測定方法

### 4. **Zettel ↔ State Mapping**
- 各状態で参照すべき Zettel の紐づけ
- 状態遷移に応じた Zettel の推奨順序
- Pinecone からの自動検索ロジック

### 5. **Implementation Guide**
- State Machine エンジンの疑似コード
- Backend（Python/Node.js）での実装例
- API スキーマ（入力・出力）

---

## 📈 このアプローチの進化ポテンシャル

| フェーズ | 実装 | 効果 |
|---|---|---|
| **Phase 1（現在）** | State Machine 定義 | 論理の明確化 |
| **Phase 2** | ルール自動調整 | ユーザーデータから勝率向上パスを検出 |
| **Phase 3** | マルチプレイヤー連携 | 複数プレイヤーの勝率データから「メタ理論」を自動更新 |
| **Phase 4** | 新キャラ対応 | 新キャラ追加時に State の追加パターンを自動生成 |
| **Phase 5** | クロスゲーム拡張 | スマブラ以外のゲーム（格ゲー等）への自動適用 |

---

## 🎓 SZ理論の「多層構造」との接続

v3 の State Machine は、SZ理論の「再帰的思考」を実装した形式：

```
層1（機械的）: State定義（今このユーザーは何ができるのか）
層2（戦術的）: Transition（そこから何を学ぶべきか）
層3（相手読み）: Victory定義（相手もこれを知ってるか）
層4（メタゲーム）: System学習（システムが自分たちの学習を学習）
```

---

## ✅ 次のステップ

v3 の実装を開始します：

1. **State空間の完全定義**（S0～S6の全遷移）
2. **Transition Rules の有向グラフ化**
3. **勝利条件の数学的定義**（期待値計算式）
4. **Zettel マッピングの自動化**
5. **実装可能な形式での仕様書出力**

---

**実行エンジンの判断：このアプローチにより、カリキュラムが「教科書」から「自律的アルゴリズム」へと昇華する。**
