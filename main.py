import sys
import subprocess
import os
import time
import json
import shutil
import glob
import re
from datetime import datetime

# --- 0. ç’°å¢ƒã®å¼·åˆ¶æ­£è¦åŒ– (Environment Force Update) ---
# GitHub Actionsã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ±šæŸ“å¯¾ç­–ã¨ã—ã¦ã€å®Ÿè¡Œæ™‚ã«æœ€æ–°ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’å¼·åˆ¶ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
try:
    subprocess.check_call([sys.executable, "-m", "pip", "install", "google-generativeai>=0.7.2"])
except:
    pass

# --- Libraries ---
import requests
import google.generativeai as genai
from groq import Groq
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
import zipfile

# --- Configuration ---
FINAL_CONTROL_DB_ID = "2b71bc8521e380868094ec506b41f664"
FINAL_FALLBACK_DB_ID = "2b71bc8521e38018a5c3c4b0c6b6627c"
TEMP_DIR = "temp_workspace"
CHUNK_LENGTH = 900  # 15åˆ†

# --- 1. åˆæœŸåŒ– (Setup) ---
def setup_env():
    if os.path.exists(TEMP_DIR): shutil.rmtree(TEMP_DIR)
    os.makedirs(TEMP_DIR)
    if os.getenv("GCP_SA_KEY"):
        with open("service_account.json", "w") as f:
            f.write(os.getenv("GCP_SA_KEY"))

setup_env()

try:
    groq_client = Groq(api_key=os.getenv("GROQ_API_KEY"))
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
    NOTION_TOKEN = os.getenv("NOTION_TOKEN")
    HEADERS = {"Authorization": f"Bearer {NOTION_TOKEN}", "Content-Type": "application/json", "Notion-Version": "2022-06-28"}
    creds = service_account.Credentials.from_service_account_file("service_account.json", scopes=['https://www.googleapis.com/auth/drive'])
    drive_service = build('drive', 'v3', credentials=creds)
    INBOX_FOLDER_ID = os.getenv("DRIVE_FOLDER_ID")
except Exception as e:
    print(f"âŒ Init Error: {e}"); sys.exit(1)

def sanitize_id(raw_id):
    if not raw_id: return None
    match = re.search(r'([a-fA-F0-9]{32})', str(raw_id).replace("-", ""))
    return match.group(1) if match else None

# --- 2. éŸ³å£°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ (Audio Pipeline) ---

def mix_audio_ffmpeg(file_paths):
    print(f"ğŸ›ï¸ Mixing {len(file_paths)} tracks...", flush=True)
    output_path = os.path.abspath(os.path.join(TEMP_DIR, "final_mix.mp3"))
    inputs = []
    for f in file_paths: inputs.extend(['-i', f])
    
    filter_part = ['-filter_complex', f'amix=inputs={len(file_paths)}:duration=longest'] if len(file_paths) > 1 else []
    cmd = ['ffmpeg', '-y'] + inputs + filter_part + ['-ac', '1', '-b:a', '64k', output_path]
    subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    return output_path

def split_audio_ffmpeg(input_path):
    print("ğŸ”ª Splitting...", flush=True)
    output_pattern = os.path.join(TEMP_DIR, "chunk_%03d.mp3")
    cmd = ['ffmpeg', '-y', '-i', input_path, '-f', 'segment', '-segment_time', str(CHUNK_LENGTH), '-ac', '1', '-b:a', '64k', output_pattern]
    subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    return sorted(glob.glob(os.path.join(TEMP_DIR, "chunk_*.mp3")))

def transcribe_with_groq(chunk_paths):
    """
    [Smart Retryæ­è¼‰]
    Rate Limit (429) ãŒç™ºç”Ÿã—ãŸå ´åˆã€ã‚¨ãƒ©ãƒ¼ã§æ­¢ã‚ãšã«è‡ªå‹•å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤ã™ã‚‹ã€‚
    """
    full_transcript = ""
    for chunk in chunk_paths:
        if not chunk.endswith(".mp3"): continue
        print(f"ğŸš€ Groq Transcribing: {os.path.basename(chunk)}", flush=True)
        
        max_retries = 5
        for attempt in range(max_retries):
            try:
                with open(chunk, "rb") as file:
                    res = groq_client.audio.transcriptions.create(
                        file=(os.path.basename(chunk), file),
                        model="whisper-large-v3", language="ja", response_format="text"
                    )
                full_transcript += res + "\n"
                break # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹
            except Exception as e:
                err_str = str(e).lower()
                if "429" in err_str or "rate limit" in err_str:
                    wait = 20 * (2 ** attempt) # 20s, 40s, 80s, 160s, 320s
                    print(f"â³ Rate Limit Hit. Waiting {wait}s... (Attempt {attempt+1}/{max_retries})", flush=True)
                    time.sleep(wait)
                else:
                    raise e # 429ä»¥å¤–ã¯å³åº§ã«ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦æŠ•ã’ã‚‹
        else:
            raise Exception("âŒ Max retries exceeded for Rate Limit.")

    return full_transcript

# --- 3. çŸ¥èƒ½åˆ†æ (Analysis Phase) ---

def analyze_text_with_gemini(transcript_text):
    print("ğŸ§  Gemini Analyzing (SZ Method)...", flush=True)
    # ãƒãƒ¼ã‚¸ãƒ§ãƒ³å•é¡Œè§£æ±ºæ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«åã‚’æŒ‡å®š
    model = genai.GenerativeModel('gemini-1.5-flash')
    
    # ã€é‡è¦ã€‘è¦ç´„ã›ãšã€SZãƒ¡ã‚½ãƒƒãƒ‰ã®å…¨å®¹ã‚’è¨˜è¿°ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    prompt = f"""
    ã‚ãªãŸã¯ä¸–ç•Œæœ€é«˜å³°ã®ã‚¹ãƒãƒ–ãƒ©ï¼ˆSuper Smash Bros.ï¼‰ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã‚ã‚Šã€è«–ç†çš„ã‹ã¤å†·å¾¹ãªã‚³ãƒ¼ãƒãƒ³ã‚°è¨˜éŒ²å®˜ã§ã™ã€‚
    æ¸¡ã•ã‚ŒãŸå¯¾è©±ãƒ­ã‚°ã‚’ç²¾èª­ã—ã€ä»¥ä¸‹ã®3ã¤ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å³å¯†ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

    **ã€Section 1: ãƒˆãƒ”ãƒƒã‚¯åˆ¥ãƒ»è©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆã€‘**
    ä¼šè©±ã®ä¸­ã§æ‰±ã‚ã‚ŒãŸã€Œä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯ï¼ˆä¾‹ï¼šå´–ä¸ŠãŒã‚Šç‹©ã‚Šã€ãƒ©ã‚¤ãƒ³ç®¡ç†ã€å¾©å¸°é˜»æ­¢ï¼‰ã€ã‚’ã™ã¹ã¦æŠ½å‡ºã—ã€
    **ãƒˆãƒ”ãƒƒã‚¯ã”ã¨ã«**ä»¥ä¸‹ã®5è¦ç´ ã‚’åŸ‹ã‚ã¦è¨˜è¿°ã™ã‚‹ã“ã¨ã€‚

    * **â‘  ç¾çŠ¶ (Status):** ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒç¾åœ¨è¡Œã£ã¦ã„ã‚‹è¡Œå‹•ã€ç™–ã€èªè­˜ã—ã¦ã„ã‚‹çŠ¶æ³ã€‚
    * **â‘¡ èª²é¡Œ (Problem):** ãã®è¡Œå‹•ã«ã‚ˆã£ã¦ç™ºç”Ÿã—ã¦ã„ã‚‹å…·ä½“çš„ãªãƒ‡ãƒ¡ãƒªãƒƒãƒˆã€‚
    * **â‘¢ åŸå›  (Root Cause):** ãªãœãã®èª²é¡ŒãŒç™ºç”Ÿã—ã¦ã„ã‚‹ã®ã‹ï¼ˆçŸ¥è­˜ä¸è¶³ã€æ“ä½œãƒŸã‚¹ã€åˆ¤æ–­ãƒŸã‚¹ç­‰ï¼‰ã€‚
    * **â‘£ æ”¹å–„æ¡ˆ (Solution):** å…·ä½“çš„ã«ã©ã†è¡Œå‹•ã‚’å¤‰ãˆã‚‹ã¹ãã‹ï¼ˆæŠ€ã®å¤‰æ›´ã€ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã€æ„è­˜é…åˆ†ï¼‰ã€‚
    * **â‘¤ ã‚„ã‚‹ã“ã¨ (Next Action):** æ¬¡å›ã®ãƒ—ãƒ¬ã‚¤ã§å³åº§ã«å®Ÿè¡Œã™ã¹ãã€å…·ä½“çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆ1è¡Œï¼‰ã€‚

    **ã€Section 2: æ™‚ç³»åˆ—ãƒ­ã‚°ã€‘**
    ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æµã‚Œã‚’æ™‚ç³»åˆ—ï¼ˆTime-Seriesï¼‰ã§è©³ç´°ã«ç®‡æ¡æ›¸ãã«ã™ã‚‹ã“ã¨ã€‚

    **ã€Section 3: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSONã€‘**
    ä»¥ä¸‹ã®JSONã®ã¿ã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨ã€‚
    {{
      "student_name": "ç”Ÿå¾’ã®åå‰ï¼ˆä¸æ˜ãªã‚‰Unknownï¼‰",
      "date": "YYYY-MM-DD",
      "next_action": "æœ€ã‚‚å„ªå…ˆåº¦ã®é«˜ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³1ã¤"
    }}

    ---
    **[DETAILED_REPORT_START]**
    (ã“ã“ã«Section 1ã‚’å‡ºåŠ›)
    **[DETAILED_REPORT_END]**

    **[RAW_LOG_START]**
    (ã“ã“ã«Section 2ã‚’å‡ºåŠ›)
    **[RAW_LOG_END]**

    **[JSON_START]**
    (ã“ã“ã«Section 3ã‚’å‡ºåŠ›)
    **[JSON_END]**
    ---

    ã€å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã€‘
    {transcript_text[:950000]}
    """
    
    response = model.generate_content(prompt)
    text = response.text.strip()
    
    def extract(s, e, src):
        m = re.search(f'{re.escape(s)}(.*?){re.escape(e)}', src, re.DOTALL)
        return m.group(1).strip() if m else ""

    report = extract("[DETAILED_REPORT_START]", "[DETAILED_REPORT_END]", text)
    time_log = extract("[RAW_LOG_START]", "[RAW_LOG_END]", text)
    json_str = extract("[JSON_START]", "[JSON_END]", text)
    
    try: data = json.loads(json_str)
    except: data = {"student_name": "Unknown", "date": datetime.now().strftime('%Y-%m-%d'), "next_action": "Review Logs"}
    return data, report, time_log

# --- 4. è³‡ç”£åŒ– (Notion & Drive) ---

def notion_query_student(name):
    db_id = sanitize_id(FINAL_CONTROL_DB_ID)
    if not db_id: return None, name
    res = requests.post(f"https://api.notion.com/v1/databases/{db_id}/query", headers=HEADERS, json={"filter": {"property": "Name", "title": {"contains": name}}})
    d = res.json()
    if d.get("results"):
        row = d["results"][0]
        n = row["properties"]["Name"]["title"][0]["plain_text"]
        tid = row["properties"]["TargetID"]["rich_text"]
        return (sanitize_id(tid[0]["plain_text"]), n) if tid else (None, n)
    return None, name

def notion_create_page_heavy(db_id, props, children):
    res = requests.post("https://api.notion.com/v1/pages", headers=HEADERS, json={"parent": {"database_id": db_id}, "properties": props, "children": children[:100]})
    pid = res.json().get('id')
    if pid and len(children) > 100:
        for i in range(100, len(children), 100):
            requests.patch(f"https://api.notion.com/v1/blocks/{pid}/children", headers=HEADERS, json={"children": children[i:i+100]})

def cleanup_drive_file(file_id, rename_to):
    q = f"name='processed_coaching_logs' and '{INBOX_FOLDER_ID}' in parents"
    folders = drive_service.files().list(q=q).execute().get('files', [])
    fid = folders[0]['id'] if folders else drive_service.files().create(body={'name': 'processed_coaching_logs', 'mimeType': 'application/vnd.google-apps.folder', 'parents': [INBOX_FOLDER_ID]}, fields='id').execute().get('id')
    
    prev = ",".join(drive_service.files().get(fileId=file_id, fields='parents').execute().get('parents', []))
    drive_service.files().update(fileId=file_id, addParents=fid, removeParents=prev, body={'name': rename_to}).execute()
    print(f"âœ… Drive updated: {rename_to}")

# --- Main Logic ---

def main():
    print("--- SZ AUTO LOGGER ULTIMATE (v79.0) ---", flush=True)
    
    files = drive_service.files().list(q=f"'{INBOX_FOLDER_ID}' in parents and trashed=false and mimeType!='application/vnd.google-apps.folder'").execute().get('files', [])
    
    if not files:
        print("â„¹ï¸ No files found.")
        return

    for file in files:
        try:
            print(f"\nğŸ“‚ Processing: {file['name']}")
            fpath = os.path.join(TEMP_DIR, file['name'])
            with open(fpath, "wb") as f:
                MediaIoBaseDownload(f, drive_service.files().get_media(fileId=file['id'])).next_chunk()
            
            # Source Filter
            srcs = []
            if file['name'].endswith('.zip'):
                with zipfile.ZipFile(fpath, 'r') as z:
                    z.extractall(TEMP_DIR)
                    for r, _, fs in os.walk(TEMP_DIR):
                        for af in fs:
                            if af.lower().endswith(('.flac', '.mp3', '.m4a', '.wav')) and 'final_mix' not in af and 'chunk' not in af:
                                srcs.append(os.path.join(r, af))
            else: srcs.append(fpath)
            
            if not srcs: continue
            
            # Process with Smart Retry
            mixed = mix_audio_ffmpeg(srcs)
            chunks = split_audio_ffmpeg(mixed)
            full_text = transcribe_with_groq(chunks)
            meta, report, logs = analyze_text_with_gemini(full_text)
            
            # Notion
            did, oname = notion_query_student(meta['student_name'])
            if not did: did = FINAL_FALLBACK_DB_ID
            
            props = {"åå‰": {"title": [{"text": {"content": f"{meta['date']} {oname} ãƒ­ã‚°"}}]}, "æ—¥ä»˜": {"date": {"start": meta['date']}}}
            content = f"### ğŸ“Š SZãƒ¡ã‚½ãƒƒãƒ‰è©³ç´°åˆ†æ\n\n{report}\n\n---\n### ğŸ“ æ™‚ç³»åˆ—ãƒ­ã‚°\n\n{logs}"
            blocks = []
            for line in content.split('\n'):
                if line.strip():
                    blocks.append({"object": "block", "type": "paragraph", "paragraph": {"rich_text": [{"text": {"content": line[:1900]}}]}})
            
            notion_create_page_heavy(sanitize_id(did), props, blocks)
            
            # Cleanup
            ext = os.path.splitext(file['name'])[1] or ".zip"
            cleanup_drive_file(file['id'], f"{meta['date']}_{oname}{ext}")

        except Exception as e:
            # ã€ç·Šæ€¥åœæ­¢ã€‘è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼æ™‚ã®ã¿ç™ºå‹•
            print(f"âŒ CRITICAL ERROR on {file['name']}: {e}")
            import traceback; traceback.print_exc()
            print("â›” ã‚·ã‚¹ãƒ†ãƒ ã‚’ç·Šæ€¥åœæ­¢ã—ã¾ã™ã€‚")
            sys.exit(1)
            
        finally:
            if os.path.exists(TEMP_DIR): shutil.rmtree(TEMP_DIR); os.makedirs(TEMP_DIR)

if __name__ == "__main__":
    main()
