import sys
import subprocess
import os
import time
import json
import shutil
import glob
import re
import traceback
import random
import copy
import difflib
from datetime import datetime, timedelta, timezone

# --- 0. SDK & Tools ---
def install_package(package):
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    except: pass

install_package("google-genai")
install_package("groq")
install_package("patool")

# --- Libraries ---
import requests
from google import genai 
from google.genai import types
from groq import Groq
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload
from googleapiclient.errors import HttpError
import patoolib

# --- Configuration ---
FINAL_CONTROL_DB_ID = "2b71bc8521e380868094ec506b41f664"
FINAL_FALLBACK_DB_ID = "2e01bc8521e380ffaf28c2ab9376b00d"
TEMP_DIR = "temp_workspace"
CHUNK_LENGTH = 900  # 15 min

# Global Variables
RESOLVED_MODEL_ID = None
BOT_EMAIL = None
STUDENT_REGISTRY = {}
COMMON_TERMS = ""

# Try to load glossary
try:
    from smash_glossary import COMMON_TERMS
except ImportError:
    pass

# --- Helper: Verbose Error Printer ---
def log_error(context, error_obj):
    if isinstance(error_obj, HttpError) and "storageQuotaExceeded" in str(error_obj):
        print(f"‚ö†Ô∏è [Quota Limit] Could not upload artifact ({context}). Skipping.", flush=True)
    else:
        print(f"\n‚ùå [ERROR] {context}", flush=True)
        print(f"   Details: {str(error_obj)}", flush=True)
        print("-" * 30, flush=True)

# --- 1. Initialization (Setup) ---
def setup_env():
    global RESOLVED_MODEL_ID, BOT_EMAIL
    if os.path.exists(TEMP_DIR): shutil.rmtree(TEMP_DIR)
    os.makedirs(TEMP_DIR)
    
    sa_key = os.getenv("GCP_SA_KEY")
    if sa_key:
        with open("service_account.json", "w") as f:
            f.write(sa_key)
        try:
            key_data = json.loads(sa_key)
            BOT_EMAIL = key_data.get("client_email", "Unknown")
            print(f"\n==========================================")
            print(f"ü§ñ BOT EMAIL: {BOT_EMAIL}")
            print(f"üëâ Ensure this email is an 'Editor' of the folder!")
            print(f"==========================================\n", flush=True)
        except: pass
    else:
        print("‚ùå ENV Error: GCP_SA_KEY is missing.")
        sys.exit(1)

setup_env()

try:
    groq_client = Groq(api_key=os.getenv("GROQ_API_KEY"))
    gemini_client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
    
    print("üíé Detecting Best Available Model...", flush=True)
    
    PRIORITY_TARGETS = ["gemini-2.0-flash", "gemini-1.5-pro", "gemini-1.5-flash"]
    for target in PRIORITY_TARGETS:
        print(f"üëâ Testing: [{target}]...", flush=True)
        try:
            gemini_client.models.generate_content(model=target, contents="Hello")
            print(f"‚úÖ SUCCESS! Using Model: [{target}]", flush=True)
            RESOLVED_MODEL_ID = target
            break
        except Exception: continue
                
    if not RESOLVED_MODEL_ID:
        RESOLVED_MODEL_ID = "gemini-1.5-flash"
        print(f"‚ö†Ô∏è All checks failed. Forcing Fallback to: {RESOLVED_MODEL_ID}")

    NOTION_TOKEN = os.getenv("NOTION_TOKEN")
    if not NOTION_TOKEN: raise Exception("NOTION_TOKEN missing")
    
    HEADERS = {"Authorization": f"Bearer {NOTION_TOKEN}", "Content-Type": "application/json", "Notion-Version": "2022-06-28"}
    creds = service_account.Credentials.from_service_account_file("service_account.json", scopes=['https://www.googleapis.com/auth/drive'])
    drive_service = build('drive', 'v3', credentials=creds)
    INBOX_FOLDER_ID = os.getenv("DRIVE_FOLDER_ID")
    if not INBOX_FOLDER_ID: raise Exception("DRIVE_FOLDER_ID missing")
    
except Exception as e:
    log_error("Initialization Failed", e)
    sys.exit(1)

def sanitize_id(raw_id):
    if not raw_id: return None
    match = re.search(r'([a-fA-F0-9]{32})', str(raw_id).replace("-", ""))
    return match.group(1) if match else None

# --- Logic: Registry & Fuzzy Match ---

def load_student_registry():
    global STUDENT_REGISTRY
    print("üìã Loading Student Registry from Notion...", flush=True)
    db_id = sanitize_id(FINAL_CONTROL_DB_ID)
    if not db_id: return

    has_more = True
    next_cursor = None
    count = 0

    while has_more:
        payload = {"page_size": 100}
        if next_cursor: payload["start_cursor"] = next_cursor
        
        try:
            res = requests.post(f"https://api.notion.com/v1/databases/{db_id}/query", headers=HEADERS, json=payload)
            if res.status_code != 200: break
            data = res.json()
            for row in data.get("results", []):
                try:
                    name_list = row["properties"]["Name"]["title"]
                    if not name_list: continue
                    name = name_list[0]["plain_text"]
                    tid_list = row["properties"]["TargetID"]["rich_text"]
                    tid = sanitize_id(tid_list[0]["plain_text"]) if tid_list else None
                    if name and tid:
                        STUDENT_REGISTRY[name] = tid
                        count += 1
                except: continue
            has_more = data.get("has_more", False)
            next_cursor = data.get("next_cursor")
        except Exception as e: break
    print(f"‚úÖ Loaded {count} students into registry.", flush=True)

def find_best_student_match(query_name):
    if not query_name or not STUDENT_REGISTRY: return None, query_name
    if query_name in STUDENT_REGISTRY: return STUDENT_REGISTRY[query_name], query_name
    matches = difflib.get_close_matches(query_name, list(STUDENT_REGISTRY.keys()), n=1, cutoff=0.4)
    if matches:
        print(f"üéØ Fuzzy Match: '{query_name}' -> '{matches[0]}'", flush=True)
        return STUDENT_REGISTRY[matches[0]], matches[0]
    return None, query_name

# --- Logic: Metadata Helpers ---

def sanitize_filename(filename):
    return filename.replace("/", "_").replace("\\", "_")

def get_jst_now():
    return datetime.now(timezone(timedelta(hours=9)))

def extract_date_smart(filename, drive_created_time_iso):
    match = re.search(r'(\d{4}-\d{2}-\d{2})_(\d{1,2}-\d{1,2}-\d{1,2})', filename)
    if match:
        d_part = match.group(1)
        t_part = match.group(2).replace('-', ':')
        t_parts = t_part.split(':')
        t_formatted = f"{int(t_parts[0]):02}:{int(t_parts[1]):02}:{int(t_parts[2]):02}"
        return f"{d_part} {t_formatted}", d_part
    
    if drive_created_time_iso:
        try:
            dt = datetime.fromisoformat(drive_created_time_iso.replace('Z', '+00:00'))
            dt_jst = dt.astimezone(timezone(timedelta(hours=9)))
            return dt_jst.strftime('%Y-%m-%d %H:%M:%S'), dt_jst.strftime('%Y-%m-%d')
        except: pass

    now_jst = get_jst_now()
    return now_jst.strftime('%Y-%m-%d %H:%M:%S'), now_jst.strftime('%Y-%m-%d')

def detect_student_candidate_raw(file_list, original_archive_name):
    strong_candidates = []
    weak_candidates = []
    ignore_files = ["raw.dat", "info.txt", "ds_store", "thumbs.db", "desktop.ini", "readme", "license"]
    ignore_names = ["hikari", "craig", "entrymonster", "bot", "ssb"]

    print("üîé Scanning internal files for student ID hint...", flush=True)
    for f in file_list:
        basename = os.path.basename(f).lower()
        if any(ign in basename for ign in ignore_files): continue
        name_part = os.path.splitext(basename)[0]
        craig_match = re.match(r'^\d+-(.+)', name_part)
        candidate = craig_match.group(1) if craig_match else name_part
        if any(ign in candidate for ign in ignore_names): continue
        if len(candidate) < 2: continue

        if craig_match: strong_candidates.append(candidate)
        else: weak_candidates.append(candidate)

    final = strong_candidates if strong_candidates else weak_candidates
    if not final:
        base = os.path.basename(original_archive_name)
        name_cleaned = re.sub(r'\.zip|\.flac|\.mp3|\.wav', '', base, flags=re.IGNORECASE)
        name_cleaned = re.sub(r'\d{4}-\d{2}-\d{2}', '', name_cleaned)
        if len(name_cleaned) > 2: final = [name_cleaned]

    if final:
        hint = ", ".join(sorted(list(set(final))))
        print(f"üí° Found Student Hint: {hint}", flush=True)
        return hint
    return None

# --- 3. Audio Pipeline ---

def run_ffmpeg_command(cmd, task_name):
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        print(f"\n‚ùå FFmpeg Error during '{task_name}':\n{e.stderr}", flush=True)
        raise e

def mix_audio_ffmpeg(file_paths):
    print(f"üéõÔ∏è Mixing {len(file_paths)} tracks...", flush=True)
    output_path = os.path.abspath(os.path.join(TEMP_DIR, "final_mix.mp3"))
    inputs = []
    valid_files = [f for f in file_paths if f.lower().endswith(('.mp3', '.wav', '.flac', '.m4a', '.aac'))]
    if not valid_files: raise Exception("No audio files.")
    for f in valid_files: inputs.extend(['-i', f])
    filter_part = ['-filter_complex', f'amix=inputs={len(valid_files)}:duration=longest'] if len(valid_files) > 1 else []
    cmd = ['ffmpeg', '-y'] + inputs + filter_part + ['-ac', '1', '-b:a', '64k', output_path]
    run_ffmpeg_command(cmd, "Mixing Audio")
    return output_path

def split_audio_ffmpeg(input_path):
    print("üî™ Splitting...", flush=True)
    output_pattern = os.path.join(TEMP_DIR, "chunk_%03d.mp3")
    cmd = ['ffmpeg', '-y', '-i', input_path, '-f', 'segment', '-segment_time', str(CHUNK_LENGTH), '-ac', '1', '-b:a', '64k', output_pattern]
    run_ffmpeg_command(cmd, "Splitting Audio")
    return sorted(glob.glob(os.path.join(TEMP_DIR, "chunk_*.mp3")))

def transcribe_with_groq(chunk_paths):
    full_transcript = ""
    for chunk in chunk_paths:
        if not chunk.endswith(".mp3"): continue
        print(f"üöÄ Groq Transcribing: {os.path.basename(chunk)}", flush=True)
        max_retries = 50
        for attempt in range(max_retries):
            try:
                with open(chunk, "rb") as file:
                    res = groq_client.audio.transcriptions.create(
                        file=(os.path.basename(chunk), file),
                        model="whisper-large-v3", language="ja", response_format="text"
                    )
                full_transcript += res + "\n"
                break 
            except Exception as e:
                err_str = str(e).lower()
                if "429" in err_str or "rate limit" in err_str:
                    wait = 70
                    print(f"‚è≥ Groq Limit. Waiting {wait}s... ({attempt+1}/{max_retries})", flush=True)
                    time.sleep(wait)
                else: 
                    log_error("Groq Transcription Failed", e)
                    raise e
        else: raise Exception("‚ùå Groq Rate Limit persists. Aborting.")
    return full_transcript

# --- 4. Intelligence Analysis (Expert Mode) ---

def analyze_text_with_gemini(transcript_text, date_hint, raw_name_hint):
    print(f"üß† Gemini Analyzing using [{RESOLVED_MODEL_ID}]...", flush=True)
    
    hint_context = f"Èå≤Èü≥Êó•ÊôÇ: {date_hint}"
    if raw_name_hint:
        hint_context += f"\n„ÄêÈáçË¶Å„Äë„Éï„Ç°„Ç§„É´Âêç„Éí„É≥„Éà: '{raw_name_hint}' („Åì„Çå„ÇíÊúÄÂÑ™ÂÖà„ÅßÁîüÂæíÂêç„Å®„Åó„Å¶Êé°Áî®„Åõ„Çà)"
    
    glossary_instruction = ""
    if COMMON_TERMS:
        glossary_instruction = f"\n„ÄêÈáçË¶ÅÂèÇÁÖßÔºö„Çπ„Éû„Éñ„É©Áî®Ë™ûÈõÜ„Äë\nË™§Â≠óË®ÇÊ≠£Áî®ËæûÊõ∏„Åß„Åô„ÄÇ‰ª•‰∏ã„ÅÆÂÆöÁæ©„Å´Âü∫„Å•„ÅçÂ∞ÇÈñÄÁî®Ë™û„ÇíË£úÊ≠£„Åõ„Çà„ÄÇ\n{COMMON_TERMS}\n"

    # ‚òÖ V128.0 PROMPT (Full Categories, Strict Mermaid, No Decor)
    prompt = f"""
    „ÅÇ„Å™„Åü„ÅØ‰∏ñÁïåÊúÄÈ´òÂ≥∞„ÅÆ„Çπ„Éû„Éñ„É©ÔºàSuper Smash Bros.Ôºâ„Ç¢„Éä„É™„Çπ„Éà„Åß„ÅÇ„Çä„ÄÅË´ñÁêÜÁöÑÊÄùËÄÉ„ÅÆÈÅî‰∫∫„Åß„Åô„ÄÇ
    Êèê‰æõ„Åï„Çå„Åü‰ºöË©±„Éá„Éº„Çø„Åã„Çâ„ÄÅ„Éó„É¨„Ç§„É§„Éº„ÅåÁõ¥Èù¢„Åó„Å¶„ÅÑ„ÇãË™≤È°å„Å®Ëß£Ê±∫Á≠ñ„ÇíÊäΩÂá∫„Åó„ÄÅ‰ª•‰∏ã„ÅÆ„Éï„Ç©„Éº„Éû„ÉÉ„Éà„ÅßÂá∫Âäõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

    „Äê„É°„Çø„Éá„Éº„ÇøÊÉÖÂ†±„Äë
    {hint_context}
    {glossary_instruction}

    „ÄêÈáçË¶ÅÂèÇÁÖßÔºö„Ç´„ÉÜ„Ç¥„É™ÂÆöÁæ©„Äë
    „É¨„Éù„Éº„Éà„ÅÆ„Äå„Éà„Éî„ÉÉ„ÇØÂêç„Äç„ÇíÊ±∫ÂÆö„Åô„ÇãÈöõ„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆÂÆöÁæ©„Å´ÊúÄ„ÇÇÂêàËá¥„Åô„Çã„ÇÇ„ÅÆ„ÇíÈÅ∏„Åπ„ÄÇ
    **ÊäÄË°ìÈù¢Ôºà1-10Ôºâ„Å®„ÄÅ„Éó„É¨„Ç§„É§„ÉºÈù¢Ôºà11-13Ôºâ„ÇíÊòéÁ¢∫„Å´Âå∫Âà•„Åô„Çã„Åì„Å®„ÄÇ**

    -- „Ç≤„Éº„É†ÂÜÖÊäÄË°ì (In-Game) --
    1. **Â∑Æ„ÅóÂêà„ÅÑ:** „Åä‰∫í„ÅÑ„ÅåÂú∞‰∏ä„Åæ„Åü„ÅØÁ©∫‰∏≠„Å´„ÅÑ„Å¶„ÄÅÊúâÂà©‰∏çÂà©„Åå„Å™„ÅÑÁä∂ÊÖã„ÄÇÊäÄ„ÅÆÂΩì„Å¶Âêà„ÅÑ„ÄÇ
    2. **ÁùÄÂú∞:** „ÄêÂÆà„Çä„ÄëËá™ÂàÜ„ÅåÁ©∫‰∏≠„Å´„ÅÑ„Å¶„ÄÅ„Çπ„ÉÜ„Éº„Ç∏„Å´Êàª„Çç„ÅÜ„Å®„Åó„Å¶„ÅÑ„ÇãÁä∂ÊÖã„ÄÇ
    3. **ÁùÄÂú∞Áã©„Çä:** „ÄêÊîª„ÇÅ„ÄëÁõ∏Êâã„ÅåÊµÆ„ÅÑ„Å¶„ÅÑ„Å¶„ÄÅ„Åù„Çå„ÇíËøΩÊíÉ„Åó„Å¶„ÅÑ„ÇãÁä∂ÊÖã„ÄÇ
    4. **Âæ©Â∏∞:** „ÄêÂÆà„Çä„ÄëËá™ÂàÜ„Åå„Çπ„ÉÜ„Éº„Ç∏Â§ñ„Å´È£õ„Å∞„Åï„Çå„ÄÅÂ¥ñ„ÇÑ„Çπ„ÉÜ„Éº„Ç∏‰∏ä„Å´Êàª„Çç„ÅÜ„Å®„Åó„Å¶„ÅÑ„ÇãÁä∂ÊÖã„ÄÇ
    5. **Âæ©Â∏∞ÈòªÊ≠¢:** „ÄêÊîª„ÇÅ„ÄëÁõ∏Êâã„Åå„Çπ„ÉÜ„Éº„Ç∏Â§ñ„Å´„ÅÑ„Å¶„ÄÅÊàª„Å£„Å¶„Åè„Çã„Å®„Åì„Çç„ÇíÈòªÊ≠¢„ÉªÊíÉÂ¢ú„Åó„Çà„ÅÜ„Å®„Åô„ÇãÁä∂ÊÖã„ÄÇ
    6. **Â¥ñ‰∏ä„Åå„Çä:** „ÄêÂÆà„Çä„ÄëËá™ÂàÜ„ÅåÂ¥ñ„Å´Êé¥„Åæ„Å£„Å¶„ÅÑ„ÇãÁä∂ÊÖã„Åã„Çâ„ÄÅ„É©„Ç§„É≥Âæ©Â∏∞„ÇíÁõÆÊåá„ÅôÁä∂ÊÖã„ÄÇ
    7. **Â¥ñÁã©„Çä:** „ÄêÊîª„ÇÅ„ÄëÁõ∏Êâã„ÅåÂ¥ñ„Å´Êé¥„Åæ„Å£„Å¶„ÅÑ„Å¶„ÄÅ„Åù„ÅÆ‰∏ä„Åå„ÇäÈöõ„ÇíÁã©„Çç„ÅÜ„Å®„Åô„ÇãÁä∂ÊÖã„ÄÇ
    8. **„ÉÄ„Ç¶„É≥Áã©„Çä/Âèó„ÅëË∫´:** Áõ∏Êâã„Åæ„Åü„ÅØËá™ÂàÜ„Åå„ÉÄ„Ç¶„É≥ÔºàËª¢ÂÄíÔºâ„Åó„ÅüÈöõ„ÅÆÊîªÈò≤„ÄÇ
    9. **ÊíÉÂ¢ú:** ‰∏äË®ò„Éï„Çß„Éº„Ç∫„ÅÆ‰∏≠„Åß„ÄÅÁâπ„Å´„ÄåÁõ∏Êâã„Çí„Éê„Éº„Çπ„Éà„Åô„Çã„Åì„Å®„Äç„Å´ÁâπÂåñ„Åó„ÅüË≠∞Ë´ñ„ÄÇ
    10. **ÊíÉÂ¢úÊãíÂê¶:** ‰∏äË®ò„Éï„Çß„Éº„Ç∫„ÅÆ‰∏≠„Åß„ÄÅÁâπ„Å´„ÄåËá™ÂàÜ„Åå„Éê„Éº„Çπ„Éà„Åï„Çå„Å™„ÅÑ„Åì„Å®„Äç„Å´ÁâπÂåñ„Åó„ÅüË≠∞Ë´ñ„ÄÇ

    -- „Éó„É¨„Ç§„É§„ÉºÁÆ°ÁêÜ (Meta-Game) --
    11. **„Éû„Ç§„É≥„Éâ„Çª„ÉÉ„Éà:** Ë©¶Âêà‰∏≠„ÅÆ„É°„É≥„Çø„É´Âà∂Âæ°„ÄÅÁ∑äÂºµ„ÄÅÊÄí„Çä„ÄÅËá™‰ø°„ÄÅÂ§ß‰ºö„Å∏„ÅÆÂøÉÊßã„Åà„Å™„Å©„ÄÇ
    12. **‰ΩìË™øÁÆ°ÁêÜ:** Áù°Áú†„ÄÅÈ£ü‰∫ã„ÄÅÁõÆ„ÅÆÁñ≤„Çå„ÄÅÂßøÂã¢„ÄÅ„Ç´„Éï„Çß„Ç§„É≥ÊëÇÂèñ„ÄÅÁîüÊ¥ª„É™„Ç∫„É†„Å™„Å©„ÄÅËÇâ‰ΩìÁöÑ„Å™„Ç≥„É≥„Éá„Ç£„Ç∑„Éß„É≥„ÄÇ
    13. **Âèñ„ÇäÁµÑ„Åø/Â∫ßÂ≠¶:** Á∑¥Áøí„É°„Éã„É•„Éº„ÄÅ„É™„Éó„É¨„Ç§ÂàÜÊûê„ÅÆË≥™„ÄÅÁõÆÊ®ôË®≠ÂÆö„ÄÅ„Çπ„Ç±„Ç∏„É•„Éº„É™„É≥„Ç∞„ÄÅÂ≠¶Áøí„Éó„É≠„Çª„ÇπÂÖ®Ëà¨„ÄÇ

    ---
    **„ÄêÈáçË¶ÅÔºöÂá∫Âäõ„É¨„Ç§„Ç¢„Ç¶„ÉàË¶èÂâá„Äë**
    * Notion„ÅßË¶ã„ÇÑ„Åô„ÅÑÈöéÂ±§ÊßãÈÄ†„Çí‰Ωú„Çã„Åì„Å®„ÄÇ
    * **ÊßãÈÄ†:**
        * „Éà„Éî„ÉÉ„ÇØÂêç: `##` (H2) ‚Äª‰∏äË®ò„ÅÆ„Ç´„ÉÜ„Ç¥„É™ÂÆöÁæ©„Åã„ÇâÈÅ∏Êäû„Åô„Çã„Åì„Å®„ÄÇ
        * Ë©≥Á¥∞È†ÖÁõÆ„ÅÆË¶ãÂá∫„Åó: `###` (H3) ‚ÄªÁÆáÊù°Êõ∏„Åç„ÅÆ„É™„Çπ„ÉàË®òÂè∑(-)„ÇÑÂ§™Â≠ó(**)„ÅØ‰Ωø„Çè„Åö„ÄÅ„Ç∑„É≥„Éó„É´„Å´Ë¶ãÂá∫„Åó„Å´„Åô„Çã„ÄÇ
    * **‰ΩôÁôΩ:** ÂêÑÈ†ÖÁõÆ„ÅÆÈñì„Å´„ÅØÂøÖ„Åö„ÄåÁ©∫Ë°å„Äç„ÇíÂÖ•„Çå„Çã„Åì„Å®„ÄÇ

    **„ÄêSection 1: Ë©≥Á¥∞ÂàÜÊûê„É¨„Éù„Éº„Éà„Äë**
    ‰ºöË©±„Éá„Éº„Çø„Åã„Çâ‰∏ªË¶Å„Å™ÊîπÂñÑ„Éù„Ç§„É≥„Éà„ÇíÊäΩÂá∫„Åó„ÄÅÂêÑ„Éà„Éî„ÉÉ„ÇØ„Å´„Å§„ÅÑ„Å¶‰ª•‰∏ã„ÅÆ**5„Å§„ÅÆË¶≥ÁÇπ**„ÇíÂøÖ„ÅöÁ∂≤ÁæÖ„Åó„Å¶Ë®òËø∞„Åõ„Çà„ÄÇÁúÅÁï•„ÅØË®±„Åï„Çå„Å™„ÅÑ„ÄÇ
    ‚Äª„É°„É≥„Çø„É´„ÇÑ‰ΩìË™øÁÆ°ÁêÜ„ÅÆÂ†¥Âêà„ÇÇ„ÄÅ„Åì„ÅÆ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØÔºàÁèæÁä∂‚ÜíË™≤È°å‚ÜíÂéüÂõ†‚ÜíÊîπÂñÑ‚Üí„ÇÑ„Çã„Åì„Å®Ôºâ„Å´ÂΩì„Å¶„ÅØ„ÇÅ„Å¶Ë´ñÁêÜÁöÑ„Å´ÂàÜËß£„Åõ„Çà„ÄÇ

    ## [„Ç´„ÉÜ„Ç¥„É™Âêç: ÂÖ∑‰ΩìÁöÑ„Å™ÂÜÖÂÆπ] (‰æã: ## ‰ΩìË™øÁÆ°ÁêÜ: Â§ß‰ºöÂâç„ÅÆÁù°Áú†„É™„Ç∫„É†)

    ### ‚ë† ÁèæÁä∂
    „Éó„É¨„Ç§„É§„Éº„ÅåÁèæÂú®Ë°å„Å£„Å¶„ÅÑ„ÇãÂÖ∑‰ΩìÁöÑ„Å™ÊåôÂãï„ÄÅÁôñ„ÄÅË™çË≠ò„ÅÆ„Ç∫„É¨„ÄÇËâØ„ÅÑÁÇπ„ÇÑ„Åô„Åß„Å´Âá∫Êù•„Å¶„ÅÑ„ÇãÁÇπ„Å´Ë®ÄÂèä„Åå„ÅÇ„Çå„Å∞„Åù„Çå„ÇíÂàóÊåô„Åó„Åü„ÅÇ„Å®„ÄÅ‰ªäÂæå„ÅÆË™≤È°å„Å´„Å§„Å™„Åå„ÇãÁÇπ=Áôñ„ÇÑË™çË≠ò„ÅÆ„Ç∫„É¨„ÅÆË®ÄÂèä„Å´ÂÖ•„Çã„ÄÇ

    ### ‚ë° Ë™≤È°å
    „Åù„ÅÆÊåôÂãï„ÅåÂºï„ÅçËµ∑„Åì„Åô„É™„Çπ„ÇØÔºà‰∏çÂà©„Éï„É¨„Éº„É†„ÄÅÊíÉÂ¢úÊãíÂê¶„Éü„Çπ„ÄÅ„É©„Ç§„É≥Âñ™Â§±„Å™„Å©Ôºâ„ÄÇ

    ### ‚ë¢ ÂéüÂõ†
    „Å™„Åú„Åù„ÅÆË™≤È°å„ÅåËµ∑„Åç„Çã„ÅÆ„ÅãÔºàÁü•Ë≠ò‰∏çË∂≥„ÄÅÊâãÁôñ„ÄÅ„É™„Çπ„ÇØÁÆ°ÁêÜ„ÅÆÁîò„Åï„ÄÅÁõ∏Êâã„ÅÆË°åÂãïÁ¢∫Ë™ç‰∏çË∂≥„Å™„Å©Ôºâ„ÄÇ

    ### ‚ë£ ÊîπÂñÑÊ°à
    ÂÖ∑‰ΩìÁöÑ„Å™‰øÆÊ≠£„Ç¢„ÇØ„Ç∑„Éß„É≥ÔºàÔºÖÂ∏Ø„Å´„Çà„ÇãÊäÄÈÅ∏Êäû„ÄÅË¶ñÁ∑ö„ÅÆÈÖç„ÇäÊñπ„ÄÅÊÑèË≠òÈÖçÂàÜÔºâ„ÄÇ

    ### ‚ë§ „ÇÑ„Çã„Åì„Å®
    Âç≥Â∫ß„Å´ÂÆüË°åÂèØËÉΩ„Å™„ÄÅÁü≠„ÅèÊòéÁ¢∫„Å™ÊåáÁ§∫„ÄÇ„Åß„Åç„Å¶„ÅÑ„ÇãÁÇπ„ÇÑËâØ„Åã„Å£„ÅüÁÇπ„Å™„Å©„ÄÅÁ∂ôÁ∂ö„Éù„Ç§„É≥„Éà„Åå„ÅÇ„Çå„Å∞„Åù„Çå„Å´„ÇÇË®ÄÂèä„ÄÇ

    **„ÄêSection 2: Ë™≤È°å„Çª„ÉÉ„Éà„Äë**
    „Äå„Éà„É™„Ç¨„Éº(Áõ∏Êâã„ÅÆË°åÂãï)„Äç‚Üí„Äå„Ç¢„ÇØ„Ç∑„Éß„É≥(Ëá™ÂàÜ„ÅÆË°åÂãï)„Äç„ÅÆÂΩ¢Âºè„ÅßÁÆáÊù°Êõ∏„Åç„Åõ„Çà„ÄÇ
    ‚Äª„É°„É≥„Çø„É´„ÇÑ‰ΩìË™ø„ÅÆÂ†¥Âêà„ÅØ„Äå„Éà„É™„Ç¨„Éº(‰∫ãË±°)„Äç‚Üí„Äå„Ç¢„ÇØ„Ç∑„Éß„É≥(ÂØæÂá¶)„Äç„Å®„Åô„Çã„ÄÇ

    **„ÄêSection 3: ÊôÇÁ≥ªÂàó„É≠„Ç∞„Äë**
    „Çª„ÉÉ„Ç∑„Éß„É≥„ÅÆÊµÅ„Çå„ÇíÊôÇÁ≥ªÂàó„ÅßË¶ÅÁ¥Ñ„Åõ„Çà„ÄÇ

    **„ÄêSection 4: „É°„Çø„Éá„Éº„ÇøJSON„Äë**
    {{
      "student_name": "ÁîüÂæíÂêç",
      "date": "YYYY-MM-DD",
      "next_action": "ÊúÄÂÑ™ÂÖà„Ç¢„ÇØ„Ç∑„Éß„É≥"
    }}

    **„ÄêSection 5: ÊÄùËÄÉ„Éï„É≠„Éº„ÉÅ„É£„Éº„Éà (Mermaid)„Äë**
    Section 1„ÅßÂàÜÊûê„Åó„Åü„ÄåÂà§Êñ≠„ÅÆÂàÜÂ≤ê„Äç„ÇÑ„ÄåÊîπÂñÑ„Ç¢„ÇØ„Ç∑„Éß„É≥„ÅÆ„Éó„É≠„Çª„Çπ„Äç„Çí„ÄÅMermaidË®òÊ≥ïÔºàflowchart TDÔºâ„ÅßË¶ñË¶öÂåñ„Åõ„Çà„ÄÇ
    
    **ÔºúMermaid‰ΩúÊàê„ÅÆÁµ∂ÂØæ„É´„Éº„É´Ôºà„Ç®„É©„ÉºÈò≤Ê≠¢ÔºâÔºû**
    1. **ÊßãÊñá:** `graph TD` „Çí‰ΩøÁî®„Åô„Çã„ÄÇ
    2. **Á¶ÅÊ≠¢ÊñáÂ≠ó:** „ÉÄ„Éñ„É´„ÇØ„Ç©„Éº„ÉÜ„Éº„Ç∑„Éß„É≥ `"`„ÄÅ**ÂçäËßí„Ç≥„É≥„Éû `,`**„ÄÅ**ÂçäËßí„Ç´„ÉÉ„Ç≥ `()`** „ÅØÁµ∂ÂØæ„Å´Á¶ÅÊ≠¢„ÄÇ
    3. **‰ª£ÊõøÊñáÂ≠ó:** Âè•Ë™≠ÁÇπ„ÅØÂÖ®Ëßí„ÅÆ `„ÄÅ` `„ÄÇ` „Çí„ÄÅ„Ç´„ÉÉ„Ç≥„ÅØÂÖ®Ëßí„ÅÆ `Ôºà` `Ôºâ` „Çí‰ΩøÁî®„Åõ„Çà„ÄÇ
    4. **ÂÜÖÂÆπ:** Âçò„Å™„ÇãÈ†ÖÁõÆ„ÅÆÁæÖÂàó„Åß„ÅØ„Å™„Åè„ÄÅ**„ÄåCheckÔºàÂà§Êñ≠Ôºâ„Äç‚Üí„ÄåBranchÔºàÂàÜÂ≤êÔºâ„Äç‚Üí„ÄåActionÔºàË°åÂãïÔºâ„Äç**„ÅÆÊµÅ„Çå„ÇíÊèè„Åè„Åì„Å®„ÄÇ
    5. **ÂΩ¢Áä∂:** Âà§Êñ≠/ÂàÜÂ≤ê„Å´„ÅØ„Å≤„ÅóÂΩ¢ {{ }}„ÄÅÂá¶ÁêÜ/Ë°åÂãï„Å´„ÅØÂõõËßí [ ] „ÇíÊ≠£„Åó„Åè‰Ωø„ÅÑÂàÜ„Åë„Çã„Åì„Å®„ÄÇ

    ---
    **Âá∫Âäõ„Éñ„É≠„ÉÉ„ÇØÔºà„Ç∑„Çπ„ÉÜ„É†Âà∂Âæ°Áî®„Çø„Ç∞ÔºâÔºö**
    
    **[DETAILED_REPORT_START]**
    (Section 1 „Å® Section 2 „ÅÆÂÜÖÂÆπ)
    **[DETAILED_REPORT_END]**

    **[RAW_LOG_START]**
    (Section 3 „ÅÆÂÜÖÂÆπ)
    **[RAW_LOG_END]**

    **[JSON_START]**
    (Section 4 „ÅÆJSON)
    **[JSON_END]**

    **[MERMAID_START]**
    (Section 5 „ÅÆMermaid„Ç≥„Éº„Éâ„ÅÆ„Åø„ÄÇ„Éê„ÉÉ„ÇØ„ÇØ„Ç©„Éº„Éà‰∏çË¶Å)
    **[MERMAID_END]**
    ---

    „ÄêÂÖ•Âäõ„ÉÜ„Ç≠„Çπ„Éà„Äë
    {transcript_text}
    """

    max_retries = 10
    for attempt in range(max_retries):
        try:
            response = gemini_client.models.generate_content(model=RESOLVED_MODEL_ID, contents=prompt)
            text = response.text.strip()
            break 
        except Exception as e:
            err_str = str(e).lower()
            if "429" in err_str or "quota" in err_str or "overloaded" in err_str:
                wait = 60 * (attempt + 1)
                print(f"‚è≥ Gemini Busy ({RESOLVED_MODEL_ID}). Waiting {wait}s...", flush=True)
                time.sleep(wait)
            else:
                log_error("Gemini Analysis Failed", e)
                return {"student_name": "AnalysisError", "date": datetime.now().strftime('%Y-%m-%d')}, f"Analysis Error: {e}", transcript_text[:2000], None
    else: return {"student_name": "QuotaError", "date": datetime.now().strftime('%Y-%m-%d')}, "Quota Limit Exceeded", transcript_text[:2000], None

    def extract_safe(s, e, src):
        m = re.search(f'{re.escape(s)}(.*?){re.escape(e)}', src, re.DOTALL)
        return m.group(1).strip() if m else None

    report = extract_safe("[DETAILED_REPORT_START]", "[DETAILED_REPORT_END]", text)
    time_log = extract_safe("[RAW_LOG_START]", "[RAW_LOG_END]", text)
    json_str = extract_safe("[JSON_START]", "[JSON_END]", text)
    mermaid_code = extract_safe("[MERMAID_START]", "[MERMAID_END]", text)

    if not report:
        print("‚ö†Ô∏è Warning: Missing REPORT tags. Fallback...", flush=True)
        if "[RAW_LOG_START]" in text:
            report = text.split("[RAW_LOG_START]")[0].replace("[DETAILED_REPORT_START]", "").strip()
        else:
            report = text

    if not time_log: time_log = "Log tags missing."
    
    if not mermaid_code:
        m_match = re.search(r'```mermaid(.*?)```', text, re.DOTALL)
        if m_match: mermaid_code = m_match.group(1).strip()
    
    if mermaid_code:
        mermaid_code = mermaid_code.replace("**", "").replace("```mermaid", "").replace("```", "").strip()

    try: 
        if json_str: data = json.loads(json_str)
        else: raise ValueError("No JSON block")
    except: 
        try:
            json_candidate = re.search(r'\{.*"student_name".*\}', text, re.DOTALL)
            if json_candidate: data = json.loads(json_candidate.group(0))
            else: data = {"student_name": "Unknown", "date": datetime.now().strftime('%Y-%m-%d'), "next_action": "Check Logs"}
        except:
            data = {"student_name": "Unknown", "date": datetime.now().strftime('%Y-%m-%d'), "next_action": "Check Logs"}
            
    return data, report, time_log, mermaid_code

def text_to_notion_blocks(text):
    blocks = []
    lines = text.split('\n')
    
    for line in lines:
        if not line.strip():
            blocks.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {"rich_text": []} 
            })
            continue
        
        if line.startswith('|') or line.startswith('+-'):
             continue 

        clean_content = line[:1900] 
        
        if line.startswith('### '):
            blocks.append({
                "object": "block",
                "type": "heading_3",
                "heading_3": {"rich_text": [{"type": "text", "text": {"content": clean_content[4:]}}]}
            })
        elif line.startswith('## '):
            blocks.append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {"rich_text": [{"type": "text", "text": {"content": clean_content[3:]}}]}
            })
        elif line.startswith('# '):
            blocks.append({
                "object": "block",
                "type": "heading_1",
                "heading_1": {"rich_text": [{"type": "text", "text": {"content": clean_content[2:]}}]}
            })
        elif line.startswith('- ') or line.startswith('* '):
            blocks.append({
                "object": "block",
                "type": "bulleted_list_item",
                "bulleted_list_item": {"rich_text": [{"type": "text", "text": {"content": clean_content[2:]}}]}
            })
        else:
            blocks.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {"rich_text": [{"type": "text", "text": {"content": clean_content}}]}
            })
            
    return blocks

# --- 5. Asset Management ---

def notion_create_page_heavy(db_id, props, children):
    print(f"üì§ Posting to Notion DB: {db_id}...", flush=True)
    res = requests.post("https://api.notion.com/v1/pages", headers=HEADERS, json={"parent": {"database_id": db_id}, "properties": props, "children": children[:100]})
    if res.status_code != 200:
        print(f"‚ö†Ô∏è Initial Post Failed ({res.status_code}). Retrying with SAFE MODE...", flush=True)
        safe_props = {}
        for key, val in props.items():
            if "title" in val: safe_props[key] = val; break
        if not safe_props:
             content_text = props.get("ÂêçÂâç", {}).get("title", [{}])[0].get("text", {}).get("content", "Log")
             safe_props = {"Name": {"title": [{"text": {"content": content_text}}]}}
        date_val = props.get("Êó•‰ªò", {}).get("date", {}).get("start", "Unknown")
        error_note = {"object": "block", "type": "callout", "callout": {"rich_text": [{"text": {"content": f"‚ö†Ô∏è Date Prop Missing. Date: {date_val}"}}]}}
        children.insert(0, error_note)
        res = requests.post("https://api.notion.com/v1/pages", headers=HEADERS, json={"parent": {"database_id": db_id}, "properties": safe_props, "children": children[:100]})
        if res.status_code != 200:
            print(f"‚ùå NOTION SAFE MODE FAILED: {res.status_code}\n{res.text}", flush=True)
            return

    response_data = res.json()
    pid = response_data.get('id')
    print(f"üîó Notion Page Created: {response_data.get('url')}", flush=True)
    if pid and len(children) > 100:
        for i in range(100, len(children), 100):
            requests.patch(f"https://api.notion.com/v1/blocks/{pid}/children", headers=HEADERS, json={"children": children[i:i+100]})

def ensure_processed_folder():
    try:
        q = f"name='processed_coaching_logs' and '{INBOX_FOLDER_ID}' in parents"
        folders = drive_service.files().list(q=q).execute().get('files', [])
        if folders: return folders[0]['id']
        folder = drive_service.files().create(body={'name': 'processed_coaching_logs', 'mimeType': 'application/vnd.google-apps.folder', 'parents': [INBOX_FOLDER_ID]}, fields='id').execute()
        return folder.get('id')
    except Exception as e:
        log_error("Failed to Get/Create Processed Folder", e)
        return INBOX_FOLDER_ID

def upload_file_to_drive(local_path, folder_id, rename_to, mime_type):
    print(f"üì§ Uploading {rename_to}...", flush=True)
    try:
        media = MediaFileUpload(local_path, mimetype=mime_type, resumable=True, chunksize=100*1024*1024)
        drive_service.files().create(
            body={'name': rename_to, 'parents': [folder_id]}, 
            media_body=media, 
            fields='id',
            supportsAllDrives=True
        ).execute()
        print("‚úÖ Upload Complete.", flush=True)
    except Exception as e:
        log_error(f"Upload Failed for {rename_to}", e)

def move_original_file(file_id, folder_id):
    if folder_id == INBOX_FOLDER_ID:
        print("‚ö†Ô∏è Skipping Move: Destination is Inbox.", flush=True)
        return
    try:
        prev_parents = drive_service.files().get(fileId=file_id, fields='parents').execute().get('parents', [])
        prev_str = ",".join(prev_parents)
        drive_service.files().update(
            fileId=file_id, 
            addParents=folder_id, 
            removeParents=prev_str,
            supportsAllDrives=True
        ).execute()
        print(f"üì¶ Archived original file to folder [{folder_id}].", flush=True)
    except Exception as e:
        log_error(f"Move Original File Failed (ID: {file_id})", e)
        print(f"üëâ TIP: Add this email to folder permissions: {BOT_EMAIL}", flush=True)

# --- Main ---
def main():
    print("--- SZ AUTO LOGGER ULTIMATE (v128.0 - Full Categories & Clean Mermaid) ---", flush=True)
    load_student_registry()
    
    try:
        files = drive_service.files().list(
            q=f"'{INBOX_FOLDER_ID}' in parents and trashed=false and mimeType!='application/vnd.google-apps.folder'",
            fields="files(id, name, createdTime)"
        ).execute().get('files', [])
    except Exception: return

    if not files: print("‚ÑπÔ∏è No files."); return

    for file in files:
        try:
            print(f"\nüìÇ Processing: {file['name']}")
            safe_name = sanitize_filename(file['name'])
            fpath = os.path.join(TEMP_DIR, safe_name)
            
            # Download
            max_dl_retries = 3
            for dl_attempt in range(max_dl_retries):
                try:
                    with open(fpath, "wb") as f:
                        request = drive_service.files().get_media(fileId=file['id'])
                        downloader = MediaIoBaseDownload(f, request, chunksize=100*1024*1024)
                        done = False
                        while done is False:
                            status, done = downloader.next_chunk()
                            print(f"   ‚¨áÔ∏è Downloading... {int(status.progress() * 100)}%", end="\r", flush=True)
                    print("\n‚úÖ Download Complete.")
                    break 
                except Exception as e:
                    print(f"\n‚ö†Ô∏è Download Interrupted: {e}. Retrying...", flush=True)
                    time.sleep(5)
            else:
                print("‚ùå Download Failed. Skipping.")
                continue

            srcs = []
            candidate_raw_name = None

            if safe_name.endswith('.zip'):
                try:
                    patoolib.extract_archive(fpath, outdir=TEMP_DIR)
                    extracted_files = []
                    for r, _, fs in os.walk(TEMP_DIR):
                        for af in fs:
                            full_p = os.path.join(r, af)
                            extracted_files.append(full_p)
                            if af.lower().endswith(('.flac', '.mp3', '.m4a', '.wav')) and 'final_mix' not in af and 'chunk' not in af:
                                srcs.append(full_p)
                    candidate_raw_name = detect_student_candidate_raw(extracted_files, file['name'])
                except Exception as e:
                    log_error(f"Archive Extraction Failed", e)
                    continue
            else: srcs.append(fpath)
            
            if not srcs: print("‚ÑπÔ∏è No audio files found."); continue
            
            # Processing
            precise_datetime, date_only = extract_date_smart(file['name'], file.get('createdTime'))
            mixed = mix_audio_ffmpeg(srcs)
            chunks = split_audio_ffmpeg(mixed)
            full_text = transcribe_with_groq(chunks)
            
            # Analysis
            meta, report, logs, mermaid_code = analyze_text_with_gemini(full_text, precise_datetime, candidate_raw_name)
            
            # DB Matching
            did, oname = find_best_student_match(meta['student_name'])
            
            # --- Build Notion Blocks (UPDATED) ---
            final_blocks = []

            # 1. Detailed Report
            report_header = "### üìä SZ„É°„ÇΩ„ÉÉ„ÉâË©≥Á¥∞ÂàÜÊûê\n\n" + report
            final_blocks.extend(text_to_notion_blocks(report_header))

            # 2. Mermaid Block
            if mermaid_code:
                final_blocks.append({"object": "block", "type": "divider", "divider": {}})
                final_blocks.append({
                    "object": "block", 
                    "type": "heading_2", 
                    "heading_2": {"rich_text": [{"text": {"content": "üß† ÊÄùËÄÉ„Éï„É≠„Éº„ÉÅ„É£„Éº„Éà"}}]}
                })
                final_blocks.append({
                    "object": "block",
                    "type": "callout",
                    "callout": {
                        "rich_text": [{"text": {"content": "‰∏ä„ÅÆÂàÜÊûêÂÜÖÂÆπ„ÇíÊßãÈÄ†Âåñ„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇÂà§Êñ≠„Å´Ëø∑„Å£„ÅüÊôÇ„ÅÆÂú∞Âõ≥„Å®„Åó„Å¶‰Ωø„Å£„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"}}],
                        "icon": {"emoji": "üó∫Ô∏è"}
                    }
                })
                final_blocks.append({
                    "object": "block",
                    "type": "code",
                    "code": {
                        "rich_text": [{"type": "text", "text": {"content": mermaid_code}}],
                        "language": "mermaid" 
                    }
                })

            # 3. Logs
            logs_content = f"\n---\n\n### üìù ÊôÇÁ≥ªÂàó„É≠„Ç∞\n\n{logs}"
            final_blocks.extend(text_to_notion_blocks(logs_content))

            # 4. Transcript
            final_blocks.append({"object": "block", "type": "divider", "divider": {}})
            final_blocks.append({"object": "block", "type": "heading_3", "heading_3": {"rich_text": [{"text": {"content": "üìú ÂÖ®ÊñáÊñáÂ≠óËµ∑„Åì„Åó"}}]}})
            
            for i in range(0, len(full_text), 1900):
                chunk_text = full_text[i:i+1900]
                final_blocks.append({"object": "block", "type": "paragraph", "paragraph": {"rich_text": [{"text": {"content": chunk_text}}]}})
            
            props = {
                "ÂêçÂâç": {"title": [{"text": {"content": f"{precise_datetime} {oname} ÈÄöË©±„É≠„Ç∞"}}]}, 
                "Êó•‰ªò": {"date": {"start": date_only}}
            }

            print("üíæ Saving to Fallback DB (All Data)...")
            notion_create_page_heavy(sanitize_id(FINAL_FALLBACK_DB_ID), copy.deepcopy(props), copy.deepcopy(final_blocks))
            
            if did and did != FINAL_FALLBACK_DB_ID:
                print(f"üë§ Saving to Student DB ({oname})...")
                notion_create_page_heavy(sanitize_id(did), copy.deepcopy(props), copy.deepcopy(final_blocks))
            
            # Artifacts
            processed_folder_id = ensure_processed_folder()
            safe_filename_time = precise_datetime.replace(':', '-').replace(' ', '_')
            
            upload_file_to_drive(mixed, processed_folder_id, f"{safe_filename_time}_{oname}_Full.mp3", 'audio/mpeg')
            
            txt_path = os.path.join(TEMP_DIR, "transcript.txt")
            with open(txt_path, "w") as f: f.write(full_text)
            upload_file_to_drive(txt_path, processed_folder_id, f"{safe_filename_time}_{oname}_Transcript.txt", 'text/plain')
            
            move_original_file(file['id'], processed_folder_id)

        except Exception as e:
            log_error(f"Processing Failed for {file['name']}", e)
            continue
        finally:
            if os.path.exists(TEMP_DIR): shutil.rmtree(TEMP_DIR); os.makedirs(TEMP_DIR)

if __name__ == "__main__": main()
