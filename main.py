import sys
import subprocess
import os
import time
import json
import shutil
import glob
import re
import traceback
import copy
import difflib
from datetime import datetime, timedelta, timezone

# --- 0. SDK & Tools ---
def install_package(package):
    try:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    except: pass

install_package("google-genai")
install_package("groq")
install_package("patool")

# --- Libraries ---
import requests
from google import genai 
from google.genai import types
from groq import Groq
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload
from googleapiclient.errors import HttpError
import patoolib

# --- Configuration ---
FINAL_CONTROL_DB_ID = "2b71bc8521e380868094ec506b41f664"
FINAL_FALLBACK_DB_ID = "2e01bc8521e380ffaf28c2ab9376b00d"
TEMP_DIR = "temp_workspace"
CHUNK_LENGTH = 900  # 15 min

# Global Variables
RESOLVED_MODEL_ID = None
BOT_EMAIL = None
STUDENT_REGISTRY = {}
COMMON_TERMS = ""

# Try to load glossary
try:
    from smash_glossary import COMMON_TERMS
except ImportError:
    pass

# --- Helper: Verbose Error Printer ---
def log_error(context, error_obj):
    if isinstance(error_obj, HttpError) and "storageQuotaExceeded" in str(error_obj):
        print(f"âš ï¸ [Quota Limit] Could not upload artifact ({context}). Skipping.", flush=True)
    else:
        print(f"\nâŒ [ERROR] {context}", flush=True)
        print(f"   Details: {str(error_obj)}", flush=True)
        print("-" * 30, flush=True)

# --- 1. Model Selection Logic (Dynamic & Strict) ---

def parse_model_score(model_name):
    """
    ãƒ¢ãƒ‡ãƒ«åã‹ã‚‰ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ãƒ†ã‚£ã‚¢ã‚’è§£æã—ã€ã‚¹ã‚³ã‚¢åŒ–ã™ã‚‹ã€‚
    æˆ»ã‚Šå€¤: (version_float, tier_score)
    """
    # Version extraction (e.g., gemini-1.5-pro -> 1.5)
    ver_match = re.search(r"gemini-(\d+\.\d+)", model_name)
    version = float(ver_match.group(1)) if ver_match else 0.0
    
    # Tier scoring
    tier = 0
    if "ultra" in model_name: tier = 5
    elif "thinking" in model_name: tier = 4.5 # Thinking models often outperform Pro
    elif "pro" in model_name: tier = 4
    elif "flash" in model_name: tier = 2
    elif "nano" in model_name: tier = 1
    
    # Experimental penalty/bonus? 
    # Current stance: Use Exp if it's the highest version available. No penalty.
    
    return version, tier

def fetch_and_rank_models(client):
    print("ğŸ“¡ Fetching available models from API...", flush=True)
    try:
        # SDKã®ä»•æ§˜ã«åˆã‚ã›ã¦ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
        # google-genai SDK v0.1+ uses client.models.list()
        all_models = list(client.models.list())
        
        candidates = []
        print(f"ğŸ” Found {len(all_models)} total models. Filtering...", flush=True)

        for m in all_models:
            # modelã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰åå‰ã‚’å–å¾— (m.name or m.display_name depending on SDK version)
            m_name = m.name.replace("models/", "") if hasattr(m, "name") else str(m)
            
            # åŸºæœ¬ãƒ•ã‚£ãƒ«ã‚¿: generateContentãŒä½¿ãˆã‚‹Geminiç³»ãƒ¢ãƒ‡ãƒ«ã®ã¿
            if "gemini" not in m_name or "vision" in m_name: 
                continue
            
            version, tier = parse_model_score(m_name)
            
            # --- STRICT THRESHOLD CHECK ---
            # ä¸‹é™: 2.5 Pro (Version >= 2.5 AND Tier >= Pro(4))
            # ãŸã ã—ã€Version 3.0 Flash (Ver=3.0, Tier=2) ã¯ 2.5 Proã‚ˆã‚Šè³¢ã„å¯èƒ½æ€§ãŒé«˜ã„ãŸã‚ã€
            # ã€ŒVersionãŒ2.5ã‚ˆã‚Šå¤§ãã‘ã‚Œã°Flashã§ã‚‚å¯ã€ã¨ã™ã‚‹ã‹ã€
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡ç¤ºé€šã‚Šã€Œ2.5ProãŒä¸‹é™ã€ã‚’å³å¯†ã«å®ˆã‚‹ã‹ã€‚
            # æŒ‡ç¤ºï¼šèƒ½åŠ›ã®ä¸‹é™ãŒ2.5Proã€‚
            # è§£é‡ˆï¼šVersion 2.5ä»¥ä¸Šã¯å¿…é ˆã€‚Version 2.5ã®å ´åˆã¯Proä»¥ä¸Šå¿…é ˆã€‚
            
            is_qualified = False
            if version > 2.5:
                is_qualified = True # 3.0 Flash etc are OK
            elif version == 2.5:
                if tier >= 4: # Pro, Thinking, Ultra
                    is_qualified = True
            
            if is_qualified:
                candidates.append({
                    "id": m_name,
                    "version": version,
                    "tier": tier,
                    "score": version * 10 + tier # Weight version heavily
                })
        
        # Sort by Score Descending
        candidates.sort(key=lambda x: x["score"], reverse=True)
        return candidates

    except Exception as e:
        print(f"âŒ Failed to list models: {e}")
        return []

def setup_env_and_model():
    global RESOLVED_MODEL_ID, BOT_EMAIL
    if os.path.exists(TEMP_DIR): shutil.rmtree(TEMP_DIR)
    os.makedirs(TEMP_DIR)
    
    # --- GCP Setup ---
    sa_key = os.getenv("GCP_SA_KEY")
    if sa_key:
        with open("service_account.json", "w") as f: f.write(sa_key)
        try:
            key_data = json.loads(sa_key)
            BOT_EMAIL = key_data.get("client_email", "Unknown")
        except: pass
    else:
        print("âŒ ENV Error: GCP_SA_KEY missing.")
        sys.exit(1)

    # --- Model Selection ---
    try:
        gemini_client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
        
        # 1. Get Ranked Candidates
        candidates = fetch_and_rank_models(gemini_client)
        
        if not candidates:
            print("âŒ CRITICAL: No models found meeting the minimum criteria (>= 2.5 Pro).")
            print("   Please check your API Key permissions or wait for model release.")
            sys.exit(1)

        print(f"ğŸ“‹ Candidate List (Top 5): {[c['id'] for c in candidates[:5]]}", flush=True)

        # 2. Test Candidates in Order
        for cand in candidates:
            mid = cand["id"]
            print(f"ğŸ‘‰ Testing Candidate: [{mid}]...", flush=True)
            try:
                # Ping test
                gemini_client.models.generate_content(model=mid, contents="Test.")
                print(f"âœ… LOCKED: Using [{mid}] (Ver: {cand['version']}, Tier: {cand['tier']})", flush=True)
                RESOLVED_MODEL_ID = mid
                break
            except Exception as e:
                print(f"   âš ï¸ Failed ({mid}): {e}")
                continue
        
        if not RESOLVED_MODEL_ID:
            print("âŒ CRITICAL: All qualified models failed connectivity checks.")
            sys.exit(1)

    except Exception as e:
        log_error("Model Setup Failed", e)
        sys.exit(1)

    # --- Other Services ---
    global groq_client, drive_service, INBOX_FOLDER_ID, HEADERS
    groq_client = Groq(api_key=os.getenv("GROQ_API_KEY"))
    NOTION_TOKEN = os.getenv("NOTION_TOKEN")
    HEADERS = {"Authorization": f"Bearer {NOTION_TOKEN}", "Content-Type": "application/json", "Notion-Version": "2022-06-28"}
    creds = service_account.Credentials.from_service_account_file("service_account.json", scopes=['https://www.googleapis.com/auth/drive'])
    drive_service = build('drive', 'v3', credentials=creds)
    INBOX_FOLDER_ID = os.getenv("DRIVE_FOLDER_ID")

# --- Execute Setup ---
setup_env_and_model()


def sanitize_id(raw_id):
    if not raw_id: return None
    match = re.search(r'([a-fA-F0-9]{32})', str(raw_id).replace("-", ""))
    return match.group(1) if match else None

# --- Logic: Registry & Fuzzy Match ---

def load_student_registry():
    global STUDENT_REGISTRY
    print("ğŸ“‹ Loading Student Registry from Notion...", flush=True)
    db_id = sanitize_id(FINAL_CONTROL_DB_ID)
    if not db_id: return

    has_more = True
    next_cursor = None
    count = 0

    while has_more:
        payload = {"page_size": 100}
        if next_cursor: payload["start_cursor"] = next_cursor
        
        try:
            res = requests.post(f"https://api.notion.com/v1/databases/{db_id}/query", headers=HEADERS, json=payload)
            if res.status_code != 200: break
            data = res.json()
            for row in data.get("results", []):
                try:
                    name_list = row["properties"]["Name"]["title"]
                    if not name_list: continue
                    name = name_list[0]["plain_text"]
                    tid_list = row["properties"]["TargetID"]["rich_text"]
                    tid = sanitize_id(tid_list[0]["plain_text"]) if tid_list else None
                    if name and tid:
                        STUDENT_REGISTRY[name] = tid
                        count += 1
                except: continue
            has_more = data.get("has_more", False)
            next_cursor = data.get("next_cursor")
        except Exception as e: break
    print(f"âœ… Loaded {count} students into registry.", flush=True)

def find_best_student_match(query_name):
    if not query_name or not STUDENT_REGISTRY: return None, query_name
    if query_name in STUDENT_REGISTRY: return STUDENT_REGISTRY[query_name], query_name
    matches = difflib.get_close_matches(query_name, list(STUDENT_REGISTRY.keys()), n=1, cutoff=0.4)
    if matches:
        print(f"ğŸ¯ Fuzzy Match: '{query_name}' -> '{matches[0]}'", flush=True)
        return STUDENT_REGISTRY[matches[0]], matches[0]
    return None, query_name

# --- Logic: Metadata Helpers ---

def sanitize_filename(filename):
    return filename.replace("/", "_").replace("\\", "_")

def get_jst_now():
    return datetime.now(timezone(timedelta(hours=9)))

def extract_date_smart(filename, drive_created_time_iso):
    match = re.search(r'(\d{4}-\d{2}-\d{2})_(\d{1,2}-\d{1,2}-\d{1,2})', filename)
    if match:
        d_part = match.group(1)
        t_part = match.group(2).replace('-', ':')
        t_parts = t_part.split(':')
        t_formatted = f"{int(t_parts[0]):02}:{int(t_parts[1]):02}:{int(t_parts[2]):02}"
        return f"{d_part} {t_formatted}", d_part
    
    if drive_created_time_iso:
        try:
            dt = datetime.fromisoformat(drive_created_time_iso.replace('Z', '+00:00'))
            dt_jst = dt.astimezone(timezone(timedelta(hours=9)))
            return dt_jst.strftime('%Y-%m-%d %H:%M:%S'), dt_jst.strftime('%Y-%m-%d')
        except: pass

    now_jst = get_jst_now()
    return now_jst.strftime('%Y-%m-%d %H:%M:%S'), now_jst.strftime('%Y-%m-%d')

def detect_student_candidate_raw(file_list, original_archive_name):
    """
    ãƒ•ã‚¡ã‚¤ãƒ«åã®ä¸€éƒ¨ãŒã€Notionãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆSTUDENT_REGISTRYï¼‰ã®ç™»éŒ²åã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ã‚’å³å¯†ã«ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã€‚
    ä¾‹: ãƒ•ã‚¡ã‚¤ãƒ«å '2-kiyamu.flac' (clean: kiyamu) -> DBå 'ã‚­ãƒ£ãƒ  kiyamu' ã«åŒ…å«ã•ã‚Œã‚‹ãŸã‚ãƒ’ãƒƒãƒˆã€‚
    """
    global STUDENT_REGISTRY
    
    ignore_files = ["raw.dat", "info.txt", "ds_store", "thumbs.db", "desktop.ini", "readme", "license"]
    # hikariã¯ã‚³ãƒ¼ãƒï¼ˆUserï¼‰ã®ãŸã‚ã€å€™è£œã‹ã‚‰é™¤å¤–
    ignore_names = ["hikari", "craig", "entrymonster", "bot", "ssb", "recording"] 

    potential_candidates = []

    print("ğŸ” Scanning internal files for registry match...", flush=True)
    
    # 1. ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‹ã‚‰å€™è£œæ–‡å­—åˆ—ã‚’æŠ½å‡º
    for f in file_list:
        basename = os.path.basename(f).lower()
        if any(ign in basename for ign in ignore_files): continue
        
        name_part = os.path.splitext(basename)[0]
        # "1-name", "2_name" ãªã©ã®ãƒ—ãƒ¬ãƒ•ã‚£ã‚¯ã‚¹ã‚’é™¤å»
        clean_name = re.sub(r'^\d+[-_]?', '', name_part)
        
        if any(ign in clean_name for ign in ignore_names): continue
        if len(clean_name) < 2: continue
        
        potential_candidates.append(clean_name)

    # 2. ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–è‡ªä½“ã®ãƒ•ã‚¡ã‚¤ãƒ«åã‚‚å€™è£œã«åŠ ãˆã‚‹
    base_archive = os.path.basename(original_archive_name)
    archive_clean = re.sub(r'\.zip|\.flac|\.mp3|\.wav', '', base_archive, flags=re.IGNORECASE)
    archive_clean = re.sub(r'\d{4}-\d{2}-\d{2}', '', archive_clean).strip()
    if len(archive_clean) > 2:
        potential_candidates.append(archive_clean)

    # 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼ˆRegistryï¼‰ã¨ã®å³å¯†ãªåŒ…å«ãƒã‚§ãƒƒã‚¯
    # ãƒ•ã‚¡ã‚¤ãƒ«åæ–‡å­—åˆ—(candidate) ãŒ DBå(db_name) ã«å«ã¾ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèª
    if STUDENT_REGISTRY:
        for candidate in potential_candidates:
            cand_lower = candidate.lower()
            for db_name in STUDENT_REGISTRY.keys():
                # Registryã‚­ãƒ¼ï¼ˆä¾‹: "ã‚­ãƒ£ãƒ  kiyamu"ï¼‰ã®ä¸­ã«å€™è£œï¼ˆ"kiyamu"ï¼‰ãŒå«ã¾ã‚Œã‚‹ã‹
                if cand_lower in db_name.lower():
                    print(f"ğŸ’¡ Registry Match Found: File '{candidate}' matches DB '{db_name}'", flush=True)
                    return db_name

    # 4. ãƒãƒƒãƒã—ãªã‹ã£ãŸå ´åˆã€Geminiã¸ã®ãƒ’ãƒ³ãƒˆã¨ã—ã¦å€™è£œæ–‡å­—åˆ—ã‚’ãã®ã¾ã¾è¿”ã™ï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼æŒ™å‹•ï¼‰
    if potential_candidates:
        fallback = potential_candidates[0]
        print(f"âš ï¸ No direct registry match. Using raw hint: {fallback}", flush=True)
        return fallback

    return None

# --- 3. Audio Pipeline ---

def run_ffmpeg_command(cmd, task_name):
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ FFmpeg Error during '{task_name}':\n{e.stderr}", flush=True)
        raise e

def mix_audio_ffmpeg(file_paths):
    print(f"ğŸ›ï¸ Mixing {len(file_paths)} tracks...", flush=True)
    output_path = os.path.abspath(os.path.join(TEMP_DIR, "final_mix.mp3"))
    inputs = []
    valid_files = [f for f in file_paths if f.lower().endswith(('.mp3', '.wav', '.flac', '.m4a', '.aac'))]
    if not valid_files: raise Exception("No audio files.")
    for f in valid_files: inputs.extend(['-i', f])
    filter_part = ['-filter_complex', f'amix=inputs={len(valid_files)}:duration=longest'] if len(valid_files) > 1 else []
    cmd = ['ffmpeg', '-y'] + inputs + filter_part + ['-ac', '1', '-b:a', '64k', output_path]
    run_ffmpeg_command(cmd, "Mixing Audio")
    return output_path

def split_audio_ffmpeg(input_path):
    print("ğŸ”ª Splitting...", flush=True)
    output_pattern = os.path.join(TEMP_DIR, "chunk_%03d.mp3")
    cmd = ['ffmpeg', '-y', '-i', input_path, '-f', 'segment', '-segment_time', str(CHUNK_LENGTH), '-ac', '1', '-b:a', '64k', output_pattern]
    run_ffmpeg_command(cmd, "Splitting Audio")
    return sorted(glob.glob(os.path.join(TEMP_DIR, "chunk_*.mp3")))

def transcribe_with_groq(chunk_paths):
    full_transcript = ""
    for chunk in chunk_paths:
        if not chunk.endswith(".mp3"): continue
        print(f"ğŸš€ Groq Transcribing: {os.path.basename(chunk)}", flush=True)
        max_retries = 50
        for attempt in range(max_retries):
            try:
                with open(chunk, "rb") as file:
                    res = groq_client.audio.transcriptions.create(
                        file=(os.path.basename(chunk), file),
                        model="whisper-large-v3", language="ja", response_format="text"
                    )
                full_transcript += res + "\n"
                break 
            except Exception as e:
                err_str = str(e).lower()
                if "429" in err_str or "rate limit" in err_str:
                    wait = 70
                    print(f"â³ Groq Limit. Waiting {wait}s... ({attempt+1}/{max_retries})", flush=True)
                    time.sleep(wait)
                else: 
                    log_error("Groq Transcription Failed", e)
                    raise e
        else: raise Exception("âŒ Groq Rate Limit persists. Aborting.")
    return full_transcript

# --- 4. Intelligence Analysis (Dynamic Expert Mode) ---

def analyze_text_with_gemini(transcript_text, date_hint, raw_name_hint):
    client = genai.Client(api_key=os.getenv("GEMINI_API_KEY")) # Re-init to be safe
    print(f"ğŸ§  Gemini Analyzing using [{RESOLVED_MODEL_ID}]...", flush=True)
    
    hint_context = f"éŒ²éŸ³æ—¥æ™‚: {date_hint}"
    if raw_name_hint:
        hint_context += f"\nã€é‡è¦ã€‘ãƒ•ã‚¡ã‚¤ãƒ«åãƒ’ãƒ³ãƒˆ: '{raw_name_hint}' (ã“ã‚Œã‚’æœ€å„ªå…ˆã§ç”Ÿå¾’åã¨ã—ã¦æ¡ç”¨ã›ã‚ˆ)"
    
    glossary_instruction = ""
    if COMMON_TERMS:
        glossary_instruction = f"\nã€é‡è¦å‚ç…§ï¼šã‚¹ãƒãƒ–ãƒ©ç”¨èªé›†ã€‘\nèª¤å­—è¨‚æ­£ç”¨è¾æ›¸ã§ã™ã€‚ä»¥ä¸‹ã®å®šç¾©ã«åŸºã¥ãå°‚é–€ç”¨èªã‚’è£œæ­£ã›ã‚ˆã€‚\n{COMMON_TERMS}\n"

    # â˜… V130.1 PROMPT (High-Fidelity Logic Extraction - FULL)
    prompt = f"""
    ã‚ãªãŸã¯è«–ç†çš„ãªæ›¸è¨˜å®˜ã§ã‚ã‚Šã€æ§‹é€ åŒ–ã®ã‚¹ãƒšã‚·ãƒ£ãƒªã‚¹ãƒˆã§ã™ã€‚
    æä¾›ã•ã‚ŒãŸä¼šè©±ãƒ‡ãƒ¼ã‚¿ï¼ˆæŒ‡å°ãƒ­ã‚°ï¼‰ã‹ã‚‰ã€æŒ‡å°å†…å®¹ã‚’å¿ å®Ÿã«æŠ½å‡ºã—ã€Notionç”¨ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

    ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æƒ…å ±ã€‘
    {hint_context}
    {glossary_instruction}

    ã€é‡è¦ï¼šåˆ†æãƒ»å‡ºåŠ›ã®çµ¶å¯¾åˆ¶ç´„ã€‘
    1. **äº‹å®Ÿã¸ã®å¿ å®Ÿæ€§ (No Hallucination):**
       - ä¼šè©±ã«å«ã¾ã‚Œã¦ã„ãªã„ã€Œè¤’ã‚è¨€è‘‰ã€ã‚„ã€Œæ„Ÿæƒ…çš„è¡¨ç¾ã€ã‚’AIãŒå‹æ‰‹ã«å‰µä½œã™ã‚‹ã“ã¨ã‚’å³ç¦ã¨ã™ã‚‹ã€‚
       - ä¼šè©±ã«å«ã¾ã‚Œã¦ã„ãªã„ã€Œæœ€æ‚ªã®æœªæ¥ï¼ˆè² ã‘ã‚‹ã€å¼•é€€ã™ã‚‹ãªã©ï¼‰ã€ã‚’å‹æ‰‹ã«äºˆæ¸¬ãƒ»è¨˜è¿°ã™ã‚‹ã“ã¨ã‚’å³ç¦ã¨ã™ã‚‹ã€‚
       - ã‚ãã¾ã§ã€Œä¼šè©±ã®ä¸­ã§å®Ÿéš›ã«æŒ‡æ‘˜ã•ã‚ŒãŸè«–ç†ã€ã®ã¿ã‚’æ§‹é€ åŒ–ã›ã‚ˆã€‚

    2. **è§£é‡ˆã®ç¦æ­¢:**
       - æ›–æ˜§ãªç™ºè¨€ã‚’ç„¡ç†ã«ãƒã‚¸ãƒ†ã‚£ãƒ–/ãƒã‚¬ãƒ†ã‚£ãƒ–ã«è§£é‡ˆã›ãšã€ç™ºè¨€ã®æ„å›³ï¼ˆãƒ­ã‚¸ãƒƒã‚¯ï¼‰ã‚’ãã®ã¾ã¾è¨˜è¿°ã›ã‚ˆã€‚
       - æŒ‡å°è€…ãŒã€Œè‰¯ã„ã€ã¨è¨€ã£ãŸç®‡æ‰€ã®ã¿ã‚’ã€Œè‰¯ã„ç‚¹ã€ã¨ã—ã€ã€Œç›´ã™ã¹ãã€ã¨è¨€ã£ãŸç®‡æ‰€ã®ã¿ã‚’ã€Œèª²é¡Œã€ã¨ã›ã‚ˆã€‚

    ã€é‡è¦å‚ç…§ï¼šã‚«ãƒ†ã‚´ãƒªå®šç¾©ã€‘
    ãƒ¬ãƒãƒ¼ãƒˆã®ã€Œãƒˆãƒ”ãƒƒã‚¯åã€ã‚’æ±ºå®šã™ã‚‹éš›ã¯ã€ä»¥ä¸‹ã®å®šç¾©ã«æœ€ã‚‚åˆè‡´ã™ã‚‹ã‚‚ã®ã‚’é¸ã¹ã€‚
    **æŠ€è¡“é¢ï¼ˆ1-10ï¼‰ã¨ã€ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼é¢ï¼ˆ11-13ï¼‰ã‚’æ˜ç¢ºã«åŒºåˆ¥ã™ã‚‹ã“ã¨ã€‚ã¾ãŸã€åŒä¸€ç•ªå·ã®è©±é¡Œã§ã‚‚è¤‡æ•°å€‹ã®ãƒˆãƒ”ãƒƒã‚¯ã®è©±ãŒå‡ºã¦ã„ã‚‹å ´åˆã¯ã€åˆ†ã‘ã‚‹ã“ã¨ã€‚ä¾‹ï¼šç€åœ°ç‹©ã‚Šæ–‡è„ˆã§ã€å›é¿ç€åœ°ã‚’ç‹©ã‚Œã¦ã„ãªã„ã“ã¨ã€ã¨ã€ç›¸æ‰‹ãŒã‚¸ãƒ£ãƒ³ãƒ—ã—ãŸå…ˆã‚’è¿½ã†å‹•ããŒå‡ºæ¥ã¦ã„ãªã„ã€ãŒã‚ã£ãŸå ´åˆã¯ã€åˆ¥ã€…ã«åˆ†è§£ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚**

    -- ã‚²ãƒ¼ãƒ å†…æŠ€è¡“ (In-Game) --
    1. **å·®ã—åˆã„:** ãŠäº’ã„ãŒåœ°ä¸Šã¾ãŸã¯ç©ºä¸­ã«ã„ã¦ã€æœ‰åˆ©ä¸åˆ©ãŒãªã„çŠ¶æ…‹ã€‚æŠ€ã®å½“ã¦åˆã„ã€‚
    2. **ç€åœ°:** ã€å®ˆã‚Šã€‘è‡ªåˆ†ãŒç©ºä¸­ã«ã„ã¦ã€ã‚¹ãƒ†ãƒ¼ã‚¸ã«æˆ»ã‚ã†ã¨ã—ã¦ã„ã‚‹çŠ¶æ…‹ã€‚
    3. **ç€åœ°ç‹©ã‚Š:** ã€æ”»ã‚ã€‘ç›¸æ‰‹ãŒæµ®ã„ã¦ã„ã¦ã€ãã‚Œã‚’è¿½æ’ƒã—ã¦ã„ã‚‹çŠ¶æ…‹ã€‚
    4. **å¾©å¸°:** ã€å®ˆã‚Šã€‘è‡ªåˆ†ãŒã‚¹ãƒ†ãƒ¼ã‚¸å¤–ã«é£›ã°ã•ã‚Œã€å´–ã‚„ã‚¹ãƒ†ãƒ¼ã‚¸ä¸Šã«æˆ»ã‚ã†ã¨ã—ã¦ã„ã‚‹çŠ¶æ…‹ã€‚
    5. **å¾©å¸°é˜»æ­¢:** ã€æ”»ã‚ã€‘ç›¸æ‰‹ãŒã‚¹ãƒ†ãƒ¼ã‚¸å¤–ã«ã„ã¦ã€æˆ»ã£ã¦ãã‚‹ã¨ã“ã‚ã‚’é˜»æ­¢ãƒ»æ’ƒå¢œã—ã‚ˆã†ã¨ã™ã‚‹çŠ¶æ…‹ã€‚
    6. **å´–ä¸ŠãŒã‚Š:** ã€å®ˆã‚Šã€‘è‡ªåˆ†ãŒå´–ã«æ´ã¾ã£ã¦ã„ã‚‹çŠ¶æ…‹ã‹ã‚‰ã€ãƒ©ã‚¤ãƒ³å¾©å¸°ã‚’ç›®æŒ‡ã™çŠ¶æ…‹ã€‚
    7. **å´–ç‹©ã‚Š:** ã€æ”»ã‚ã€‘ç›¸æ‰‹ãŒå´–ã«æ´ã¾ã£ã¦ã„ã¦ã€ãã®ä¸ŠãŒã‚Šéš›ã‚’ç‹©ã‚ã†ã¨ã™ã‚‹çŠ¶æ…‹ã€‚
    8. **ãƒ€ã‚¦ãƒ³ç‹©ã‚Š/å—ã‘èº«:** ç›¸æ‰‹ã¾ãŸã¯è‡ªåˆ†ãŒãƒ€ã‚¦ãƒ³ï¼ˆè»¢å€’ï¼‰ã—ãŸéš›ã®æ”»é˜²ã€‚
    9. **æ’ƒå¢œ:** ä¸Šè¨˜ãƒ•ã‚§ãƒ¼ã‚ºã®ä¸­ã§ã€ç‰¹ã«ã€Œç›¸æ‰‹ã‚’ãƒãƒ¼ã‚¹ãƒˆã™ã‚‹ã“ã¨ã€ã«ç‰¹åŒ–ã—ãŸè­°è«–ã€‚
    10. **æ’ƒå¢œæ‹’å¦:** ä¸Šè¨˜ãƒ•ã‚§ãƒ¼ã‚ºã®ä¸­ã§ã€ç‰¹ã«ã€Œè‡ªåˆ†ãŒãƒãƒ¼ã‚¹ãƒˆã•ã‚Œãªã„ã“ã¨ã€ã«ç‰¹åŒ–ã—ãŸè­°è«–ã€‚

    -- ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ç®¡ç† (Meta-Game) --
    11. **ãƒã‚¤ãƒ³ãƒ‰ã‚»ãƒƒãƒˆ:** è©¦åˆä¸­ã®ãƒ¡ãƒ³ã‚¿ãƒ«åˆ¶å¾¡ã€ç·Šå¼µã€æ€’ã‚Šã€è‡ªä¿¡ã€å¤§ä¼šã¸ã®å¿ƒæ§‹ãˆãªã©ã€‚
    12. **ä½“èª¿ç®¡ç†:** ç¡çœ ã€é£Ÿäº‹ã€ç›®ã®ç–²ã‚Œã€å§¿å‹¢ã€ã‚«ãƒ•ã‚§ã‚¤ãƒ³æ‘‚å–ã€ç”Ÿæ´»ãƒªã‚ºãƒ ãªã©ã€è‚‰ä½“çš„ãªã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒ³ã€‚
    13. **å–ã‚Šçµ„ã¿/åº§å­¦:** ç·´ç¿’ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã€ãƒªãƒ—ãƒ¬ã‚¤åˆ†æã®è³ªã€ç›®æ¨™è¨­å®šã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã€å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹å…¨èˆ¬ã€‚

    ---
    **ã€é‡è¦ï¼šå‡ºåŠ›ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¦å‰‡ã€‘**
    * Notionã§è¦‹ã‚„ã™ã„éšå±¤æ§‹é€ ã‚’ä½œã‚‹ã“ã¨ã€‚
    * **æ§‹é€ :**
        * ãƒˆãƒ”ãƒƒã‚¯å: `##` (H2) â€»ä¸Šè¨˜ã®ã‚«ãƒ†ã‚´ãƒªå®šç¾©ã‹ã‚‰é¸æŠã™ã‚‹ã“ã¨ã€‚
        * è©³ç´°é …ç›®ã®è¦‹å‡ºã—: `###` (H3) â€»ç®‡æ¡æ›¸ãã®ãƒªã‚¹ãƒˆè¨˜å·(-)ã‚„å¤ªå­—(**)ã¯ä½¿ã‚ãšã€ã‚·ãƒ³ãƒ—ãƒ«ã«è¦‹å‡ºã—ã«ã™ã‚‹ã€‚
    * **ä½™ç™½:** å„é …ç›®ã®é–“ã«ã¯å¿…ãšã€Œç©ºè¡Œã€ã‚’å…¥ã‚Œã‚‹ã“ã¨ã€‚

    **ã€Section 1: è©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆã€‘**
    ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¸»è¦ãªæ”¹å–„ãƒã‚¤ãƒ³ãƒˆã‚’æŠ½å‡ºã—ã€å„ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ä»¥ä¸‹ã®3ã¤ã®è¦³ç‚¹ã‚’å¿…ãšç¶²ç¾…ã—ã¦è¨˜è¿°ã›ã‚ˆã€‚çœç•¥ã¯è¨±ã•ã‚Œãªã„ã€‚

    ## [ã‚«ãƒ†ã‚´ãƒªå: å…·ä½“çš„ãªå†…å®¹] (ä¾‹: ## ä½“èª¿ç®¡ç†: å¤§ä¼šå‰ã®ç¡çœ ãƒªã‚ºãƒ )

    ### â‘  ç¾çŠ¶ã®æŒ™å‹• (Fact)
    ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒã©ã®ã‚ˆã†ãªå‹•ãã‚’ã—ã¦ã„ãŸã‹ã€ã‚ã‚‹ã„ã¯ã©ã®ã‚ˆã†ãªèªè­˜ã‚’æŒã£ã¦ã„ãŸã‹ã€‚
    ä¼šè©±å†…ã§è¨€åŠã•ã‚ŒãŸäº‹å®Ÿã®ã¿ã‚’è¨˜è¿°ã™ã‚‹ã€‚
    â€»æŒ‡å°è€…ãŒè‚¯å®šçš„ã«è©•ä¾¡ã—ãŸéƒ¨åˆ†ãŒã‚ã‚Œã°ã€ã“ã“ã«ã€Œè©•ä¾¡ç‚¹ã€ã¨ã—ã¦è¨˜è¼‰ã™ã‚‹ã€‚ï¼ˆç„¡ã‘ã‚Œã°è¨˜è¼‰ã—ãªã„ã“ã¨ï¼‰

    ### â‘¡ æŒ‡æ‘˜äº‹é … (Logic)
    ãã®æŒ™å‹•ã«å¯¾ã—ã¦ã€ã©ã®ã‚ˆã†ãªä¿®æ­£æŒ‡ç¤ºã‚„è«–ç†çš„æŒ‡æ‘˜ãŒå…¥ã£ãŸã‹ã€‚
    ã€Œãªãœãã‚ŒãŒãƒ€ãƒ¡ãªã®ã‹ã€ã€Œã©ã†ã‚ã‚‹ã¹ãã‹ã€ã¨ã„ã†æŒ‡å°è€…ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã‚‹ã€‚

    ### â‘¢ æ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ (Action)
    å…·ä½“çš„ã«ã©ã†å‹•ãã¹ãã‹ã€ä½•ã‚’æ„è­˜ã™ã¹ãã‹ã€‚

    **ã€Section 2: èª²é¡Œã‚»ãƒƒãƒˆã€‘**
    èª²é¡Œã‚’ç®‡æ¡æ›¸ãã›ã‚ˆã€‚çœç•¥ã¯è¨±ã•ã‚Œãªã„ã€‚
    ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«é–¢ã—ã¦ã€æŠ€è¡“é¢(1~10)ã®å ´åˆã¯ã€
    ã€ŒçŠ¶æ³(è·é›¢&ã‚¿ã‚¤ãƒŸãƒ³ã‚°&ãã®ä»–æƒ…å ±ã«è¨€åŠ)ã€â†’ã€Œæ­£è§£è¡Œå‹•(ã‚­ãƒ£ãƒ©ã®å‹•ã + è„³ã®å‹•ã)ã€ã®å½¢ã€‚è¡Œå‹•ãŒï¼’æ®µéšã«åˆ†ã‹ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚‚è¨˜è¼‰ã™ã‚‹ã€‚

    ä¾‹:
    ç€åœ°ç‹©ã‚Š æ€¥é™ä¸‹å›é¿ç€åœ°ç‹©ã‚Š
    çŠ¶æ³ï¼šç›¸æ‰‹å¤§ã‚¸ãƒ£ãƒ³ãƒ—1å€‹åˆ†ä¸Š ç›¸æ‰‹ã‚¸ãƒ£ãƒ³ãƒ—ãªã—
    æ­£è§£ï¼šå¼•ãã‚¹ãƒ†(æš´ã‚Œã‚±ã‚¢) + æš´ã‚Œ&å›é¿ç¢ºèª â†’ DAå·®ã—è¿”ã— or æ¨ªã‚¹ãƒ

    ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼é¢(11~13)ã®å ´åˆã¯ã€Œãƒˆãƒªã‚¬ãƒ¼(äº‹è±¡)ã€â†’ã€Œã‚¢ã‚¯ã‚·ãƒ§ãƒ³(å¯¾å‡¦)ã€ã¨ã™ã‚‹ã€‚

    **ã€Section 3: æ™‚ç³»åˆ—ãƒ­ã‚°ã€‘**
    ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æµã‚Œã‚’æ™‚ç³»åˆ—ã§è¦ç´„ã›ã‚ˆã€‚

    **ã€Section 4: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSONã€‘**
    {{
      "student_name": "ç”Ÿå¾’å",
      "date": "YYYY-MM-DD",
      "next_action": "æœ€å„ªå…ˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³"
    }}

    **ã€Section 5: æ€è€ƒãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ (Mermaid)ã€‘**
    Section 1ã§åˆ†æã—ãŸã€Œåˆ¤æ–­ã®åˆ†å²ã€ã‚„ã€Œæ”¹å–„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒ—ãƒ­ã‚»ã‚¹ã€ã‚’ã€Mermaidè¨˜æ³•ï¼ˆflowchart TDï¼‰ã§è¦–è¦šåŒ–ã›ã‚ˆã€‚
    
    **ï¼œMermaidä½œæˆã®çµ¶å¯¾ãƒ«ãƒ¼ãƒ«ï¼ˆã‚¨ãƒ©ãƒ¼é˜²æ­¢ï¼‰ï¼**
    1. **æ§‹æ–‡:** `graph TD` ã‚’ä½¿ç”¨ã™ã‚‹ã€‚
    2. **ç¦æ­¢æ–‡å­—:** ãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ `"`ã€**åŠè§’ã‚³ãƒ³ãƒ `,`**ã€**åŠè§’ã‚«ãƒƒã‚³ `()`** ã¯çµ¶å¯¾ã«ç¦æ­¢ã€‚
    3. **ä»£æ›¿æ–‡å­—:** å¥èª­ç‚¹ã¯å…¨è§’ã® `ã€` `ã€‚` ã‚’ã€ã‚«ãƒƒã‚³ã¯å…¨è§’ã® `ï¼ˆ` `ï¼‰` ã‚’ä½¿ç”¨ã›ã‚ˆã€‚
    4. **å†…å®¹:** å˜ãªã‚‹é …ç›®ã®ç¾…åˆ—ã§ã¯ãªãã€**ã€ŒCheckï¼ˆåˆ¤æ–­ï¼‰ã€â†’ã€ŒBranchï¼ˆåˆ†å²ï¼‰ã€â†’ã€ŒActionï¼ˆè¡Œå‹•ï¼‰ã€**ã®æµã‚Œã‚’æãã“ã¨ã€‚
    5. **å½¢çŠ¶:** åˆ¤æ–­/åˆ†å²ã«ã¯ã²ã—å½¢ {{ }}ã€å‡¦ç†/è¡Œå‹•ã«ã¯å››è§’ [ ] ã‚’æ­£ã—ãä½¿ã„åˆ†ã‘ã‚‹ã“ã¨ã€‚

    ---
    **å‡ºåŠ›ãƒ–ãƒ­ãƒƒã‚¯ï¼ˆã‚·ã‚¹ãƒ†ãƒ åˆ¶å¾¡ç”¨ã‚¿ã‚°ï¼‰ï¼š**
    
    **[DETAILED_REPORT_START]**
    (Section 1 ã¨ Section 2 ã®å†…å®¹)
    **[DETAILED_REPORT_END]**

    **[RAW_LOG_START]**
    (Section 3 ã®å†…å®¹)
    **[RAW_LOG_END]**

    **[JSON_START]**
    (Section 4 ã®JSON)
    **[JSON_END]**

    **[MERMAID_START]**
    (Section 5 ã®Mermaidã‚³ãƒ¼ãƒ‰ã®ã¿ã€‚ãƒãƒƒã‚¯ã‚¯ã‚©ãƒ¼ãƒˆä¸è¦)
    **[MERMAID_END]**
    ---

    ã€å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã€‘
    {transcript_text}
    """

    max_retries = 10
    for attempt in range(max_retries):
        try:
            response = client.models.generate_content(model=RESOLVED_MODEL_ID, contents=prompt)
            text = response.text.strip()
            break 
        except Exception as e:
            err_str = str(e).lower()
            if "429" in err_str or "quota" in err_str or "overloaded" in err_str:
                wait = 60 * (attempt + 1)
                print(f"â³ Gemini Busy ({RESOLVED_MODEL_ID}). Waiting {wait}s...", flush=True)
                time.sleep(wait)
            else:
                log_error("Gemini Analysis Failed", e)
                return {"student_name": "AnalysisError", "date": datetime.now().strftime('%Y-%m-%d')}, f"Analysis Error: {e}", transcript_text[:2000], None
    else: return {"student_name": "QuotaError", "date": datetime.now().strftime('%Y-%m-%d')}, "Quota Limit Exceeded", transcript_text[:2000], None

    def extract_safe(s, e, src):
        m = re.search(f'{re.escape(s)}(.*?){re.escape(e)}', src, re.DOTALL)
        return m.group(1).strip() if m else None

    report = extract_safe("[DETAILED_REPORT_START]", "[DETAILED_REPORT_END]", text)
    time_log = extract_safe("[RAW_LOG_START]", "[RAW_LOG_END]", text)
    json_str = extract_safe("[JSON_START]", "[JSON_END]", text)
    mermaid_code = extract_safe("[MERMAID_START]", "[MERMAID_END]", text)

    if not report:
        print("âš ï¸ Warning: Missing REPORT tags. Fallback...", flush=True)
        if "[RAW_LOG_START]" in text:
            report = text.split("[RAW_LOG_START]")[0].replace("[DETAILED_REPORT_START]", "").strip()
        else:
            report = text

    if not time_log: time_log = "Log tags missing."
    
    if not mermaid_code:
        m_match = re.search(r'```mermaid(.*?)```', text, re.DOTALL)
        if m_match: mermaid_code = m_match.group(1).strip()
    
    if mermaid_code:
        mermaid_code = mermaid_code.replace("**", "").replace("```mermaid", "").replace("```", "").strip()

    try: 
        if json_str: data = json.loads(json_str)
        else: raise ValueError("No JSON block")
    except: 
        try:
            json_candidate = re.search(r'\{.*"student_name".*\}', text, re.DOTALL)
            if json_candidate: data = json.loads(json_candidate.group(0))
            else: data = {"student_name": "Unknown", "date": datetime.now().strftime('%Y-%m-%d'), "next_action": "Check Logs"}
        except:
            data = {"student_name": "Unknown", "date": datetime.now().strftime('%Y-%m-%d'), "next_action": "Check Logs"}
            
    return data, report, time_log, mermaid_code
def text_to_notion_blocks(text):
    blocks = []
    lines = text.split('\n')
    
    for line in lines:
        if not line.strip():
            blocks.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {"rich_text": []} 
            })
            continue
        
        if line.startswith('|') or line.startswith('+-'):
             continue 

        clean_content = line[:1900] 
        
        if line.startswith('### '):
            blocks.append({
                "object": "block",
                "type": "heading_3",
                "heading_3": {"rich_text": [{"type": "text", "text": {"content": clean_content[4:]}}]}
            })
        elif line.startswith('## '):
            blocks.append({
                "object": "block",
                "type": "heading_2",
                "heading_2": {"rich_text": [{"type": "text", "text": {"content": clean_content[3:]}}]}
            })
        elif line.startswith('# '):
            blocks.append({
                "object": "block",
                "type": "heading_1",
                "heading_1": {"rich_text": [{"type": "text", "text": {"content": clean_content[2:]}}]}
            })
        elif line.startswith('- ') or line.startswith('* '):
            blocks.append({
                "object": "block",
                "type": "bulleted_list_item",
                "bulleted_list_item": {"rich_text": [{"type": "text", "text": {"content": clean_content[2:]}}]}
            })
        else:
            blocks.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {"rich_text": [{"type": "text", "text": {"content": clean_content}}]}
            })
            
    return blocks

# --- 5. Asset Management ---

def notion_create_page_heavy(db_id, props, children):
    print(f"ğŸ“¤ Posting to Notion DB: {db_id}...", flush=True)
    res = requests.post("https://api.notion.com/v1/pages", headers=HEADERS, json={"parent": {"database_id": db_id}, "properties": props, "children": children[:100]})
    if res.status_code != 200:
        print(f"âš ï¸ Initial Post Failed ({res.status_code}). Retrying with SAFE MODE...", flush=True)
        safe_props = {}
        for key, val in props.items():
            if "title" in val: safe_props[key] = val; break
        if not safe_props:
             content_text = props.get("åå‰", {}).get("title", [{}])[0].get("text", {}).get("content", "Log")
             safe_props = {"Name": {"title": [{"text": {"content": content_text}}]}}
        date_val = props.get("æ—¥ä»˜", {}).get("date", {}).get("start", "Unknown")
        error_note = {"object": "block", "type": "callout", "callout": {"rich_text": [{"text": {"content": f"âš ï¸ Date Prop Missing. Date: {date_val}"}}]}}
        children.insert(0, error_note)
        res = requests.post("https://api.notion.com/v1/pages", headers=HEADERS, json={"parent": {"database_id": db_id}, "properties": safe_props, "children": children[:100]})
        if res.status_code != 200:
            print(f"âŒ NOTION SAFE MODE FAILED: {res.status_code}\n{res.text}", flush=True)
            return

    response_data = res.json()
    pid = response_data.get('id')
    print(f"ğŸ”— Notion Page Created: {response_data.get('url')}", flush=True)
    if pid and len(children) > 100:
        for i in range(100, len(children), 100):
            requests.patch(f"https://api.notion.com/v1/blocks/{pid}/children", headers=HEADERS, json={"children": children[i:i+100]})

def ensure_processed_folder():
    try:
        q = f"name='processed_coaching_logs' and '{INBOX_FOLDER_ID}' in parents"
        folders = drive_service.files().list(q=q).execute().get('files', [])
        if folders: return folders[0]['id']
        folder = drive_service.files().create(body={'name': 'processed_coaching_logs', 'mimeType': 'application/vnd.google-apps.folder', 'parents': [INBOX_FOLDER_ID]}, fields='id').execute()
        return folder.get('id')
    except Exception as e:
        log_error("Failed to Get/Create Processed Folder", e)
        return INBOX_FOLDER_ID

def upload_file_to_drive(local_path, folder_id, rename_to, mime_type):
    print(f"ğŸ“¤ Uploading {rename_to}...", flush=True)
    try:
        media = MediaFileUpload(local_path, mimetype=mime_type, resumable=True, chunksize=100*1024*1024)
        drive_service.files().create(
            body={'name': rename_to, 'parents': [folder_id]}, 
            media_body=media, 
            fields='id',
            supportsAllDrives=True
        ).execute()
        print("âœ… Upload Complete.", flush=True)
    except Exception as e:
        log_error(f"Upload Failed for {rename_to}", e)

def move_original_file(file_id, folder_id):
    if folder_id == INBOX_FOLDER_ID:
        print("âš ï¸ Skipping Move: Destination is Inbox.", flush=True)
        return
    try:
        prev_parents = drive_service.files().get(fileId=file_id, fields='parents').execute().get('parents', [])
        prev_str = ",".join(prev_parents)
        drive_service.files().update(
            fileId=file_id, 
            addParents=folder_id, 
            removeParents=prev_str,
            supportsAllDrives=True
        ).execute()
        print(f"ğŸ“¦ Archived original file to folder [{folder_id}].", flush=True)
    except Exception as e:
        log_error(f"Move Original File Failed (ID: {file_id})", e)
        print(f"ğŸ‘‰ TIP: Add this email to folder permissions: {BOT_EMAIL}", flush=True)

# --- Main ---
def main():
    print("--- SZ AUTO LOGGER ULTIMATE (v130.0 - Dynamic Spec Selection) ---", flush=True)
    
    # æ¥ç¶šãƒ†ã‚¹ãƒˆæ¸ˆã¿ã®RESOLVED_MODEL_IDãŒã™ã§ã«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã•ã‚Œã¦ã„ã‚‹çŠ¶æ…‹ã§é–‹å§‹
    if not RESOLVED_MODEL_ID:
        print("âŒ Model Selection Failed during Setup. Aborting.")
        return

    load_student_registry()
    
    try:
        files = drive_service.files().list(
            q=f"'{INBOX_FOLDER_ID}' in parents and trashed=false and mimeType!='application/vnd.google-apps.folder'",
            fields="files(id, name, createdTime)"
        ).execute().get('files', [])
    except Exception: return

    if not files: print("â„¹ï¸ No files."); return

    for file in files:
        try:
            print(f"\nğŸ“‚ Processing: {file['name']}")
            safe_name = sanitize_filename(file['name'])
            fpath = os.path.join(TEMP_DIR, safe_name)
            
            # Download
            max_dl_retries = 3
            for dl_attempt in range(max_dl_retries):
                try:
                    with open(fpath, "wb") as f:
                        request = drive_service.files().get_media(fileId=file['id'])
                        downloader = MediaIoBaseDownload(f, request, chunksize=100*1024*1024)
                        done = False
                        while done is False:
                            status, done = downloader.next_chunk()
                            print(f"   â¬‡ï¸ Downloading... {int(status.progress() * 100)}%", end="\r", flush=True)
                    print("\nâœ… Download Complete.")
                    break 
                except Exception as e:
                    print(f"\nâš ï¸ Download Interrupted: {e}. Retrying...", flush=True)
                    time.sleep(5)
            else:
                print("âŒ Download Failed. Skipping.")
                continue

            srcs = []
            candidate_raw_name = None

            if safe_name.endswith('.zip'):
                try:
                    patoolib.extract_archive(fpath, outdir=TEMP_DIR)
                    extracted_files = []
                    for r, _, fs in os.walk(TEMP_DIR):
                        for af in fs:
                            full_p = os.path.join(r, af)
                            extracted_files.append(full_p)
                            if af.lower().endswith(('.flac', '.mp3', '.m4a', '.wav')) and 'final_mix' not in af and 'chunk' not in af:
                                srcs.append(full_p)
                    candidate_raw_name = detect_student_candidate_raw(extracted_files, file['name'])
                except Exception as e:
                    log_error(f"Archive Extraction Failed", e)
                    continue
            else: srcs.append(fpath)
            
            if not srcs: print("â„¹ï¸ No audio files found."); continue
            
            # Processing
            precise_datetime, date_only = extract_date_smart(file['name'], file.get('createdTime'))
            mixed = mix_audio_ffmpeg(srcs)
            chunks = split_audio_ffmpeg(mixed)
            full_text = transcribe_with_groq(chunks)
            
            # Analysis
            meta, report, logs, mermaid_code = analyze_text_with_gemini(full_text, precise_datetime, candidate_raw_name)
            
            # DB Matching
            did, oname = find_best_student_match(meta['student_name'])
            
            # --- Build Notion Blocks (UPDATED) ---
            final_blocks = []

            # 1. Detailed Report
            report_header = "### ğŸ“Š SZãƒ¡ã‚½ãƒƒãƒ‰è©³ç´°åˆ†æ\n\n" + report
            final_blocks.extend(text_to_notion_blocks(report_header))

            # 2. Mermaid Block
            if mermaid_code:
                final_blocks.append({"object": "block", "type": "divider", "divider": {}})
                final_blocks.append({
                    "object": "block", 
                    "type": "heading_2", 
                    "heading_2": {"rich_text": [{"text": {"content": "ğŸ§  æ€è€ƒãƒ•ãƒ­ãƒ¼ãƒãƒ£ãƒ¼ãƒˆ"}}]}
                })
                final_blocks.append({
                    "object": "block",
                    "type": "callout",
                    "callout": {
                        "rich_text": [{"text": {"content": "ä¸Šã®åˆ†æå†…å®¹ã‚’æ§‹é€ åŒ–ã—ãŸã‚‚ã®ã§ã™ã€‚åˆ¤æ–­ã«è¿·ã£ãŸæ™‚ã®åœ°å›³ã¨ã—ã¦ä½¿ã£ã¦ãã ã•ã„ã€‚"}}],
                        "icon": {"emoji": "ğŸ—ºï¸"}
                    }
                })
                final_blocks.append({
                    "object": "block",
                    "type": "code",
                    "code": {
                        "rich_text": [{"type": "text", "text": {"content": mermaid_code}}],
                        "language": "mermaid" 
                    }
                })

            # 3. Logs
            logs_content = f"\n---\n\n### ğŸ“ æ™‚ç³»åˆ—ãƒ­ã‚°\n\n{logs}"
            final_blocks.extend(text_to_notion_blocks(logs_content))

            # 4. Transcript
            final_blocks.append({"object": "block", "type": "divider", "divider": {}})
            final_blocks.append({"object": "block", "type": "heading_3", "heading_3": {"rich_text": [{"text": {"content": "ğŸ“œ å…¨æ–‡æ–‡å­—èµ·ã“ã—"}}]}})
            
            for i in range(0, len(full_text), 1900):
                chunk_text = full_text[i:i+1900]
                final_blocks.append({"object": "block", "type": "paragraph", "paragraph": {"rich_text": [{"text": {"content": chunk_text}}]}})
            
            props = {
                "åå‰": {"title": [{"text": {"content": f"{precise_datetime} {oname} é€šè©±ãƒ­ã‚°"}}]}, 
                "æ—¥ä»˜": {"date": {"start": date_only}}
            }

            print("ğŸ’¾ Saving to Fallback DB (All Data)...")
            notion_create_page_heavy(sanitize_id(FINAL_FALLBACK_DB_ID), copy.deepcopy(props), copy.deepcopy(final_blocks))
            
            if did and did != FINAL_FALLBACK_DB_ID:
                print(f"ğŸ‘¤ Saving to Student DB ({oname})...")
                notion_create_page_heavy(sanitize_id(did), copy.deepcopy(props), copy.deepcopy(final_blocks))
            
            # Artifacts
            processed_folder_id = ensure_processed_folder()
            safe_filename_time = precise_datetime.replace(':', '-').replace(' ', '_')
            
            upload_file_to_drive(mixed, processed_folder_id, f"{safe_filename_time}_{oname}_Full.mp3", 'audio/mpeg')
            
            txt_path = os.path.join(TEMP_DIR, "transcript.txt")
            with open(txt_path, "w") as f: f.write(full_text)
            upload_file_to_drive(txt_path, processed_folder_id, f"{safe_filename_time}_{oname}_Transcript.txt", 'text/plain')
            
            move_original_file(file['id'], processed_folder_id)

        except Exception as e:
            log_error(f"Processing Failed for {file['name']}", e)
            continue
        finally:
            if os.path.exists(TEMP_DIR): shutil.rmtree(TEMP_DIR); os.makedirs(TEMP_DIR)

if __name__ == "__main__": main()
