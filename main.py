import os
import sys
import time
import json
import shutil
import subprocess
import glob
import re
from datetime import datetime

# --- Libraries ---
import requests
import google.generativeai as genai
from groq import Groq
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
import zipfile

# --- Configuration ---
FINAL_CONTROL_DB_ID = "2b71bc8521e380868094ec506b41f664"
FINAL_FALLBACK_DB_ID = "2b71bc8521e38018a5c3c4b0c6b6627c"
TEMP_DIR = "temp_workspace"
CHUNK_LENGTH = 900  # 15åˆ† (Groq APIåˆ¶é™å›é¿)

# ==========================================
# Phase 1: ç´ æã®ç´”åŒ–ã¨æº–å‚™ (Input & Normalization)
# ==========================================

def setup_env():
    """ç’°å¢ƒåˆæœŸåŒ–: ä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€ã®æµ„åŒ–ã¨èªè¨¼è¨­å®š"""
    if os.path.exists(TEMP_DIR): shutil.rmtree(TEMP_DIR)
    os.makedirs(TEMP_DIR)
    
    if os.getenv("GCP_SA_KEY"):
        with open("service_account.json", "w") as f:
            f.write(os.getenv("GCP_SA_KEY"))

# ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Ÿè¡Œ
setup_env()

# ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
try:
    groq_client = Groq(api_key=os.getenv("GROQ_API_KEY"))
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
    
    NOTION_TOKEN = os.getenv("NOTION_TOKEN")
    HEADERS = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28"
    }
    
    creds = service_account.Credentials.from_service_account_file("service_account.json", scopes=['https://www.googleapis.com/auth/drive'])
    drive_service = build('drive', 'v3', credentials=creds)
    INBOX_FOLDER_ID = os.getenv("DRIVE_FOLDER_ID")
    
except Exception as e:
    print(f"âŒ Init Critical Error: {e}")
    sys.exit(1)


def sanitize_id(raw_id):
    if not raw_id: return None
    match = re.search(r'([a-fA-F0-9]{32})', str(raw_id).replace("-", ""))
    return match.group(1) if match else None


def mix_audio_ffmpeg(file_paths):
    """
    [å¼·åˆ¶æ­£è¦åŒ–]
    ã‚ã‚‰ã‚†ã‚‹éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆã—ã€GroqãŒç¢ºå®Ÿã«å‡¦ç†ã§ãã‚‹
    'ãƒ¢ãƒãƒ©ãƒ«ãƒ»64kbpsãƒ»MP3' ã«å¤‰æ›ã™ã‚‹ã€‚
    """
    print(f"ğŸ›ï¸ Mixing & Converting {len(file_paths)} tracks...", flush=True)
    output_path = os.path.abspath(os.path.join(TEMP_DIR, "final_mix.mp3"))
    
    inputs = []
    for f in file_paths: inputs.extend(['-i', f])
    
    filter_cmd = []
    if len(file_paths) > 1:
        filter_cmd = ['-filter_complex', f'amix=inputs={len(file_paths)}:duration=longest']
        
    cmd = ['ffmpeg', '-y'] + inputs + filter_cmd + ['-ac', '1', '-b:a', '64k', output_path]
    subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    return output_path


def split_audio_ffmpeg(input_path):
    """[åˆ†å‰²å‡¦ç†] 15åˆ†ã”ã¨ã«åˆ†å‰²ã—ã¦APIåˆ¶é™ã‚’å›é¿"""
    print("ğŸ”ª Splitting into chunks...", flush=True)
    output_pattern = os.path.join(TEMP_DIR, "chunk_%03d.mp3")
    
    cmd = [
        'ffmpeg', '-y', '-i', input_path, 
        '-f', 'segment', '-segment_time', str(CHUNK_LENGTH), 
        '-ac', '1', '-b:a', '64k', output_pattern
    ]
    subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    return sorted(glob.glob(os.path.join(TEMP_DIR, "chunk_*.mp3")))


def transcribe_with_groq(chunk_paths):
    """[é«˜é€Ÿãƒ†ã‚­ã‚¹ãƒˆåŒ–] åˆ†å‰²MP3ã‚’é †æ¬¡Whisperã«ã‹ã‘ã‚‹"""
    full_transcript = ""
    for chunk in chunk_paths:
        if not chunk.endswith(".mp3"): continue
        
        print(f"ğŸš€ Groq Transcribing: {os.path.basename(chunk)}", flush=True)
        with open(chunk, "rb") as file:
            transcription = groq_client.audio.transcriptions.create(
                file=(os.path.basename(chunk), file),
                model="whisper-large-v3",
                language="ja",
                response_format="text"
            )
            full_transcript += transcription + "\n"
    return full_transcript


# ==========================================
# Phase 2: çŸ¥èƒ½åˆ†æã¨æ§‹é€ åŒ– (The Brain)
# ==========================================

def analyze_text_with_gemini(transcript_text):
    """
    [SZãƒ¡ã‚½ãƒƒãƒ‰åˆ†æ]
    5è¦ç´ (Status, Problem, Root Cause, Solution, Next Action)ã‚’å³æ ¼ã«æŠ½å‡ºã€‚
    """
    print("ğŸ§  Gemini Analyzing (SZ Method - 5 Elements)...", flush=True)
    model = genai.GenerativeModel('gemini-1.5-flash')
    
    prompt = f"""
    ã‚ãªãŸã¯ä¸–ç•Œæœ€é«˜å³°ã®ã‚¹ãƒãƒ–ãƒ©ï¼ˆSuper Smash Bros.ï¼‰ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã‚ã‚Šã€è«–ç†çš„ã‹ã¤å†·å¾¹ãªã‚³ãƒ¼ãƒãƒ³ã‚°è¨˜éŒ²å®˜ã§ã™ã€‚
    æ¸¡ã•ã‚ŒãŸå¯¾è©±ãƒ­ã‚°ã‚’ç²¾èª­ã—ã€ä»¥ä¸‹ã®3ã¤ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å³å¯†ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

    **ã€Section 1: ãƒˆãƒ”ãƒƒã‚¯åˆ¥ãƒ»è©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆã€‘**
    ä¼šè©±ã®ä¸­ã§æ‰±ã‚ã‚ŒãŸã€Œä¸»è¦ãªãƒˆãƒ”ãƒƒã‚¯ï¼ˆä¾‹ï¼šå´–ä¸ŠãŒã‚Šç‹©ã‚Šã€ãƒ©ã‚¤ãƒ³ç®¡ç†ã€å¾©å¸°é˜»æ­¢ï¼‰ã€ã‚’ã™ã¹ã¦æŠ½å‡ºã—ã€
    **ãƒˆãƒ”ãƒƒã‚¯ã”ã¨ã«**ä»¥ä¸‹ã®5è¦ç´ ã‚’åŸ‹ã‚ã¦è¨˜è¿°ã™ã‚‹ã“ã¨ã€‚

    * **â‘  ç¾çŠ¶ (Status):** ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒç¾åœ¨è¡Œã£ã¦ã„ã‚‹è¡Œå‹•ã€ç™–ã€èªè­˜ã—ã¦ã„ã‚‹çŠ¶æ³ã€‚
    * **â‘¡ èª²é¡Œ (Problem):** ãã®è¡Œå‹•ã«ã‚ˆã£ã¦ç™ºç”Ÿã—ã¦ã„ã‚‹å…·ä½“çš„ãªãƒ‡ãƒ¡ãƒªãƒƒãƒˆã€‚
    * **â‘¢ åŸå›  (Root Cause):** ãªãœãã®èª²é¡ŒãŒç™ºç”Ÿã—ã¦ã„ã‚‹ã®ã‹ï¼ˆçŸ¥è­˜ä¸è¶³ã€æ“ä½œãƒŸã‚¹ã€åˆ¤æ–­ãƒŸã‚¹ç­‰ï¼‰ã€‚
    * **â‘£ æ”¹å–„æ¡ˆ (Solution):** å…·ä½“çš„ã«ã©ã†è¡Œå‹•ã‚’å¤‰ãˆã‚‹ã¹ãã‹ï¼ˆæŠ€ã®å¤‰æ›´ã€ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã€æ„è­˜é…åˆ†ï¼‰ã€‚
    * **â‘¤ ã‚„ã‚‹ã“ã¨ (Next Action):** æ¬¡å›ã®ãƒ—ãƒ¬ã‚¤ã§å³åº§ã«å®Ÿè¡Œã™ã¹ãã€å…·ä½“çš„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆ1è¡Œï¼‰ã€‚

    **ã€Section 2: æ™‚ç³»åˆ—ãƒ­ã‚°ã€‘**
    ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æµã‚Œã‚’æ™‚ç³»åˆ—ï¼ˆTime-Seriesï¼‰ã§è©³ç´°ã«ç®‡æ¡æ›¸ãã«ã™ã‚‹ã“ã¨ã€‚

    **ã€Section 3: ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSONã€‘**
    ä»¥ä¸‹ã®JSONã®ã¿ã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨ã€‚
    {{
      "student_name": "ç”Ÿå¾’ã®åå‰ï¼ˆä¸æ˜ãªã‚‰Unknownï¼‰",
      "date": "YYYY-MM-DD",
      "next_action": "æœ€ã‚‚å„ªå…ˆåº¦ã®é«˜ã„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³1ã¤"
    }}

    ---
    **[DETAILED_REPORT_START]**
    (ã“ã“ã«Section 1ã‚’å‡ºåŠ›)
    **[DETAILED_REPORT_END]**

    **[RAW_LOG_START]**
    (ã“ã“ã«Section 2ã‚’å‡ºåŠ›)
    **[RAW_LOG_END]**

    **[JSON_START]**
    (ã“ã“ã«Section 3ã‚’å‡ºåŠ›)
    **[JSON_END]**
    ---

    ã€å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã€‘
    {transcript_text[:950000]}
    """
    
    response = model.generate_content(prompt)
    text = response.text.strip()
    
    # æ­£è¦è¡¨ç¾ã§æŠ½å‡º
    def extract(s, e, src):
        m = re.search(f'{re.escape(s)}(.*?){re.escape(e)}', src, re.DOTALL)
        return m.group(1).strip() if m else ""

    report = extract("[DETAILED_REPORT_START]", "[DETAILED_REPORT_END]", text)
    time_log = extract("[RAW_LOG_START]", "[RAW_LOG_END]", text)
    json_str = extract("[JSON_START]", "[JSON_END]", text)
    
    try:
        data = json.loads(json_str)
    except:
        data = {"student_name": "Unknown", "date": datetime.now().strftime('%Y-%m-%d'), "next_action": "Check Logs"}
        
    return data, report, time_log


# ==========================================
# Phase 3: è³‡ç”£åŒ–ã¨æ•´ç† (Storage & Cleanup)
# ==========================================

def notion_query_student(student_name):
    """ç”Ÿå¾’åã‹ã‚‰TargetIDã‚’å–å¾—"""
    db_id = sanitize_id(FINAL_CONTROL_DB_ID)
    if not db_id: return None, student_name
    
    url = f"https://api.notion.com/v1/databases/{db_id}/query"
    res = requests.post(url, headers=HEADERS, json={"filter": {"property": "Name", "title": {"contains": student_name}}})
    d = res.json()
    
    if d.get("results"):
        row = d["results"][0]
        name = row["properties"]["Name"]["title"][0]["plain_text"]
        tid = row["properties"]["TargetID"]["rich_text"]
        return (sanitize_id(tid[0]["plain_text"]), name) if tid else (None, name)
    return None, student_name


def notion_create_page_heavy(db_id, props, all_children):
    """[åˆ†å‰²æ›¸ãè¾¼ã¿] 100ãƒ–ãƒ­ãƒƒã‚¯åˆ¶é™ã‚’å›é¿ã—ã¦å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
    # 1. ãƒšãƒ¼ã‚¸ä½œæˆ
    res = requests.post("https://api.notion.com/v1/pages", headers=HEADERS, json={
        "parent": {"database_id": db_id},
        "properties": props,
        "children": all_children[:100]
    })
    page_id = res.json().get('id')
    
    # 2. è¿½è¨˜ (Append)
    if page_id and len(all_children) > 100:
        url_append = f"https://api.notion.com/v1/blocks/{page_id}/children"
        for i in range(100, len(all_children), 100):
            chunk = all_children[i : i + 100]
            requests.patch(url_append, headers=HEADERS, json={"children": chunk})


def cleanup_drive_file(file_id, rename_to):
    """[æ•´åˆæ€§ç¢ºä¿] ãƒªãƒãƒ¼ãƒ ã¨ç§»å‹•ã‚’åŒæ™‚ã«è¡Œã†"""
    # ãƒ•ã‚©ãƒ«ãƒ€ç¢ºä¿
    q = f"name='processed_coaching_logs' and '{INBOX_FOLDER_ID}' in parents and trashed=false"
    folders = drive_service.files().list(q=q).execute().get('files', [])
    target_id = folders[0]['id'] if folders else drive_service.files().create(
        body={'name': 'processed_coaching_logs', 'mimeType': 'application/vnd.google-apps.folder', 'parents': [INBOX_FOLDER_ID]},
        fields='id'
    ).execute().get('id')
    
    # è¦ªãƒ•ã‚©ãƒ«ãƒ€å–å¾—
    file_meta = drive_service.files().get(fileId=file_id, fields='parents').execute()
    prev_parents = ",".join(file_meta.get('parents', []))
    
    # æ›´æ–°å®Ÿè¡Œ
    drive_service.files().update(
        fileId=file_id,
        addParents=target_id,
        removeParents=prev_parents,
        body={'name': rename_to}
    ).execute()
    print(f"âœ… Drive Updated: {rename_to}", flush=True)


# ==========================================
# Main Loop (åŸå­æ€§ã®ä¿è¨¼)
# ==========================================

def main():
    print("--- SZ AUTO LOGGER ULTIMATE (v77.0) ---", flush=True)
    
    # ã‚¹ã‚­ãƒ£ãƒ³
    files = drive_service.files().list(
        q=f"'{INBOX_FOLDER_ID}' in parents and trashed=false and mimeType!='application/vnd.google-apps.folder'",
        fields="files(id, name)"
    ).execute().get('files', [])
    
    if not files:
        print("â„¹ï¸ No files found.")
        return

    for file in files:
        try:
            print(f"\nğŸ“‚ Processing: {file['name']}")
            fpath = os.path.join(TEMP_DIR, file['name'])
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            with open(fpath, "wb") as f:
                downloader = MediaIoBaseDownload(f, drive_service.files().get_media(fileId=file['id']))
                done = False
                while not done: _, done = downloader.next_chunk()
            
            # --- [é‡è¦] ç´ æã®ç´”åŒ– (Logic Filtering) ---
            audio_sources = []
            if file['name'].endswith('.zip'):
                with zipfile.ZipFile(fpath, 'r') as z:
                    z.extractall(TEMP_DIR)
                    for root, _, fs in os.walk(TEMP_DIR):
                        for af in fs:
                            # ã‚´ãƒŸãƒ•ã‚¡ã‚¤ãƒ«(final_mix, chunk)ã‚„ZIPè‡ªä½“ã‚’é™¤å¤–
                            lower = af.lower()
                            if lower.endswith(('.flac', '.mp3', '.m4a', '.wav')):
                                if 'final_mix' not in lower and 'chunk' not in lower:
                                    audio_sources.append(os.path.join(root, af))
            else:
                audio_sources.append(fpath)

            if not audio_sources:
                print("âš ï¸ No valid audio found (Skipping).")
                continue

            # --- å‡¦ç†å®Ÿè¡Œ ---
            mixed_mp3 = mix_audio_ffmpeg(audio_sources)
            chunk_paths = split_audio_ffmpeg(mixed_mp3)
            full_text = transcribe_with_groq(chunk_paths)
            meta, report, time_log = analyze_text_with_gemini(full_text)
            
            # --- ä¿å­˜ ---
            dest_id, off_name = notion_query_student(meta.get('student_name', 'Unknown'))
            if not dest_id: dest_id = FINAL_FALLBACK_DB_ID
            
            props = {
                "åå‰": {"title": [{"text": {"content": f"{meta['date']} {off_name} ãƒ­ã‚°"}}]},
                "æ—¥ä»˜": {"date": {"start": meta['date']}}
            }
            
            content = f"### ğŸ“Š SZãƒ¡ã‚½ãƒƒãƒ‰è©³ç´°åˆ†æ\n\n{report}\n\n---\n### ğŸ“ æ™‚ç³»åˆ—ãƒ­ã‚°\n\n{time_log}"
            blocks = []
            for line in content.split('\n'):
                if line.strip():
                    blocks.append({
                        "object": "block", "type": "paragraph", 
                        "paragraph": {"rich_text": [{"text": {"content": line[:1900]}}]}
                    })
            
            notion_create_page_heavy(sanitize_id(dest_id), props, blocks)
            
            # --- å®Œäº†å‡¦ç† ---
            ext = os.path.splitext(file['name'])[1] or ".zip"
            new_name = f"{meta.get('date', 'Unknown')}_{off_name}{ext}"
            cleanup_drive_file(file['id'], rename_to=new_name)

        except Exception as e:
            print(f"âŒ Error: {e}")
            import traceback
            traceback.print_exc()
            
        finally:
            # [åŸå­æ€§] æ¯å›ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ¶ˆã—ã¦æ¬¡ã®ãƒ«ãƒ¼ãƒ—ã¸
            if os.path.exists(TEMP_DIR):
                shutil.rmtree(TEMP_DIR)
                os.makedirs(TEMP_DIR)

if __name__ == "__main__":
    main()
