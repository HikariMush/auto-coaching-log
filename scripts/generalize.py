import os
import sys
import time
import json
import requests
import re
from google import genai
from google.genai import types

# --- Environment Variable Validation ---
def validate_environment():
    """ç’°å¢ƒå¤‰æ•°ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€ä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã‚¨ãƒ©ãƒ¼ã‚’å‡ºåŠ›ã—ã¦çµ‚äº†"""
    required_vars = ["NOTION_TOKEN", "GEMINI_API_KEY"]
    missing_vars = []
    
    for var in required_vars:
        if not os.getenv(var):
            missing_vars.append(var)
    
    if missing_vars:
        print("âŒ CRITICAL ERROR: Missing required environment variables!", flush=True)
        for var in missing_vars:
            print(f"   - {var}", flush=True)
        print("\nğŸ“ Please set these variables in your .env file or GitHub Actions secrets:", flush=True)
        print("   NOTION_TOKEN: Your Notion integration token")
        print("   GEMINI_API_KEY: Your Google Gemini API key")
        sys.exit(1)

# Run validation at script start
validate_environment()

# --- Config ---
SOURCE_LOG_DB_ID = "2e01bc8521e380ffaf28c2ab9376b00d"
TARGET_THEORY_DB_ID = "2e21bc8521e38029b8b1d5c4b49731eb"

NOTION_TOKEN = os.getenv("NOTION_TOKEN")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

HEADERS = {
    "Authorization": f"Bearer {NOTION_TOKEN}",
    "Content-Type": "application/json",
    "Notion-Version": "2022-06-28"
}

# --- Dynamic Model Resolver ---
def resolve_best_model():
    client = genai.Client(api_key=GEMINI_API_KEY)
    print("ğŸ’ Querying Google API for the absolute latest models...", flush=True)

    try:
        # 1. APIã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªå…¨ãƒ¢ãƒ‡ãƒ«ã‚’å‹•çš„ã«å–å¾—
        all_models = list(client.models.list())
        candidates = []
        
        for m in all_models:
            # ãƒ¢ãƒ‡ãƒ«IDã®æŠ½å‡º (ä¾‹: models/gemini-1.5-pro -> gemini-1.5-pro)
            name = m.name.replace("models/", "")
            
            # ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’å¯¾è±¡ã«ã™ã‚‹ (embeddingã‚„visionå˜ä½“ãƒ¢ãƒ‡ãƒ«ã‚’é™¤å¤–)
            if "gemini" in name and "vision" not in name and "embedding" not in name:
                candidates.append(name)

        # 2. æœ€å¼·ãƒ¢ãƒ‡ãƒ«ã‚’æ±ºã‚ã‚‹ãŸã‚ã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ­ã‚¸ãƒƒã‚¯
        def model_score(name):
            score = 0
            # ãƒãƒ¼ã‚¸ãƒ§ãƒ³åˆ¤å®š (æ•°å­—ãŒå¤§ãã„ã»ã©å‰ã„)
            version_match = re.search(r"(\d+\.\d+)", name)
            if version_match:
                version = float(version_match.group(1))
                score += version * 1000  # 2.5 -> 2500, 2.0 -> 2000
            
            # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¤å®š
            if "ultra" in name: score += 300
            elif "pro" in name: score += 200
            elif "flash" in name: score += 100
            
            # æœ€æ–°ãƒ»å®Ÿé¨“çš„ãƒ¢ãƒ‡ãƒ«ã®å„ªå…ˆ (Expã¯æœ€æ–°æ©Ÿèƒ½ãŒå…¥ã£ã¦ã„ã‚‹ã“ã¨ãŒå¤šã„)
            if "exp" in name: score += 50
            if "thinking" in name: score += 20 # æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ä»˜ããªã‚‰ã•ã‚‰ã«åŠ ç‚¹
            
            # å®‰å®šç‰ˆ(001, 002ç­‰)ã‚ˆã‚Šæœ€æ–°ã®æ—¥ä»˜ä»˜ãã‚’å„ªå…ˆã—ãŸã„å ´åˆãªã©ã¯ã“ã“ã§èª¿æ•´
            # ã“ã“ã§ã¯ã‚·ãƒ³ãƒ—ãƒ«ã«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨Pro/FlashåŸºæº–ã¨ã™ã‚‹
            return score

        # ã‚¹ã‚³ã‚¢ãŒé«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
        candidates.sort(key=model_score, reverse=True)
        
        print(f"ğŸ“‹ Detected Candidates (Top 5): {candidates[:5]}", flush=True)

        # 3. ä¸Šã‹ã‚‰é †ã«ç–é€šãƒ†ã‚¹ãƒˆ (Rate Limitãªã©ã§ä½¿ãˆãªã„ã‚„ã¤ã¯ã‚¹ã‚­ãƒƒãƒ—)
        for model in candidates:
            try:
                client.models.generate_content(
                    model=model, 
                    contents="Test",
                    config=types.GenerateContentConfig(response_mime_type="text/plain")
                )
                print(f"âœ… ACTIVATED STRONGEST MODEL: {model}", flush=True)
                return model
            except Exception as e:
                # æ¨©é™ãŒãªã„ã€å»ƒæ­¢ã•ã‚ŒãŸã€RateLimitãªã©ã®å ´åˆã¯æ¬¡ã¸
                continue

    except Exception as e:
        print(f"âŒ Failed to list models dynamically: {e}")
    
    # ä¸‡ãŒä¸€å…¨æ»…ã—ãŸå ´åˆã®æœ€å¾Œã®ç ¦ (ã“ã“ã«ã¯æ¥ãªã„ã¯ãšã ãŒå¿µã®ç‚º)
    print("âš ï¸ Dynamic resolution failed. Fallback to hardcoded safe model.")
    return "gemini-1.5-pro"

ACTIVE_MODEL_ID = None

# --- Notion API Helpers ---
def get_page_content(page_id):
    all_text = ""
    has_more = True
    start_cursor = None
    while has_more:
        url = f"https://api.notion.com/v1/blocks/{page_id}/children?page_size=100"
        if start_cursor: url += f"&start_cursor={start_cursor}"
        try:
            res = requests.get(url, headers=HEADERS)
            if res.status_code != 200: break
            data = res.json()
            for block in data.get("results", []):
                btype = block.get("type")
                if not btype or "rich_text" not in block.get(btype, {}): continue
                text_list = block[btype].get("rich_text", [])
                line = "".join([t.get("text", {}).get("content", "") for t in text_list])
                if line: all_text += line + "\n"
            has_more = data.get("has_more", False)
            start_cursor = data.get("next_cursor")
        except: break
    return all_text

def mark_log_as_processed(page_id):
    url = f"https://api.notion.com/v1/pages/{page_id}"
    payload = {"properties": {"AIå‡¦ç†æ¸ˆã¿": {"checkbox": True}}}
    try:
        requests.patch(url, headers=HEADERS, json=payload)
        print(f"   â˜‘ï¸ Marked as processed: {page_id}")
    except Exception as e:
        print(f"   âš ï¸ Failed to mark processed: {e}")

def text_to_blocks(text):
    blocks = []
    lines = text.split('\n')
    for line in lines:
        if not line.strip(): continue
        ct = line.replace('**', '')[:1900]
        if line.startswith('### '):
            blocks.append({"object":"block", "type":"heading_3", "heading_3":{"rich_text":[{"text":{"content":ct[4:]}}]}})
        elif line.startswith('## '):
            blocks.append({"object":"block", "type":"heading_2", "heading_2":{"rich_text":[{"text":{"content":ct[3:]}}]}})
        elif line.startswith('- '):
            blocks.append({"object":"block", "type":"bulleted_list_item", "bulleted_list_item":{"rich_text":[{"text":{"content":ct[2:]}}]}})
        else:
            blocks.append({"object":"block", "type":"paragraph", "paragraph":{"rich_text":[{"text":{"content":ct}}]}})
    return blocks

# --- Gemini Logic ---
def generate_theories(log_text):
    client = genai.Client(api_key=GEMINI_API_KEY)
    
    prompt = f"""
    ã‚ãªãŸã¯ã‚¹ãƒãƒ–ãƒ©ã®ã‚³ãƒ¼ãƒãƒ³ã‚°ãƒ­ã‚°åˆ†æAIã§ã™ã€‚
    å…¥åŠ›ã•ã‚ŒãŸãƒ­ã‚°ã‹ã‚‰ã€Œç†è«–(Theory)ã€ã‚’æŠ½å‡ºã—ã€æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚
    
    ã€åˆ†æãƒ«ãƒ¼ãƒ«ã€‘
    1. **Scopeåˆ¤å®š**: 
       - ãã®ç†è«–ã¯ã€Œå…¨ã‚­ãƒ£ãƒ©å…±é€šã®ä¸€èˆ¬è«–ã€ã‹ã€ã€Œç‰¹å®šã®ã‚­ãƒ£ãƒ©å¯¾ã€ã‹ï¼Ÿ
       - å€¤ã¯å¿…ãš **"å…¨èˆ¬"** ã¾ãŸã¯ **"ã‚­ãƒ£ãƒ©å¯¾"** ã®ã„ãšã‚Œã‹ã«ã™ã‚‹ã“ã¨ã€‚
    
    2. **Category (è¤‡æ•°å¯)**:
       - è©²å½“ã™ã‚‹ã‚«ãƒ†ã‚´ãƒªã‚’ãƒªã‚¹ãƒˆå½¢å¼ã§å…¨ã¦æŠ½å‡ºã›ã‚ˆã€‚
       - é¸æŠè‚¢: å¾©å¸°é˜»æ­¢, å¾©å¸°, å´–ä¸ŠãŒã‚Š, å´–ç‹©ã‚Š, ç«‹ã¡å›ã‚Š, æ€è€ƒ, ãƒ¡ãƒ³ã‚¿ãƒ«, æ’ƒå¢œ, æ’ƒå¢œæ‹’å¦, ã‚³ãƒ³ãƒœ, ãã®ä»–

    3. **ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼æŠ½å‡º**:
       - Player Char: ä½¿ç”¨ã‚­ãƒ£ãƒ©ï¼ˆä¸æ˜/å…¨èˆ¬ãªã‚‰ "å…¨èˆ¬"ï¼‰ã€‚
       - Target Char: å¯¾ç­–å¯¾è±¡ï¼ˆScopeãŒ"å…¨èˆ¬"ãªã‚‰ç©ºæ¬„ï¼‰ã€‚
       - åç§°ã¯ã€Œã‚¹ãƒãƒ–ãƒ©SPã®æ—¥æœ¬èªæ­£å¼åç§°ã€ã€‚

    Format (JSON Array):
    [
      {{
        "theory_name": "ã‚¿ã‚¤ãƒˆãƒ« (30æ–‡å­—ä»¥å†…)",
        "scope": "å…¨èˆ¬" | "ã‚­ãƒ£ãƒ©å¯¾", 
        "categories": ["å¾©å¸°é˜»æ­¢", "æ’ƒå¢œ"], 
        "player_char": "ã‚¯ãƒ©ã‚¦ãƒ‰",
        "target_char": "ãƒã‚¹",
        "importance": "S",
        "tags": ["ã‚¸ãƒ£ãƒ³ãƒ—èª­ã¿", "ç©ºå‰"],
        "abstract": "3è¡Œè¦ç´„",
        "detail": "### è§£èª¬\\nè©³ç´°ãªç†è«–ã€‚", 
        "source_context": "å…ƒãƒ­ã‚°ã‹ã‚‰ã®å¼•ç”¨"
      }}
    ]

    Log Content:
    {log_text[:25000]}
    """
    
    max_retries = 3
    for attempt in range(max_retries):
        try:
            res = client.models.generate_content(
                model=ACTIVE_MODEL_ID, 
                contents=prompt, 
                config=types.GenerateContentConfig(response_mime_type="application/json")
            )
            return json.loads(res.text)
        except Exception as e:
            if "429" in str(e) or "Resource exhausted" in str(e):
                print(f"âš ï¸ Rate Limit hit ({ACTIVE_MODEL_ID}). Waiting 30s...", flush=True)
                time.sleep(30)
                continue
            else:
                print(f"Gemini Error: {e}")
                return []
    return []

# --- Save Logic ---
def save_theory(theory, log_id):
    target_char = theory.get("target_char", "")
    if target_char == "å…¨èˆ¬": target_char = None 
    
    raw_cats = theory.get("categories", [])
    if isinstance(raw_cats, str): raw_cats = [raw_cats]
    
    props = {
        "Theory Name": {"title": [{"text": {"content": theory.get("theory_name", "Untitled")}}]},
        "Category": {"multi_select": [{"name": c} for c in raw_cats]},
        "Scope": {"select": {"name": theory.get("scope", "å…¨èˆ¬")}},
        "Player Char": {"select": {"name": theory.get("player_char", "å…¨èˆ¬")}}, 
        "Target Char": {"select": {"name": target_char}} if target_char else {"select": None},
        "Importance": {"select": {"name": theory.get("importance", "B (çŠ¶æ³é™å®š)")}},
        "Abstract": {"rich_text": [{"text": {"content": theory.get("abstract", "")}}]},
        "Source Log": {"relation": [{"id": log_id}]},
        "Verification": {"status": {"name": "Draft"}},
        "Tags": {"multi_select": [{"name": t} for t in theory.get("tags", [])]}
    }
    
    children = []
    if "source_context" in theory:
        children.append({
            "object":"block", "type":"callout", 
            "callout":{
                "rich_text":[{"text":{"content": f"Source: {theory['source_context'][:1900]}"}}],
                "icon": {"emoji": "ğŸ’¡"}
            }
        })
    children.extend(text_to_blocks(theory.get("detail", "")))

    try:
        res = requests.post(
            "https://api.notion.com/v1/pages", 
            headers=HEADERS, 
            json={"parent": {"database_id": TARGET_THEORY_DB_ID}, "properties": props, "children": children}
        )
        if res.status_code == 200:
            print(f"âœ… Saved: [{theory.get('scope')}] {theory.get('theory_name')}")
        else:
            print(f"âŒ Save Failed ({res.status_code}): {res.text}")
    except Exception as e:
        print(f"âŒ Network Error: {e}")

# --- Main ---
def main():
    global ACTIVE_MODEL_ID
    print("--- Generalization Started (Dynamic Latest-Model Search Mode) ---", flush=True)
    
    try:
        # å¾…æ©Ÿãƒ«ãƒ¼ãƒ—: æœ‰åŠ¹ãªãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚‹ã¾ã§ç²˜ã‚‹
        max_model_wait = 10  # æœ€å¤§10å›è©¦è¡Œï¼ˆæœ€å¤§600ç§’ï¼‰
        attempt = 0
        while attempt < max_model_wait:
            ACTIVE_MODEL_ID = resolve_best_model()
            if ACTIVE_MODEL_ID:
                print(f"âœ… Model resolved: {ACTIVE_MODEL_ID}", flush=True)
                break
            attempt += 1
            print(f"â³ Waiting 60s for models to become available... (Attempt {attempt}/{max_model_wait})", flush=True)
            time.sleep(60)
        
        if not ACTIVE_MODEL_ID:
            print("âŒ CRITICAL: Could not resolve any model after max attempts", flush=True)
            sys.exit(1)

        has_more = True
        error_count = 0
        max_consecutive_errors = 5
        
        while has_more:
            query = {
                "filter": {
                    "property": "AIå‡¦ç†æ¸ˆã¿",
                    "checkbox": {"equals": False}
                },
                "page_size": 10,
                "sorts": [{"property": "æ—¥ä»˜", "direction": "descending"}]
            }
            
            try:
                res = requests.post(
                    f"https://api.notion.com/v1/databases/{SOURCE_LOG_DB_ID}/query",
                    headers=HEADERS,
                    json=query,
                    timeout=30
                )
                if res.status_code != 200:
                    print(f"âš ï¸ Notion API error ({res.status_code}): {res.text[:200]}", flush=True)
                    error_count += 1
                    if error_count >= max_consecutive_errors:
                        print(f"âŒ Max consecutive errors reached. Exiting.", flush=True)
                        break
                    time.sleep(10)
                    continue
                
                logs = res.json().get("results", [])
                error_count = 0  # Reset error count on success
                
            except requests.exceptions.Timeout:
                print(f"âš ï¸ Request timeout. Retrying...", flush=True)
                error_count += 1
                time.sleep(10)
                continue
            except Exception as e:
                print(f"âŒ Failed to fetch logs: {e}", flush=True)
                error_count += 1
                if error_count >= max_consecutive_errors:
                    print(f"âŒ Max consecutive errors reached. Exiting.", flush=True)
                    break
                time.sleep(10)
                continue
            
            if not logs:
                print("â„¹ï¸ No more unprocessed logs found.", flush=True)
                has_more = False
                break

            print(f"ğŸ” Processing batch of {len(logs)} logs with {ACTIVE_MODEL_ID}...", flush=True)

            for log in logs:
                try:
                    log_id = log.get('id')
                    print(f"\nğŸ“„ Processing Log: {log_id}", flush=True)
                    content = get_page_content(log_id)
                    if len(content) < 30:
                        print(f"   âŠ˜ Content too short, skipping", flush=True)
                        mark_log_as_processed(log_id)
                        continue
                    
                    theories = generate_theories(content)
                    
                    if not theories:
                        print(f"   âŠ˜ No theories extracted, marking as processed", flush=True)
                        mark_log_as_processed(log_id)
                        continue

                    for t in theories:
                        save_theory(t, log_id)
                        time.sleep(1)
                    
                    mark_log_as_processed(log_id)
                    time.sleep(2)
                    
                except Exception as e:
                    print(f"   âŒ Error processing log {log_id}: {e}", flush=True)
                    try:
                        mark_log_as_processed(log_id)
                    except:
                        pass
                    continue
        
        print("\nâœ… Generalization completed successfully", flush=True)
        
    except KeyboardInterrupt:
        print("\nâš ï¸ Interrupted by user", flush=True)
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ Fatal error in main loop: {e}", flush=True)
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
