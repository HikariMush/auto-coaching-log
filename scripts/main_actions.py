import os
import sys
import time
import json
import shutil
import subprocess
import glob
from datetime import datetime

# å¿…è¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒª: pip install google-generativeai groq requests google-api-python-client google-auth
import requests
import google.generativeai as genai
from groq import Groq
from google.oauth2 import service_account
from googleapiclient.discovery import build
from googleapiclient.http import MediaIoBaseDownload
import zipfile

# --- è¨­å®š ---
FINAL_CONTROL_DB_ID = "2b71bc8521e380868094ec506b41f664" 
FINAL_FALLBACK_DB_ID = "2b71bc8521e38018a5c3c4b0c6b6627c"
TEMP_DIR = "temp_workspace"
CHUNK_LENGTH = 900  # 15åˆ† (ç§’)

# --- åˆæœŸåŒ–å‡¦ç† ---
def setup_env():
    if os.path.exists(TEMP_DIR): shutil.rmtree(TEMP_DIR)
    os.makedirs(TEMP_DIR)
    
    # Service Accountç”Ÿæˆ
    if os.getenv("GCP_SA_KEY"):
        with open("service_account.json", "w") as f:
            f.write(os.getenv("GCP_SA_KEY"))

setup_env()

# Client Init
try:
    groq_client = Groq(api_key=os.getenv("GROQ_API_KEY"))
    genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
    
    NOTION_TOKEN = os.getenv("NOTION_TOKEN")
    HEADERS = {
        "Authorization": f"Bearer {NOTION_TOKEN}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28"
    }
    
    SCOPES = ['https://www.googleapis.com/auth/drive']
    creds = service_account.Credentials.from_service_account_file("service_account.json", scopes=SCOPES)
    drive_service = build('drive', 'v3', credentials=creds)
    
except Exception as e:
    print(f"âŒ Init Error: {e}")
    sys.exit(1)

# --- FFmpeg Helpers (The Core Logic) ---

def run_ffmpeg(cmd):
    """FFmpegã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã€ã‚¨ãƒ©ãƒ¼ãªã‚‰ä¾‹å¤–ã‚’æŠ•ã’ã‚‹"""
    try:
        subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    except subprocess.CalledProcessError as e:
        print(f"âš ï¸ FFmpeg Error: {e}")
        raise

def mix_audio_ffmpeg(file_paths):
    """è¤‡æ•°ã®éŸ³å£°ã‚’FFmpegã§1ã¤ã®MP3(64k)ã«çµ±åˆãƒ»åœ§ç¸®ã™ã‚‹"""
    if not file_paths: return None
    print(f"ğŸ›ï¸ Mixing {len(file_paths)} tracks...", flush=True)
    
    output_path = os.path.join(TEMP_DIR, "mixed_full.mp3")
    
    # 1ãƒ•ã‚¡ã‚¤ãƒ«ãªã‚‰å˜ã«å¤‰æ›
    if len(file_paths) == 1:
        cmd = ['ffmpeg', '-y', '-i', file_paths[0], '-b:a', '64k', output_path]
    else:
        # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ãªã‚‰amixãƒ•ã‚£ãƒ«ã‚¿ã§çµ±åˆ
        inputs = []
        for f in file_paths:
            inputs.extend(['-i', f])
        
        # amix=inputs=N:duration=longest
        filter_cmd = f"amix=inputs={len(file_paths)}:duration=longest"
        cmd = ['ffmpeg', '-y'] + inputs + ['-filter_complex', filter_cmd, '-b:a', '64k', output_path]
    
    run_ffmpeg(cmd)
    return output_path

def split_audio_ffmpeg(input_path):
    """é•·å°ºMP3ã‚’æŒ‡å®šç§’æ•°ï¼ˆ900ç§’ï¼‰ã”ã¨ã«åˆ†å‰²ã™ã‚‹"""
    print("ğŸ”ª Splitting audio into chunks...", flush=True)
    output_pattern = os.path.join(TEMP_DIR, "chunk_%03d.mp3")
    
    # -f segment ã§åˆ†å‰²ã€‚-c copy ã§å†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãªã—ï¼ˆçˆ†é€Ÿï¼‰
    cmd = [
        'ffmpeg', '-y', '-i', input_path, 
        '-f', 'segment', '-segment_time', str(CHUNK_LENGTH), 
        '-c', 'copy', output_pattern
    ]
    run_ffmpeg(cmd)
    
    chunks = sorted(glob.glob(os.path.join(TEMP_DIR, "chunk_*.mp3")))
    print(f"â¡ï¸ Created {len(chunks)} chunks.")
    return chunks

# --- AI Helpers ---

def transcribe_with_groq(chunk_paths):
    """Groq API (Whisper Large v3) ã§åˆ†å‰²ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é †æ¬¡æ–‡å­—èµ·ã“ã—"""
    full_transcript = ""
    print("ğŸš€ Starting Groq Transcription...", flush=True)
    
    for i, chunk in enumerate(chunk_paths):
        print(f"   Processing chunk {i+1}/{len(chunk_paths)}...", flush=True)
        with open(chunk, "rb") as file:
            transcription = groq_client.audio.transcriptions.create(
                file=(chunk, os.path.basename(chunk)),
                model="whisper-large-v3",
                language="ja",
                response_format="text"
            )
            full_transcript += transcription + "\n"
            
    return full_transcript

def summarize_with_gemini(transcript_text):
    """Gemini 1.5 Flashã§ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®è¦ç´„ãƒ»æ§‹é€ åŒ–"""
    print("ğŸ§  Summarizing with Gemini 1.5 Flash...", flush=True)
    model = genai.GenerativeModel('gemini-1.5-flash')
    
    prompt = f"""
    ã‚ãªãŸã¯ãƒ—ãƒ­ã®ã‚¹ãƒãƒ–ãƒ©ã‚³ãƒ¼ãƒã®ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆAIã§ã™ã€‚
    ä»¥ä¸‹ã¯ã‚³ãƒ¼ãƒãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®å…¨æ–‡å­—èµ·ã“ã—ãƒ­ã‚°ã§ã™ã€‚ã“ã‚Œã‚’åˆ†æã—ã€æŒ‡å®šã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

    ã€åˆ†æè¦ä»¶ã€‘
    1. ç”Ÿå¾’ã®åå‰ã‚’ç‰¹å®šã›ã‚ˆï¼ˆä¸æ˜ãªå ´åˆã¯ "Unknown"ï¼‰ã€‚
    2. ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æ—¥ä»˜ã‚’ç‰¹å®šã›ã‚ˆï¼ˆä¸æ˜ãªå ´åˆã¯ä»Šæ—¥ã®æ—¥ä»˜ {datetime.now().strftime('%Y-%m-%d')}ï¼‰ã€‚
    3. å†…å®¹ã‚’ã€Œç¾çŠ¶ãƒ»èª²é¡Œãƒ»è§£æ±ºç­–ãƒ»ãƒã‚¯ã‚¹ãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§åˆ†æã—ã€è¦ç´„ã‚’ä½œæˆã›ã‚ˆã€‚

    ã€å‡ºåŠ›JSONå½¢å¼ã€‘
    {{
      "student_name": "Name",
      "date": "YYYY-MM-DD",
      "summary": "è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ200æ–‡å­—ä»¥å†…ï¼‰",
      "next_action": "å…·ä½“çš„ãªæ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³"
    }}

    --- TRANSCRIPT START ---
    {transcript_text[:1000000]} 
    --- TRANSCRIPT END ---
    """
    # Flashã¯100ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§ã„ã‘ã‚‹ã®ã§ã€åŸºæœ¬çš„ã«ã¯truncateä¸è¦ã ãŒå¿µã®ãŸã‚
    
    response = model.generate_content(prompt, generation_config={"response_mime_type": "application/json"})
    return json.loads(response.text)

# --- Notion Helpers ---
def notion_create_page(db_id, props, children):
    url = "https://api.notion.com/v1/pages"
    # å­ä¾›è¦ç´ ãŒå¤šã„å ´åˆã¯åˆ†å‰²ã—ã¦è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŒã€ä»Šå›ã¯ç°¡æ˜“å®Ÿè£…
    payload = {"parent": {"database_id": db_id}, "properties": props, "children": children[:100]} 
    res = requests.post(url, headers=HEADERS, json=payload)
    if res.status_code != 200:
        print(f"âš ï¸ Notion Error: {res.text}")
    return res.json()

# --- Main Flow ---

def main():
    print("--- SZ AUTO LOGGER (FFmpeg + Groq Edition) ---", flush=True)
    
    folder_id = os.getenv("DRIVE_FOLDER_ID")
    if not folder_id: return

    # 1. Check Drive
    results = drive_service.files().list(
        q=f"'{folder_id}' in parents and mimeType != 'application/vnd.google-apps.folder' and trashed = false",
        fields="files(id, name)", orderBy="createdTime desc"
    ).execute()
    files = results.get('files', [])
    
    if not files:
        print("â„¹ï¸ No new files.", flush=True)
        return

    for file in files:
        print(f"\nğŸ“‚ Processing: {file['name']}", flush=True)
        
        # 2. Download & Extract
        file_path = os.path.join(TEMP_DIR, file['name'])
        with open(file_path, "wb") as fh:
            downloader = MediaIoBaseDownload(fh, drive_service.files().get_media(fileId=file['id']))
            done = False
            while not done: _, done = downloader.next_chunk()
            
        audio_paths = []
        if file['name'].endswith('.zip'):
            with zipfile.ZipFile(file_path, 'r') as z:
                z.extractall(TEMP_DIR)
                for root, _, fs in os.walk(TEMP_DIR):
                    for f in fs:
                        if f.lower().endswith(('.flac', '.mp3', '.m4a', '.wav')):
                            audio_paths.append(os.path.join(root, f))
        else:
            audio_paths.append(file_path)

        # 3. Audio Pipeline (Mix -> Split)
        mixed_file = mix_audio_ffmpeg(audio_paths)
        chunks = split_audio_ffmpeg(mixed_file)
        
        # 4. Transcribe (Groq)
        full_text = transcribe_with_groq(chunks)
        
        # 5. Summarize (Gemini)
        data = summarize_with_gemini(full_text)
        
        # 6. Notion Logic (Simplified for brevity)
        print(f"ğŸ“ Writing to Notion: {data['student_name']}")
        
        # é€ä¿¡å…ˆIDæ±ºå®šãƒ­ã‚¸ãƒƒã‚¯ï¼ˆæ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¨åŒã˜æƒ³å®šï¼‰
        # ã“ã“ã§ã¯Inbox(Fallback)ã«æ›¸ãè¾¼ã‚€ä¾‹ã®ã¿è¨˜è¿°
        target_db = FINAL_FALLBACK_DB_ID 
        
        props = {
            "åå‰": {"title": [{"text": {"content": f"{data['date']} {data['student_name']} ãƒ­ã‚°"}}]},
            "æ—¥ä»˜": {"date": {"start": data['date']}}
        }
        
        # æœ¬æ–‡ï¼ˆè¦ç´„ + å…¨æ–‡ãƒ­ã‚°ï¼‰
        children = [
            {"object": "block", "type": "callout", "callout": {
                "rich_text": [{"text": {"content": data['summary']}}],
                "icon": {"emoji": "ğŸ¤–"}
            }},
            {"object": "block", "type": "heading_3", "heading_3": {"rich_text": [{"text": {"content": "Full Transcript"}}]}}
        ]
        
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’2000æ–‡å­—ã”ã¨ã«åˆ†å‰²ã—ã¦ãƒ–ãƒ­ãƒƒã‚¯åŒ–
        for i in range(0, len(full_text), 2000):
            chunk_text = full_text[i:i+2000]
            children.append({
                "object": "block", "type": "paragraph",
                "paragraph": {"rich_text": [{"text": {"content": chunk_text}}]}
            })
            
        notion_create_page(target_db, props, children)
        
        # 7. Move processed file
        # (Drive move logic here - same as before)
        print("âœ… Done.")

if __name__ == "__main__":
    main()
