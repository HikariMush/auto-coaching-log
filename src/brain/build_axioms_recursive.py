import os
import json
import dspy
import sys
import io
import glob
import re
import time
from google import genai

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', line_buffering=True)

def find_ultimate_logic_engine():
    api_key = os.getenv("GEMINI_API_KEY", "").strip()
    client = genai.Client(api_key=api_key)
    print("ğŸ“¡ ç‰©ç†çŸ¥èƒ½ã®æœ€çµ‚è©•ä¾¡ãƒ•ã‚§ãƒ¼ã‚ºé–‹å§‹...", flush=True)
    
    try:
        all_models = list(client.models.list())
        candidates = []
        
        # æ’é™¤å¯¾è±¡ï¼ˆç”»åƒç”Ÿæˆã€åŸ‹ã‚è¾¼ã¿ã€è»½é‡ãƒ¢ãƒ‡ãƒ«ã€å®Ÿé¨“çš„æ©Ÿèƒ½ï¼‰ [cite: 2026-01-16]
        ban_list = ["nano", "imagen", "veo", "embedding", "gecko", "aqa", "vision", "audio", "tts", "robotics", "computer-use", "deep-research"]

        for m in all_models:
            full_id = m.name # models/gemini-3-pro-preview ç­‰
            raw_id = full_id.lower()
            
            if not "gemini" in raw_id and not "gemma" in raw_id: continue
            if any(x in raw_id for x in ban_list): continue
            
            # çŸ¥èƒ½ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼šVersionã‚’100å€ã€Tierã‚’è£œæ­£ [cite: 2026-01-16]
            score = 0
            v_match = re.search(r"(\d+\.?\d*)", raw_id)
            if v_match:
                score += float(v_match.group(1)) * 100
            
            if "ultra" in raw_id: score += 50
            elif "pro" in raw_id: score += 30
            elif "thinking" in raw_id: score += 40
            
            if "preview" in raw_id or "exp" in raw_id:
                score += 5
                
            candidates.append({"score": score, "full_id": full_id, "short_id": full_id.split("/")[-1]})
        
        candidates.sort(key=lambda x: x["score"], reverse=True)
        
        # å®Ÿå°„è©¦é¨“ï¼šãƒªã‚¹ãƒˆã®æœ€ä¸Šä½ã‹ã‚‰ç”Ÿå­˜ç¢ºèª [cite: 2026-01-16]
        for entry in candidates:
            print(f"  æ¤œè¨¼: {entry['short_id']} (Score: {entry['score']})", end=" -> ", flush=True)
            try:
                client.models.generate_content(model=entry['full_id'], contents="ping")
                print("ç–é€šæˆåŠŸ")
                return entry['full_id'], entry['short_id']
            except:
                print("å¤±æ•—")
                continue
        return "models/gemini-1.5-pro", "gemini-1.5-pro"
    except Exception as e:
        print(f"è‡´å‘½çš„æ¢ç´¢å¤±æ•—: {e}")
        return "models/gemini-1.5-pro", "gemini-1.5-pro"

# 1. æœ€å¼·çŸ¥èƒ½ã®ãƒ­ãƒƒã‚¯
RAW_FULL_ID, SHORT_ID = find_ultimate_logic_engine()
print(f"\nSZ Logic Engine (Locked): {SHORT_ID}\n")

# DSPyæ§‹æˆï¼ˆNative SDK ãƒ–ãƒªãƒƒã‚¸ï¼‰ [cite: 2026-01-16]
class SZNativeLM(dspy.LM):
    def __init__(self, model_id, api_key):
        super().__init__(model_id)
        self.client = genai.Client(api_key=api_key)
        self.model_id = model_id
    def __call__(self, prompt=None, messages=None, **kwargs):
        content = prompt if prompt else messages[-1]['content']
        # æ†²æ³•ç·¨çº‚ã®ãŸã‚ã€å¸¸ã«æ±ºå®šè«–çš„ï¼ˆtemperature=0.0ï¼‰ã«å‹•ä½œ [cite: 2026-01-16]
        res = self.client.models.generate_content(model=self.model_id, contents=content, config={'temperature': 0.0})
        return [res.text]

lm = SZNativeLM(RAW_FULL_ID, os.getenv("GEMINI_API_KEY").strip())
dspy.settings.configure(lm=lm)

class AxiomSynthesizer(dspy.Signature):
    """æ–­ç‰‡åŒ–ã•ã‚ŒãŸä»•æ§˜ã‚’ã€LaTeX ($) ã‚’ç”¨ã„ãŸä¸€ã¤ã®åŸºæœ¬ç†è«–ï¼ˆæ†²æ³•ï¼‰ã¸çµ±åˆã›ã‚ˆã€‚"""
    fragments = dspy.InputField()
    refined_axioms = dspy.OutputField(desc="è«–ç†çŸ›ç›¾ã‚’æ’é™¤ã—ãŸæ•°å¼ã¨ç‰©ç†æ³•å‰‡ã®ãƒªã‚¹ãƒˆ")

def process_file_recursive(file_path):
    title = os.path.basename(file_path).replace(".txt", "")
    with open(file_path, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    # ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼šãƒ¢ãƒ‡ãƒ«ã®æ–‡è„ˆç¶­æŒèƒ½åŠ›ã‚’æœ€å¤§åŒ– [cite: 2026-01-16]
    chunk_size = 500
    chunks = ["".join(lines[i:i + chunk_size]) for i in range(0, len(lines), chunk_size)]
    print(f"ã€ç·¨çº‚ä¸­ã€‘: {title} ({len(lines)} è¡Œ / {len(chunks)} ãƒãƒ£ãƒ³ã‚¯)")
    
    fragments = []
    client = genai.Client(api_key=os.getenv("GEMINI_API_KEY").strip())
    for idx, c_text in enumerate(chunks):
        print(f"  - æŠ½å‡º {idx+1}/{len(chunks)}...", end=" ", flush=True)
        res = client.models.generate_content(
            model=RAW_FULL_ID, 
            contents=f"ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã‚¹ãƒãƒ–ãƒ©ã®ç‰©ç†ãƒ»ä»•æ§˜ã‚’æŠ½å‡ºã›ã‚ˆã€‚æ•°å¼ã¯ LaTeX ($) ã‚’ä½¿ç”¨ã€‚\n\n{c_text}"
        )
        fragments.append(res.text)
        print("å®Œäº†")

    print(f"  - è«–ç†çµ±åˆãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹...")
    synthesizer = dspy.ChainOfThought(AxiomSynthesizer)
    final_res = synthesizer(fragments="\n\n".join(fragments))
    return {"category": title, "axioms": final_res.refined_axioms}

def main():
    files = sorted(glob.glob("src/brain/raw_data/*.txt"))
    final_constitution = []
    for f in files:
        res = process_file_recursive(f)
        final_constitution.append(res)
        # ä¸€é …ç›®ã”ã¨ã«ä¿å­˜ã—ã€ä¸æ…®ã®åœæ­¢ã«å‚™ãˆã‚‹ [cite: 2026-01-16]
        with open("pending_basic_theory.json", "w", encoding="utf-8") as out:
            json.dump(final_constitution, out, ensure_ascii=False, indent=2)
    print("\n--- DONE: SZçµ¶å¯¾æ†²æ³•ã®ç‰©ç†çš„ç·¨çº‚ãŒå®Œäº†ã—ã¾ã—ãŸ ---")

if __name__ == "__main__":
    main()
