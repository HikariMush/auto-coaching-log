#!/usr/bin/env python3
"""
SmashZettel-Bot: Coach Optimizer
Runs dspy.Teleprompter optimization on collected training data.

V2æ©Ÿèƒ½:
- è¦ç´ åˆ¥æœ€é©åŒ–: å„è¦ç´ ï¼ˆ[1]ã€œ[4]ï¼‰ã‚’å€‹åˆ¥ã«æœ€é©åŒ–
- å…¨æ–‡æœ€é©åŒ–ã¨ã®ä½µç”¨

Usage:
    python -m src.utils.optimize_coach
    
Environment variables:
    GEMINI_API_KEY: Google Gemini API key
    PINECONE_API_KEY: Pinecone API key
"""

import os
import json
import sys
from pathlib import Path
from typing import Dict, List, Any, Callable
from datetime import datetime

import dspy
import google.generativeai as genai

# LLM Model Configuration
LLM_MODEL = "gemini-2-5-flash"  # Updated to Gemini 2.5 Flash

# --- è¦ç´ åˆ¥Signatureå®šç¾© ---
class FrameDataElement(dspy.Signature):
    """[1] ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ãƒ»åŸºç¤æƒ…å ±ã®ç”Ÿæˆã«ç‰¹åŒ–ã—ãŸSignature"""
    context = dspy.InputField(desc="æ¤œç´¢ã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿")
    question = dspy.InputField(desc="ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•")
    frame_data_info = dspy.OutputField(desc="ç™ºç”ŸFã€å…¨ä½“Fã€ãƒ€ãƒ¡ãƒ¼ã‚¸%ãªã©ã®æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã€‚ç°¡æ½”ã‹ã¤æ­£ç¢ºã«ã€‚")

class TechnicalElement(dspy.Signature):
    """[2] æŠ€è¡“çš„è§£èª¬ã®ç”Ÿæˆã«ç‰¹åŒ–ã—ãŸSignature"""
    context = dspy.InputField(desc="æŠ€è¡“çš„ãªç†è«–ãƒ‡ãƒ¼ã‚¿")
    question = dspy.InputField(desc="ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•")
    technical_explanation = dspy.OutputField(desc="ç¡¬ç›´å·®ã®è¨ˆç®—ã€ç¢ºå®šåæ’ƒã€ã‚³ãƒ³ãƒœãƒ«ãƒ¼ãƒˆãªã©ã€‚è¨ˆç®—å¼ã‚’æ˜è¨˜ã€‚")

class PracticalElement(dspy.Signature):
    """[3] å®Ÿæˆ¦ã§ã®ä½¿ã„æ–¹ã®ç”Ÿæˆã«ç‰¹åŒ–ã—ãŸSignature"""
    context = dspy.InputField(desc="ç«‹ã¡å›ã‚Šãƒ»æˆ¦è¡“ãƒ‡ãƒ¼ã‚¿")
    question = dspy.InputField(desc="ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•")
    practical_usage = dspy.OutputField(desc="å…·ä½“çš„ãªä½¿ç”¨å ´é¢ã€ãƒªã‚¹ã‚¯ãƒ»ãƒªã‚¿ãƒ¼ãƒ³ã€çŠ¶æ³åˆ¥ã®é¸æŠè‚¢ã€‚3ã¤ä»¥ä¸Šã®å…·ä½“ä¾‹ã‚’å«ã‚ã‚‹ã€‚")

class NotesElement(dspy.Signature):
    """[4] è£œè¶³ãƒ»æ³¨æ„ç‚¹ã®ç”Ÿæˆã«ç‰¹åŒ–ã—ãŸSignature"""
    context = dspy.InputField(desc="ã‚­ãƒ£ãƒ©å·®ãƒ»æ³¨æ„ç‚¹ãƒ‡ãƒ¼ã‚¿")
    question = dspy.InputField(desc="ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•")
    notes_and_tips = dspy.OutputField(desc="ã‚­ãƒ£ãƒ©å·®ã€åˆå¿ƒè€…å‘ã‘ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã€ã‚ˆãã‚ã‚‹é–“é•ã„ãªã©ã€‚")

# Configuration
DATA_DIR = Path(__file__).parent.parent.parent / 'data'
TRAINING_DATA_FILE = DATA_DIR / 'training_data.jsonl'
ELEMENT_FEEDBACK_FILE = DATA_DIR / 'element_feedback.jsonl'
OPTIMIZED_STATE_FILE = DATA_DIR / 'optimized_coach_state.json'
OPTIMIZATION_HISTORY_DIR = DATA_DIR / 'optimization_history'
MIN_TRAINING_EXAMPLES = 30

def load_training_data() -> List[Dict[str, Any]]:
    """Load training data from JSONL file"""
    if not TRAINING_DATA_FILE.exists():
        print(f"âŒ Training data not found: {TRAINING_DATA_FILE}")
        return []
    
    entries = []
    with open(TRAINING_DATA_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try:
                    entries.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
    
    return entries

def load_element_feedback() -> List[Dict[str, Any]]:
    """Load element-specific feedback from JSONL file"""
    if not ELEMENT_FEEDBACK_FILE.exists():
        print(f"ğŸ“ Element feedback not found: {ELEMENT_FEEDBACK_FILE}")
        return []
    
    entries = []
    with open(ELEMENT_FEEDBACK_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                try:
                    entries.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
    
    print(f"âœ… Loaded {len(entries)} element-specific feedback entries")
    return entries

def analyze_element_patterns(element_feedback: List[Dict]) -> Dict[int, List[str]]:
    """Analyze common improvement patterns for each element"""
    patterns = {1: [], 2: [], 3: [], 4: []}
    
    for entry in element_feedback:
        element_num = entry.get('element_number')
        correction = entry.get('correction', '')
        
        if element_num in patterns:
            patterns[element_num].append(correction)
    
    # Summarize patterns
    summary = {}
    for element_num, corrections in patterns.items():
        if corrections:
            summary[element_num] = {
                'count': len(corrections),
                'examples': corrections[:3]  # Top 3 examples
            }
    
    return summary

def prepare_dspy_dataset(entries: List[Dict]) -> List[dspy.Example]:
    """Convert JSONL entries to dspy.Example objects"""
    dataset = []
    
    for entry in entries:
        example = dspy.Example(
            question=entry.get('question', ''),
            gold_analysis=entry.get('gold_answer', ''),  # User correction
            context=entry.get('context', ''),
        ).with_inputs('question', 'context')
        
        dataset.append(example)
    
    return dataset

def coaching_quality_metric(gold: Any, pred: Any, trace=None) -> float:
    """
    === DSPy Optimization Metric ===
    
    Evaluates coaching quality across multiple dimensions:
    1. Relevance: Is advice related to the question?
    2. Specificity: Is it character/move specific?
    3. Actionability: Can user execute the advice?
    4. Tone: Is tone calm and objective (not overly enthusiastic)?
    
    Args:
        gold: Gold standard (user correction)
        pred: Prediction from model (dspy.Prediction)
        trace: dspy trace (for debugging)
    
    Returns:
        float: Quality score 0.0-1.0
    """
    
    # Extract text
    if isinstance(pred, dspy.Prediction):
        pred_text = getattr(pred, 'advice', str(pred))
    else:
        pred_text = str(pred)
    
    if isinstance(gold, dspy.Example):
        gold_text = gold.gold_analysis
    else:
        gold_text = str(gold)
    
    # Calculate sub-scores
    relevance_score = measure_relevance(pred_text, gold_text)
    specificity_score = measure_specificity(pred_text)
    actionability_score = measure_actionability(pred_text)
    tone_score = measure_tone_calmness(pred_text)
    
    # Combine with weights
    final_score = (
        0.35 * relevance_score +
        0.25 * specificity_score +
        0.25 * actionability_score +
        0.15 * tone_score
    )
    
    return final_score

def measure_relevance(pred: str, gold: str) -> float:
    """
    Measure relevance: Does prediction match user's correction?
    
    Heuristics:
    - Keyword overlap
    - Length similarity
    - Topic alignment
    """
    if not pred or not gold:
        return 0.0
    
    pred_lower = pred.lower()
    gold_lower = gold.lower()
    
    # Keyword overlap
    pred_words = set(pred_lower.split())
    gold_words = set(gold_lower.split())
    overlap = len(pred_words & gold_words)
    max_words = max(len(pred_words), len(gold_words))
    
    keyword_score = overlap / max_words if max_words > 0 else 0.0
    
    # Length similarity (both should be substantial)
    pred_len = len(pred_lower)
    gold_len = len(gold_lower)
    
    if pred_len > 10 and gold_len > 10:
        length_score = min(pred_len, gold_len) / max(pred_len, gold_len)
    else:
        length_score = 0.0
    
    # Combined relevance
    relevance = (0.7 * keyword_score + 0.3 * length_score)
    
    return min(1.0, relevance)

def measure_specificity(text: str) -> float:
    """
    Measure specificity: Are there character/move names and details?
    
    Heuristics:
    - Presence of character names (ã‚¯ãƒ©ã‚¦ãƒ‰, ãƒã‚¹, etc.)
    - Presence of move names (ç©ºå‰, ä¸ŠB, etc.)
    - Presence of numbers (% damage, frames, etc.)
    - Presence of Japanese technical terms
    """
    if not text:
        return 0.0
    
    score = 0.0
    
    # Character names (common SmashBros characters)
    characters = ['ã‚¯ãƒ©ã‚¦ãƒ‰', 'ãƒã‚¹', 'ãƒãƒªã‚ª', 'ãƒ”ã‚«ãƒãƒ¥ã‚¦', 'ãƒ‰ãƒ³ã‚­ãƒ¼', 
                  'ãƒªãƒ³ã‚¯', 'ã‚µãƒ ã‚¹', 'ãƒ¨ãƒƒã‚·ãƒ¼', 'ã‚«ãƒ¼ãƒ“ã‚£', 'ã‚¦ãƒ«ãƒ•']
    char_count = sum(1 for char in characters if char in text)
    score += min(0.3, char_count * 0.05)
    
    # Move names
    moves = ['ç©ºå‰', 'ç©ºå¾Œ', 'ç©ºä¸‹', 'ç©ºä¸Š', 'æ¨ªB', 'ä¸ŠB', 'ä¸‹B', 'DA', 'DAA',
             'Nå›', 'NA', 'æ¨ªå¼·', 'ä¸Šå¼·', 'ä¸‹å¼·', 'ã¤ã‹ã¿', 'ã¤ã‹ã¿æŠ•ã’']
    move_count = sum(1 for move in moves if move in text)
    score += min(0.3, move_count * 0.04)
    
    # Numbers (frames, %, damage)
    import re
    number_pattern = r'\d+'
    numbers = re.findall(number_pattern, text)
    score += min(0.2, len(numbers) * 0.02)
    
    # Technical terms
    technical_terms = ['ç¡¬ç›´', 'ã‚¬ãƒ¼ãƒ‰', 'ã‚·ãƒ¼ãƒ«ãƒ‰', 'ãµã£ã¨ã°ã—', 'ãƒ€ãƒ¡ãƒ¼ã‚¸', 
                      'ã‚³ãƒ³ãƒœ', 'ãƒªã‚»ãƒƒãƒˆ', 'ç€åœ°']
    term_count = sum(1 for term in technical_terms if term in text)
    score += min(0.2, term_count * 0.03)
    
    return min(1.0, score)

def measure_actionability(text: str) -> float:
    """
    Measure actionability: Can user execute the advice?
    
    Heuristics:
    - Step-by-step instructions
    - Action verbs (ã‚„ã‚‹, ã™ã‚‹, é¿ã‘ã‚‹, etc.)
    - Concrete examples
    - Condition/result pairs (if X then Y)
    """
    if not text:
        return 0.0
    
    score = 0.0
    
    # Action verbs
    action_verbs = ['ã‚„ã‚‹', 'ã™ã‚‹', 'é¿ã‘ã‚‹', 'ç‹™ã†', 'ä½¿ã†', 'ã¤ãªã', 
                   'ã‚³ãƒ³ãƒœ', 'æ´ã‚€', 'ã‚¬ãƒ¼ãƒ‰', 'ãšã‚‰ã™', 'å—ã‘èº«']
    verb_count = sum(1 for verb in action_verbs if verb in text)
    score += min(0.4, verb_count * 0.05)
    
    # Structural markers (ç¤ºã™ã‚¹ãƒ†ãƒƒãƒ—)
    markers = ['1.', '2.', '3.', 'â‘ ', 'â‘¡', 'â‘¢', 'â†’', 'ãã®å¾Œ', 'æ¬¡ã«']
    marker_count = sum(1 for marker in markers if marker in text)
    score += min(0.3, marker_count * 0.06)
    
    # Conditional statements (if-then)
    conditional_markers = ['ã‚‚ã—', 'ãªã‚‰', 'ã¨', 'å ´åˆ', 'æ™‚ã«']
    conditional_count = sum(1 for marker in conditional_markers if marker in text)
    score += min(0.3, conditional_count * 0.04)
    
    return min(1.0, score)

def measure_tone_calmness(text: str) -> float:
    """
    Measure tone calmness: Is the response calm and objective?
    
    Heuristics:
    - Penalty for overly enthusiastic expressions (ã€Œçµ¶å¯¾ã«ï¼ã€ã€Œè¶…å¼·ã„ï¼ã€etc.)
    - Penalty for excessive exclamation marks
    - Bonus for analytical terms (ã€Œåˆ†æã™ã‚‹ã¨ã€ã€Œãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€etc.)
    - Bonus for conditional/balanced language (ã€ŒãŸã ã—ã€ã€Œå ´åˆã«ã‚ˆã‚‹ã€etc.)
    
    Returns:
        float: Calmness score 0.0-1.0 (higher = calmer)
    """
    if not text:
        return 0.5
    
    score = 1.0  # Start at maximum calmness
    
    # Penalty: Overly enthusiastic expressions
    enthusiastic_phrases = [
        'çµ¶å¯¾ã«', 'è¶…å¼·ã„', 'è¶…é‡è¦', 'æœ€å¼·', 'å¿…ãšå‹ã¦ã‚‹', 'ã‚ã¡ã‚ƒãã¡ã‚ƒ',
        'ãƒ¤ãƒã„', 'ã¶ã£å£Šã‚Œ', 'åœ§å€’çš„', 'å®Œç’§', 'ç„¡æ•µ'
    ]
    for phrase in enthusiastic_phrases:
        count = text.count(phrase)
        score -= count * 0.1
    
    # Penalty: Excessive exclamation marks
    exclamation_count = text.count('ï¼') + text.count('!')
    if exclamation_count > 3:
        score -= (exclamation_count - 3) * 0.05
    
    # Penalty: All-caps enthusiasm (rarely applies to Japanese but check anyway)
    import re
    all_caps_words = re.findall(r'\b[A-Z]{3,}\b', text)
    score -= len(all_caps_words) * 0.05
    
    # Bonus: Analytical/objective language
    analytical_terms = [
        'åˆ†æã™ã‚‹ã¨', 'ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰', 'çµ±è¨ˆçš„ã«', 'å®¢è¦³çš„ã«', 'æ¤œè¨¼ã™ã‚‹ã¨',
        'ç†è«–ä¸Š', 'å®Ÿéš›ã«ã¯', 'ç¢ºç‡çš„ã«', 'æ•°å€¤çš„ã«'
    ]
    analytical_count = sum(1 for term in analytical_terms if term in text)
    score += analytical_count * 0.05
    
    # Bonus: Balanced/conditional language
    balanced_terms = [
        'ãŸã ã—', 'å ´åˆã«ã‚ˆã‚‹', 'çŠ¶æ³æ¬¡ç¬¬', 'ã‚±ãƒ¼ã‚¹ãƒã‚¤ã‚±ãƒ¼ã‚¹', 'ä¸€æ¦‚ã«ã¯',
        'å¿…ãšã—ã‚‚', 'ã¨ã¯é™ã‚‰ãªã„', 'å¯èƒ½æ€§ãŒã‚ã‚‹', 'å‚¾å‘ãŒã‚ã‚‹'
    ]
    balanced_count = sum(1 for term in balanced_terms if term in text)
    score += balanced_count * 0.05
    
    # Bonus: Calm instructional language
    calm_phrases = [
        'è€ƒãˆã‚‰ã‚Œã¾ã™', 'æ¨å¥¨ã—ã¾ã™', 'æ¤œè¨ã—ã¦ãã ã•ã„', 'ãŠã™ã™ã‚ã§ã™',
        'æœ‰åŠ¹ã§ã™', 'åŠ¹æœçš„ã§ã™', 'å‚è€ƒã«ã—ã¦ãã ã•ã„'
    ]
    calm_count = sum(1 for phrase in calm_phrases if phrase in text)
    score += calm_count * 0.03
    
    return max(0.0, min(1.0, score))

def run_optimization(trainset: List[dspy.Example], metric: Callable) -> Any:
    """
    Run dspy.Teleprompter optimization
    
    Args:
        trainset: Training examples (from load_training_data)
        metric: Quality metric function
    
    Returns:
        Optimized SmashCoach model
    """
    
    print("\n" + "="*70)
    print("ğŸ”„ Running dspy.Teleprompter Optimization")
    print("="*70)
    
    # Import the coach model
    from src.brain.model import SmashCoach, create_coach
    
    # Initialize coach
    print("\n1ï¸âƒ£  Initializing base coach...")
    base_coach = create_coach()
    
    # Initialize optimizer
    print("2ï¸âƒ£  Initializing dspy.Teleprompter...")
    try:
        optimizer = dspy.Teleprompter(
            metric=metric,
            trainset=trainset,
            valset=trainset[:len(trainset)//5],  # 20% for validation
            num_trials=100,
            seed=42
        )
    except Exception as e:
        print(f"âš ï¸  Could not use dspy.Teleprompter: {e}")
        print("   Falling back to manual prompt tuning...")
        return base_coach
    
    # Run optimization
    print("3ï¸âƒ£  Running optimization trials...")
    print("   (This may take 10-30 minutes and $5-15 in API costs)\n")
    
    try:
        optimized_coach = optimizer.compile(
            student=base_coach,
            trainset=trainset,
            metric=metric,
        )
        print("\nâœ… Optimization complete!")
        return optimized_coach
    except Exception as e:
        print(f"\nâš ï¸  Optimization failed: {e}")
        print("   Using base coach as fallback")
        return base_coach

def save_optimized_state(coach: Any) -> None:
    """Save optimized coach state to file"""
    
    print(f"\nğŸ’¾ Saving optimized state to {OPTIMIZED_STATE_FILE}")
    
    DATA_DIR.mkdir(exist_ok=True)
    
    # Serialize coach state
    state = {
        'timestamp': datetime.now().isoformat(),
        'model_type': 'SmashCoach',
        'version': '1.0',
        'prompts': {
            # Extract prompts from coach module
            'analysis': getattr(coach.analyze, '_signature', None),
            'advice': getattr(coach.advise, '_signature', None),
        },
        'training_examples': len(load_training_data()),
    }
    
    with open(OPTIMIZED_STATE_FILE, 'w', encoding='utf-8') as f:
        json.dump(state, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"âœ… Saved to {OPTIMIZED_STATE_FILE}")

def compare_before_after(base_coach: Any, optimized_coach: Any, test_queries: List[str]) -> List[Dict]:
    """
    Compare predictions before and after optimization
    
    Returns:
        List[Dict]: æ¯”è¼ƒçµæœã®ãƒªã‚¹ãƒˆï¼ˆå±¥æ­´ä¿å­˜ç”¨ï¼‰
    """
    
    print("\n" + "="*70)
    print("ğŸ“Š Before vs After Comparison")
    print("="*70)
    
    comparisons = []
    
    for i, query in enumerate(test_queries[:3], 1):
        print(f"\n{'â”€'*70}")
        print(f"Query {i}: {query}")
        print(f"{'â”€'*70}")
        
        comparison = {
            "query": query,
            "before": {},
            "after": {}
        }
        
        try:
            pred_before = base_coach.forward(query)
            before_text = str(pred_before.analysis) + "\n" + str(pred_before.advice)
            comparison["before"] = {
                "analysis": str(pred_before.analysis),
                "advice": str(pred_before.advice)
            }
            print(f"\nğŸ“Œ BEFORE Optimization:")
            print(f"   Analysis: {str(pred_before.analysis)[:100]}...")
            print(f"   Advice: {str(pred_before.advice)[:100]}...")
        except Exception as e:
            print(f"   âŒ Error: {e}")
            comparison["before"]["error"] = str(e)
        
        try:
            pred_after = optimized_coach.forward(query)
            after_text = str(pred_after.analysis) + "\n" + str(pred_after.advice)
            comparison["after"] = {
                "analysis": str(pred_after.analysis),
                "advice": str(pred_after.advice)
            }
            print(f"\nâœ¨ AFTER Optimization:")
            print(f"   Analysis: {str(pred_after.analysis)[:100]}...")
            print(f"   Advice: {str(pred_after.advice)[:100]}...")
        except Exception as e:
            print(f"   âŒ Error: {e}")
            comparison["after"]["error"] = str(e)
        
        comparisons.append(comparison)
    
    return comparisons

def save_optimization_history(
    timestamp: str,
    training_count: int,
    element_feedback_count: int,
    comparisons: List[Dict],
    optimized_elements: Dict[int, Any]
) -> str:
    """
    æœ€é©åŒ–å±¥æ­´ã‚’ä¿å­˜
    
    Args:
        timestamp: å®Ÿè¡Œæ—¥æ™‚
        training_count: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿æ•°
        element_feedback_count: è¦ç´ åˆ¥ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ•°
        comparisons: Before/Afteræ¯”è¼ƒçµæœ
        optimized_elements: æœ€é©åŒ–ã•ã‚ŒãŸè¦ç´ 
    
    Returns:
        str: ä¿å­˜ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    """
    OPTIMIZATION_HISTORY_DIR.mkdir(parents=True, exist_ok=True)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å: YYYYMMDD_HHMMSS_optimization.md
    filename = f"{timestamp.replace(':', '').replace('-', '').replace('.', '')[:15]}_optimization.md"
    filepath = OPTIMIZATION_HISTORY_DIR / filename
    
    # Markdownå½¢å¼ã§ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = f"""# æœ€é©åŒ–å±¥æ­´ãƒ¬ãƒãƒ¼ãƒˆ

**å®Ÿè¡Œæ—¥æ™‚**: {timestamp}

## ğŸ“Š ãƒ‡ãƒ¼ã‚¿çµ±è¨ˆ

- **ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿**: {training_count}ä»¶
- **è¦ç´ åˆ¥ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯**: {element_feedback_count}ä»¶
- **æœ€é©åŒ–ã•ã‚ŒãŸè¦ç´ æ•°**: {len(optimized_elements)}å€‹

## ğŸ”§ æœ€é©åŒ–ã•ã‚ŒãŸè¦ç´ 

"""
    
    element_names = {
        1: "ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ãƒ»åŸºç¤æƒ…å ±",
        2: "æŠ€è¡“çš„è§£èª¬",
        3: "å®Ÿæˆ¦ã§ã®ä½¿ã„æ–¹",
        4: "è£œè¶³ãƒ»æ³¨æ„ç‚¹"
    }
    
    for elem_num in sorted(optimized_elements.keys()):
        report += f"- [{elem_num}] {element_names.get(elem_num, 'Unknown')}\n"
    
    report += f"\n## ğŸ“ Before vs After æ¯”è¼ƒ\n\n"
    
    for i, comp in enumerate(comparisons, 1):
        report += f"### ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ {i}\n\n"
        report += f"**è³ªå•**: {comp['query']}\n\n"
        
        if "error" not in comp["before"]:
            report += f"#### ğŸ“Œ BEFOREï¼ˆæœ€é©åŒ–å‰ï¼‰\n\n"
            report += f"**Analysis**:\n```\n{comp['before'].get('analysis', 'N/A')[:300]}\n```\n\n"
            report += f"**Advice**:\n```\n{comp['before'].get('advice', 'N/A')[:300]}\n```\n\n"
        
        if "error" not in comp["after"]:
            report += f"#### âœ¨ AFTERï¼ˆæœ€é©åŒ–å¾Œï¼‰\n\n"
            report += f"**Analysis**:\n```\n{comp['after'].get('analysis', 'N/A')[:300]}\n```\n\n"
            report += f"**Advice**:\n```\n{comp['after'].get('advice', 'N/A')[:300]}\n```\n\n"
        
        # å¤‰åŒ–ã®åˆ†æ
        if "error" not in comp["before"] and "error" not in comp["after"]:
            before_len = len(comp['before'].get('advice', ''))
            after_len = len(comp['after'].get('advice', ''))
            report += f"**å¤‰åŒ–**: å›ç­”é•· {before_len}æ–‡å­— â†’ {after_len}æ–‡å­—ï¼ˆ{after_len - before_len:+d}æ–‡å­—ï¼‰\n\n"
        
        report += "---\n\n"
    
    report += f"""## ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã‚’ç¢ºèªã—ã¦ã€æ”¹å–„ã®æ–¹å‘æ€§ã‚’è©•ä¾¡
2. æ”¹å–„ãŒä¸ååˆ†ãªå ´åˆ:
   - ã•ã‚‰ã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åé›†ï¼ˆç‰¹ã«å¼±ã„è¦ç´ ï¼‰
   - ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’èª¿æ•´
   - å†åº¦æœ€é©åŒ–ã‚’å®Ÿè¡Œ
3. æ”¹å–„ãŒååˆ†ãªå ´åˆ:
   - Botã‚’å†èµ·å‹•ã—ã¦æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’é©ç”¨
   - ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è©•ä¾¡ã‚’åé›†

## ğŸ“ ãƒ¡ãƒ¢

- å‰å›ã®æœ€é©åŒ–ã‹ã‚‰ã®å·®åˆ†ã‚’ç¢ºèªã™ã‚‹å ´åˆã€å‰å›ã®ãƒ¬ãƒãƒ¼ãƒˆã¨æ¯”è¼ƒã—ã¦ãã ã•ã„
- æœ€é©åŒ–ã¯æ®µéšçš„ã«æ”¹å–„ã™ã‚‹ãŸã‚ã€1å›ã§å®Œç’§ã«ãªã‚‹ã¨ã¯é™ã‚Šã¾ã›ã‚“
- å®šæœŸçš„ãªæœ€é©åŒ–ï¼ˆ2é€±é–“ã«1å›ç¨‹åº¦ï¼‰ã‚’æ¨å¥¨ã—ã¾ã™
"""
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nğŸ“„ Optimization history saved to: {filepath}")
    return str(filepath)

def optimize_element(element_number: int, element_feedback: List[Dict], element_signature, metric: Callable) -> Any:
    """
    ç‰¹å®šã®è¦ç´ ã‚’å€‹åˆ¥ã«æœ€é©åŒ–
    
    Args:
        element_number: è¦ç´ ç•ªå·ï¼ˆ1-4ï¼‰
        element_feedback: ãã®è¦ç´ ã«é–¢ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒªã‚¹ãƒˆ
        element_signature: è¦ç´ ç”¨ã®Signatureï¼ˆFrameDataElementç­‰ï¼‰
        metric: è©•ä¾¡ãƒ¡ãƒˆãƒªãƒƒã‚¯
    
    Returns:
        æœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«
    """
    if not element_feedback:
        print(f"  âš ï¸  No feedback for element {element_number}, skipping...")
        return None
    
    print(f"\n  ğŸ“ Optimizing element {element_number} with {len(element_feedback)} examples...")
    
    # ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’DSPy Exampleã«å¤‰æ›
    element_dataset = []
    for fb in element_feedback:
        example = dspy.Example(
            question=fb.get('question', ''),
            correction=fb.get('correction', ''),
            context=""  # ç°¡ç•¥åŒ–ã®ãŸã‚contextã¯ç©º
        ).with_inputs('question', 'context')
        element_dataset.append(example)
    
    # æœ€é©åŒ–å®Ÿè¡Œ
    try:
        base_module = dspy.ChainOfThought(element_signature)
        
        # Teleprompter ã§æœ€é©åŒ–ï¼ˆè©¦è¡Œå›æ•°ã‚’æ¸›ã‚‰ã™ï¼‰
        optimizer = dspy.Teleprompter(
            metric=metric,
            trainset=element_dataset,
            valset=element_dataset[:max(1, len(element_dataset)//5)],
            num_trials=20,  # è¦ç´ åˆ¥ãªã®ã§è»½ã
            seed=42
        )
        
        optimized = optimizer.compile(
            student=base_module,
            trainset=element_dataset,
            metric=metric
        )
        
        print(f"  âœ… Element {element_number} optimization complete")
        return optimized
        
    except Exception as e:
        print(f"  âš ï¸  Element {element_number} optimization failed: {e}")
        return None

def main():
    """Main optimization pipeline (V2: è¦ç´ åˆ¥æœ€é©åŒ–å¯¾å¿œ)"""
    
    print("\n" + "="*70)
    print("ğŸš€ SmashZettel-Bot: Coach Optimizer V2 (è¦ç´ åˆ¥æœ€é©åŒ–å¯¾å¿œ)")
    print("="*70)
    
    # Step 1: Load training data
    print("\nğŸ“‚ Loading training data...")
    entries = load_training_data()
    
    if not entries:
        print(f"âŒ No training data found. Run the bot first to collect /teach corrections.")
        return 1
    
    print(f"âœ… Loaded {len(entries)} training examples")
    
    # Step 1.5: Load and analyze element feedback
    print("\nğŸ” Loading element-specific feedback...")
    element_feedback = load_element_feedback()
    
    element_patterns = {}
    if element_feedback:
        element_patterns = analyze_element_patterns(element_feedback)
        print("\nğŸ“Š Element Feedback Analysis:")
        element_names = {
            1: "ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ãƒ»åŸºç¤æƒ…å ±",
            2: "æŠ€è¡“çš„è§£èª¬",
            3: "å®Ÿæˆ¦ã§ã®ä½¿ã„æ–¹",
            4: "è£œè¶³ãƒ»æ³¨æ„ç‚¹"
        }
        for elem_num, data in element_patterns.items():
            print(f"\n  [{elem_num}] {element_names[elem_num]}")
            print(f"      ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯æ•°: {data['count']}ä»¶")
            print(f"      æ”¹å–„ä¾‹:")
            for i, example in enumerate(data['examples'][:2], 1):
                print(f"        {i}. {example[:80]}{'...' if len(example) > 80 else ''}")
    
    if len(entries) < MIN_TRAINING_EXAMPLES:
        print(f"\nâš ï¸  Warning: Only {len(entries)} examples.")
        print(f"   Minimum recommended: {MIN_TRAINING_EXAMPLES}")
        print(f"   Optimization quality may be poor with fewer examples.")
    
    # Step 2: Prepare DSPy dataset
    print("\nğŸ“Š Preparing DSPy dataset...")
    trainset = prepare_dspy_dataset(entries)
    print(f"âœ… Prepared {len(trainset)} examples")
    
    # Step 2.5: è¦ç´ åˆ¥æœ€é©åŒ–ï¼ˆelement_feedbackãŒã‚ã‚‹å ´åˆï¼‰
    optimized_elements = {}
    if element_feedback and len(element_feedback) >= 10:
        print("\nğŸ”§ Running element-specific optimization...")
        
        # è¦ç´ åˆ¥ã«ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åˆ†é¡
        elements_data = {1: [], 2: [], 3: [], 4: []}
        for fb in element_feedback:
            elem_num = fb.get('element_number')
            if elem_num in elements_data:
                elements_data[elem_num].append(fb)
        
        # å„è¦ç´ ã‚’å€‹åˆ¥ã«æœ€é©åŒ–
        element_signatures = {
            1: FrameDataElement,
            2: TechnicalElement,
            3: PracticalElement,
            4: NotesElement
        }
        
        for elem_num in [1, 2, 3, 4]:
            if elements_data[elem_num]:
                optimized = optimize_element(
                    elem_num,
                    elements_data[elem_num],
                    element_signatures[elem_num],
                    coaching_quality_metric
                )
                if optimized:
                    optimized_elements[elem_num] = optimized
        
        print(f"\nâœ… Optimized {len(optimized_elements)} elements individually")
    
    # Step 3: Run optimization
    print("\nğŸ”§ Starting full model optimization...")
    
    from src.brain.model import create_coach
    base_coach = create_coach()
    
    optimized_coach = run_optimization(trainset, coaching_quality_metric)
    
    # Step 4: Save optimized state
    save_optimized_state(optimized_coach)
    
    # Step 5: Compare before/after
    test_queries = [
        entries[0].get('question', ''),
        entries[len(entries)//2].get('question', ''),
        entries[-1].get('question', ''),
    ]
    comparisons = compare_before_after(base_coach, optimized_coach, test_queries)
    
    # Step 5.5: Save optimization history
    timestamp = datetime.now().isoformat()
    history_file = save_optimization_history(
        timestamp=timestamp,
        training_count=len(entries),
        element_feedback_count=len(element_feedback) if element_feedback else 0,
        comparisons=comparisons,
        optimized_elements=optimized_elements
    )
    
    # Step 6: Report
    print("\n" + "="*70)
    print("âœ… Optimization Complete!")
    print("="*70)
    
    # Enhanced report with element feedback
    element_summary = ""
    if element_feedback:
        total_element_fb = len(element_feedback)
        element_summary = f"""
ğŸ“Š Element-Specific Feedback Summary:
   Total: {total_element_fb}ä»¶
   è¦ç´ åˆ¥ã®æ”¹å–„ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå­¦ç¿’ã•ã‚Œã¾ã—ãŸã€‚
   è©³ç´°ã¯ä¸Šè¨˜ã®ã€ŒElement Feedback Analysisã€ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
"""
    
    print(f"""
ğŸ“„ æœ€é©åŒ–å±¥æ­´ãƒ¬ãƒãƒ¼ãƒˆ: {history_file}

Next steps:
1. Review before/after comparison above and optimization history report
2. If quality improved:
   - Update src/main.py to load optimized model
   - Restart Discord bot
   - Monitor quality of responses
3. If quality didn't improve:
   - Collect more training examples (30+ recommended)
   - Use /teach_element for focused improvements
   - Improve metric scoring
   - Re-run optimization
{element_summary}
ğŸ“ Note: Remember to commit your optimized state:
   git add data/optimized_coach_state.json data/element_feedback.jsonl data/optimization_history/
   git commit -m "chore: Update coach optimization"
   git push origin main
""")
    
    return 0

if __name__ == '__main__':
    sys.exit(main())
